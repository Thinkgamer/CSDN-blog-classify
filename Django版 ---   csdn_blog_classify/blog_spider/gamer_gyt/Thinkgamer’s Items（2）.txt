CSDN博客分类系统的分析与实现

一：爬虫爬取csdn博客各个系列的博文和标签
       在这里只给出主要代码：

       

二：对其进行词频统计，找出频率最高的N个词，写入文件（主要是为第三步分类提供训练的数据集PS：小编的训练集不是太准确，各路大神若有好的意见可以给指导指导）
      在这里简化为使用MapReduce程序统计tag

三：使用贝叶斯分类算法进行分类
        贝叶斯算法原理请参考：http://blog.csdn.net/gamer_gyt/article/details/47205371

        Python代码实现请参考：http://blog.csdn.net/gamer_gyt/article/details/47860945

        分类代码实现如下：      

    




【大创_社区划分】——PageRank算法MapReduce实现

PageRank算法的分析和Python实现参考：http://blog.csdn.net/gamer_gyt/article/details/47443877

举例来讲：
假设每个网页都有一个自己的默认PR值，相当于人为添加给它是一种属性，用来标识网页的等级或者重要性，从而依据此标识达到排名目的。假设有ID号是1的一个网页，PR值是10，假如它产生了到ID=3，ID=6，ID=8 ，ID=9这4个网页的链接。那么可以理解为ID=1的网页向ID=3，6，8，9的4个网页各贡献了2.5的PR值。如果想求任意一个网页假设其ID=3的PR值，需要得到所有的其他网页对ID=3这个网页的贡献的总和，再按照函数“所求PR”=“总和”*0.85+0.15得到。经过循环多次上述过程求得的网页PR值，可以作为网页排名的标识。
1：准备数据 
理论数据是通过网页爬虫得到了某个特定封闭系统的所有网页的信息，为了测试程序，可以自己模拟生成自己定义特定格式的数据。下面是我用来测试的数据，存储方式如图



（注：对于自定义模拟数据，在PR初始值的选取时，所有的网页是“平等”的，不会说自己写的网页和Google的热门网页有多少差别，但是按照某种法则经过一定运算后PR是不一样的，比如很多其他的网页可能会链接到google，它的PR自然会比你的高。所以初始值的选取按照这种逻辑来讲符合现实些，即所有网页默认PR值相等。但是即使初始值定的不一样，整个系统的PR总和可能会变化，最后的每个网页PR也会发生变化，但是这种量之间的变化，不会影响到网页自身的通过比较大小方式上的逻辑排名。
2：MapReduce过程
map接受的数据格式默认是<偏移量，文本行>这样的<key,value>对，形如<0,1    5  2 3 4 5><20,2    10 3 5 8 9>.
目标 ：将默认数据格式，转换成自定义格式<key,value>对。
已知 ：hadoop框架在Map阶段的时候会自动实现sort过程，就是将相同的key的所有value保存到list，形如<key,list(1,1,1,2)>这种形式，例如上述对ID=2的网页有ID=1，6，7，8.这4个网页贡献（1.25，1，5/3，5），那么如果你采用的key是网页ID，那么hadoop框架会以此种形式<2,list(1.25，1，5/3，5)>输出，传递给reduce。
Reduce阶段：
分析：这个阶段操作就相对简单很多，读取map的输出<key,value>，并解析出来。
具体操作：把values中的数字相加即为对应id的PageRank值。
结果如下图：


代码如下




【大创_社区划分】——PageRank算法的解析与Python实现


一、什么是pagerank
PageRank的Page可是认为是网页，表示网页排名，也可以认为是Larry Page(google 产品经理)，因为他是这个算法的发明者之一，还是google CEO（^_^）。PageRank算法计算每一个网页的PageRank值，然后根据这个值的大小对网页的重要性进行排序。它的思想是模拟一个悠闲的上网者，上网者首先随机选择一个网页打开，然后在这个网页上呆了几分钟后，跳转到该网页所指向的链接，这样无所事事、漫无目的地在网页上跳来跳去，PageRank就是估计这个悠闲的上网者分布在各个网页上的概率。
二、最简单pagerank模型
互联网中的网页可以看出是一个有向图，其中网页是结点，如果网页A有链接到网页B，则存在一条有向边A->B，下面是一个简单的示例：

这个例子中只有四个网页，如果当前在A网页，那么悠闲的上网者将会各以1/3的概率跳转到B、C、D，这里的3表示A有3条出链，如果一个网页有k条出链，那么跳转任意一个出链上的概率是1/k，同理D到B、C的概率各为1/2，而B到C的概率为0。一般用转移矩阵表示上网者的跳转概率，如果用n表示网页的数目，则转移矩阵M是一个n*n的方阵；如果网页j有k个出链，那么对每一个出链指向的网页i，有M[i][j]=1/k，而其他网页的M[i][j]=0；上面示例图对应的转移矩阵如下：

初试时，假设上网者在每一个网页的概率都是相等的，即1/n，于是初试的概率分布就是一个所有值都为1/n的n维列向量V0，用V0去右乘转移矩阵M，就得到了第一步之后上网者的概率分布向量MV0,（nXn）*(nX1)依然得到一个nX1的矩阵。下面是V1的计算过程：

注意矩阵M中M[i][j]不为0表示用一个链接从j指向i，M的第一行乘以V0，表示累加所有网页到网页A的概率即得到9/24。得到了V1后，再用V1去右乘M得到V2，一直下去，最终V会收敛，即Vn=MV(n-1)，上面的图示例，不断的迭代，最终V=[3/9,2/9,2/9,2/9]‘：


三、终止点问题
上述上网者的行为是一个马尔科夫过程的实例，要满足收敛性，需要具备一个条件：

图是强连通的，即从任意网页可以到达其他任意网页：
互联网上的网页不满足强连通的特性，因为有一些网页不指向任何网页，如果按照上面的计算，上网者到达这样的网页后便走投无路、四顾茫然，导致前面累计得到的转移概率被清零，这样下去，最终的得到的概率分布向量所有元素几乎都为0。假设我们把上面图中C到A的链接丢掉，C变成了一个终止点，得到下面这个图：

对应的转移矩阵为：

连续迭代下去，最终所有元素都为0：


四、陷阱问题
另外一个问题就是陷阱问题，即有些网页不存在指向其他网页的链接，但存在指向自己的链接。比如下面这个图：

上网者跑到C网页后，就像跳进了陷阱，陷入了漩涡，再也不能从C中出来，将最终导致概率分布值全部转移到C上来，这使得其他网页的概率分布值为0，从而整个网页排名就失去了意义。如果按照上面图对应的转移矩阵为：

不断的迭代下去，就变成了这样：


五、解决终止点问题和陷阱问题
上面过程，我们忽略了一个问题，那就是上网者是一个悠闲的上网者，而不是一个愚蠢的上网者，我们的上网者是聪明而悠闲，他悠闲，漫无目的，总是随机的选择网页，他聪明，在走到一个终结网页或者一个陷阱网页（比如两个示例中的C），不会傻傻的干着急，他会在浏览器的地址随机输入一个地址，当然这个地址可能又是原来的网页，但这里给了他一个逃离的机会，让他离开这万丈深渊。模拟聪明而又悠闲的上网者，对算法进行改进，每一步，上网者可能都不想看当前网页了，不看当前网页也就不会点击上面的连接，而上悄悄地在地址栏输入另外一个地址，而在地址栏输入而跳转到各个网页的概率是1/n。假设上网者每一步查看当前网页的概率为a，那么他从浏览器地址栏跳转的概率为(1-a)，于是原来的迭代公式转化为：

现在我们来计算带陷阱的网页图的概率分布：

重复迭代下去，得到：


六、用Python实现Page Rank算法


