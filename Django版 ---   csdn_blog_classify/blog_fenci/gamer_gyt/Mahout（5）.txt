Mahout学习之聚类算法Kmeans


一：kMeans算法介绍


        聚类分析是一种静态数据分析方法，常被用于机器学习，模式识别，数据挖掘等领域。通常认为，聚类是一种无监督式的机器学习方法，它的过程是这样的：在未知样本类别的情况下，通过计算样本彼此间的距离（欧式距离,马式距离，汉明距离，余弦距离等）来估计样本所属类别。从结构性来划分，聚类方法分为自上而下和自下而上两种方法，前者的算法是先把所有样本视为一类，然后不断从这个大类中分离出小类，直到不能再分为止；后者则相反，首先所有样本自成一类，然后不断两两合并，直到最终形成几个大类。　

常用的聚类方法主要有以下四种： 　　//照搬的wiki，比较懒...

Connectivity based clustering　　（如hierarchical clustering 层次聚类法)

Centroid-based clustering　　(如kmeans)

Distribution-based clustering

Density-based clustering

　　Kmeans聚类是一种自下而上的聚类方法，它的优点是简单、速度快；缺点是聚类结果与初始中心的选择有关系，且必须提供聚类的数目。Kmeans的第二个缺点是致命的，因为在有些时候，我们不知道样本集将要聚成多少个类别，这种时候kmeans是不适合的，推荐使用hierarchical 或meanshift来聚类。第一个缺点可以通过多次聚类取最佳结果来解决。

　　Kmeans的计算过程大概表示如下

随机选择k个聚类中心. 最终的类别个数<= k

计算每个样本到各个中心的距离

每个样本聚类到离它最近的中心

重新计算每个新类的中心

重复以上步骤直到满足收敛要求。(通常就是中心点不再改变或满足一定迭代次数).


二：Mahout实现

1.数据准备

仿造数据1.txt


8 8

7 7

6.1 6.1

9 9

2 2

1 1

0 0

2.9 2.9


2.txt


8.1 8.1

7.1 7.1

6.2 6.2

7.1 7.1

2.1 2.1

1.1 1.1

0.1 0.1

3.0 3.0


2.将数据转换为序列文件


方法1：命令行转换，具体参考：点击打开链接

方法2：代码转换，可借用canopy算法中的代码，具体参考：点击打开链接






3.运行


bin/mahout kmeans -i /yourFilePath -o /yourFileOutputPath -c /yourStartCenter -k 2 -x 5 -cl

参数意义说明：

-i 设置文件输入路径， -o 为文件输出路径  -c 为初始输入聚类中心 -k 表示聚类的数目 -x 表示最大的循环次数 -cl 表示算法完成后进行原始数据的分类








4.结果分析


因为k值为2，所以在clusters-0中有两个聚类中心文件

初始化聚类中心为[2.0,2.0]  [2.1,2.1]

clueters-1和cluster-2的输出为[1.033,1.033] [5.725,5.725]  ,   [1.525,1.525]  [7.325,7.325]

最后的聚类中心是[1.525，1.525]  [7.325，7.325]

使用此聚类中心对数据进行聚类，得到clusteredPoints文件夹下边的文件












Mahout分类算法学习之实现Naive Bayes分类示例

1.简介
(1) 贝叶斯分类器的分类原理发源于古典概率理论，是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。朴素贝叶斯分类器(Naive Bayes Classifier)做了一个简单的假定：给定目标值时属性之间相互条件独立，即给定元组的类标号，假定属性值有条件地相互独立，即在属性间不存在依赖关系。朴素贝叶斯分类模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。

(2) Mahout 实现了Traditional Naive Bayes 和Complementary Naive Bayes，后者是在前者的基础上增加了结果分析功能(Result Analyzer).

(3) 主要相关的Mahout类:
org.apache.mahout.classifier.naivebayes.NaiveBayesModel
org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifier
org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifier

2.数据

使用20 newsgroups data (http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz) ，数据集按时间分为训练数据和测试数据，总大小约为85MB，每个数据文件为一条信息，文件头部几行指定消息的发送者、长度、类型、使用软件，以及主题等，然后用空行将其与正文隔开，正文没有固定的格式。

3.目标

根据新闻文档内容，将其分到不同的文档类型中。

4.程序

使用Mahout自带示例程序，主要的训练类和测试类分别为TrainNaiveBayesJob.java和TestNaiveBayesDriver.java，JAR包为mahout-core-0.7-job.jar，详细代码见(mahout-distribution-0.7/core/src/main/java/org/apache/mahout/classifier/naivebayes/trainning,mahout-distribution-0.7/core/src/main/java/org/apache/mahout/classifier/naivebayes/test).

5.步骤
(1) 数据准备

①将20news-bydate.tar.gz解压，并将20news-bydate中的所有子文夹中的内容复制到20news-all中，该步骤已经完成，20news-all文件夹存放在hdfs:/share/data/ Mahout_examples_Data_Set中
②将20news-all放在hdfs的用户根目录下
user@hadoop:~/workspace$hadoop dfs -cp /share/data/Mahout_examples_Data_Set/20news-all .
③从20newsgroups data创建序列文件(sequence files)
user@hadoop:~/workspace$mahout seqdirectory -i 20news-all -o 20news-seq
④将序列文件转化为向量
user@hadoop:~/workspace$mahout seq2sparse -i ./20news-seq -o ./20news-vectors  -lnorm -nv  -wt tfidf  
⑤将向量数据集分为训练数据和检测数据，以随机40-60拆分
user@hadoop:~/workspace$mahout split -i ./20news-vectors/tfidf-vectors --trainingOutput ./20news-train-vectors --testOutput ./20news-test-vectors --randomSelectionPct 40 --overwrite --sequenceFiles -xm sequential

(2)训练朴素贝叶斯模型
user@hadoop:~/workspace$mahout trainnb -i  ./20news-train-vectors -el -o ./model -li ./labelindex -ow -c  

(3)检验朴素贝叶斯模型
user@hadoop:~/workspace$mahout testnb -i ./20news-train-vectors -m ./model -l ./labelindex -ow -o 20news-testing -c
结果如下：

(4)检测模型分类效果
user@hadoop:~/workspace$mahout testnb -i ./20news-test-vectors -m ./model -l ./labelindex -ow -o ./20news-testing -c
结果如下：

(5)查看结果，将序列文件转化为文本
user@hadoop:~/workspace$mahout seqdumper -i ./20news-testing/part-m-00000 -o ./20news_testing.res
user@hadoop:~/workspace$cat 20news_testging.res

结果如下：




Mahout学习之命令行创建序列文件

一：命令行转换
创建新的工作目录
mkdir lastfm
mkdir ./lastfm/original
export WORK_DIR=/home/thinkgamer/document/lastfm
cd $WORK_DIR
自己准备一个数据集放在original文件夹下,例如将点击打开链接下边的数据保存在synthetic_control.data中进行转换，首先将其放在origiinal文件夹中
进入mahout的安装目录，前提是hadoop环境是启动的
cd /usr/local/hadoop/mahout
bin/mahout seqdirectory -i $WORK_DIR/original -o $WORK_DIR/sequencesfile
然后进入$WORK_DIR目录下有一个sequencesfile文件夹
cd $WORK_DIR
cd sequencesfile
ls
会显示如下：
part-m-00000  _SUCCESS

命令行查看part-m-00000的文件内容为：
bin/mahout seqdumper -i $WORK_DIR/sequencesfile/part-m-00000 | more0

二：mapreduce转换
具体请参考mahout运行canopy程序中的程序：点击打开链接
三：命令行转换为文本文件
bin/mahout seqdumper -i $WORK_DIR/sequencesfile/part-m-00000 -o $WORK_DIR/clusteranalyzer

mahout版本不同如果上边的不好使，换用下边的
bin/mahout seqdumper -s $WORK_DIR/sequencesfile/part-m-00000 -o $WORK_DIR/clusteranalyzer


Mahout学习之运行canopy算法错误及解决办法

一：将Text转换成Vector序列文件时

 在Hadoop中运行编译打包好的jar程序，可能会报下面的错误：
Exception in thread "main" java.lang.NoClassDefFoundError:  org/apache/mahout/common/AbstractJob 
书中和网上给的解决办法都是：把Mahout根目录下的相应的jar包复制到Hadoop根目录下的lib文件夹下，同时重启Hadoop
但是到了小编这里不管怎么尝试，都不能解决，最终放弃了打包成jar运行的念头，就在对源码进行了修改，在eclipse运行了
二：java.lang.Exception: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.Text
此种错误，由于数据集是从网上下载的，故小编猜测是数据集的问题
因为小编尝试了使用一小部分数据集进行测试，此种情况下自己可以对数据集进行判断，并没有提示任何错误。
三：当在命令行里直接用命令转化文件格式时抛出如下错误：
ERROR common.AbstractJob: Unexpected --seqFileDir while processing Job-Specific Options

注：转化命令为：bin/mahout clusterdump --seqFileDir /home/thinkgamer/document/canopy/output/clusters-0-final/ --pointsDir /home/thinkgamer/document/canopy/output/clusteredPoints/ --output /home/thinkgamer/document/canopy/clusteranalyze.txt
上网搜了搜热心的网友给出的解决办法是：将--seqFileDir换成--input即可

Mahout学习之Mahout简介、安装、配置、入门程序测试

一、Mahout简介
查了Mahout的中文意思——驭象的人，再看看Mahout的logo，好吧，想和小黄象happy地玩耍，得顺便陪陪这位驭象人耍耍了...
附logo：



（就是他，骑在象头上的那个Mahout）  



步入正文啦：

       Mahout 是一个很强大的数据挖掘工具，是一个分布式机器学习算法的集合，包括：被称为Taste的分布式协同过滤的实现、分类、聚类等。Mahout最大的优点就是基于hadoop实现，把很多以前运行于单机上的算法，转化为了MapReduce模式，这样大大提升了算法可处理的数据量和处理性能。



在Mahout实现的机器学习算法：





算法类


算法名


中文名




分类算法


Logistic Regression


逻辑回归




Bayesian


贝叶斯




SVM


支持向量机




Perceptron


感知器算法




Neural Network


神经网络




Random Forests


随机森林




Restricted Boltzmann Machines


有限波尔兹曼机




聚类算法


Canopy Clustering


Canopy聚类




K-means Clustering


K均值算法




Fuzzy K-means


模糊K均值




Expectation Maximization


EM聚类（期望最大化聚类）




Mean Shift Clustering


均值漂移聚类




Hierarchical Clustering


层次聚类




Dirichlet Process Clustering


狄里克雷过程聚类




Latent Dirichlet Allocation


LDA聚类




Spectral Clustering


谱聚类




关联规则挖掘


Parallel FP Growth Algorithm


并行FP Growth算法




回归


Locally Weighted Linear Regression


局部加权线性回归




降维/维约简


Singular Value Decomposition


奇异值分解




Principal Components Analysis


主成分分析




Independent Component Analysis


独立成分分析




Gaussian Discriminative Analysis


高斯判别分析




进化算法


并行化了Watchmaker框架


 




推荐/协同过滤


Non-distributed recommenders


Taste(UserCF, ItemCF, SlopeOne）




Distributed Recommenders


ItemCF




向量相似度计算


RowSimilarityJob


计算列间相似度




VectorDistanceJob


计算向量间距离




非Map-Reduce算法


Hidden Markov Models


隐马尔科夫模型




集合方法扩展


Collections


扩展了java的Collections类






二、Mahout安装、配置 

一、下载Mahout
http://archive.apache.org/dist/mahout/


二、解压
tar -zxvf mahout-distribution-0.9.tar.gz


三、配置环境变量
3.1、配置Mahout环境变量

# set mahout environment
export MAHOUT_HOME=/home/yujianxin/mahout/mahout-distribution-0.9
export MAHOUT_CONF_DIR=$MAHOUT_HOME/conf
export PATH=$MAHOUT_HOME/conf:$MAHOUT_HOME/bin:$PATH

3.2、配置Mahout所需的Hadoop环境变量

 # set hadoop environment
export HADOOP_HOME=/home/yujianxin/hadoop/hadoop-1.1.2 
export HADOOP_CONF_DIR=$HADOOP_HOME/conf 
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_HOME_WARN_SUPPRESS=not_null




四、验证Mahout是否安装成功
        执行命令mahout。若列出一些算法，则成功，如图：
         

       

五、使用Mahout 之入门级使用
5.1、启动Hadoop
5.2、下载测试数据
           
http://archive.ics.uci.edu/ml/databases/synthetic_control/链接中的synthetic_control.data
5.3、上传测试数据

hadoop fs -put synthetic_control.data /user/root/testdata
5.4  使用Mahout中的kmeans聚类算法，执行命令：

mahout -core  org.apache.mahout.clustering.syntheticcontrol.kmeans.Job
花费9分钟左右完成聚类 。 



5.5 查看聚类结果
    执行hadoop fs -ls /user/root/output，查看聚类结果。 









齐活，收工。Mahout继续学习中......

