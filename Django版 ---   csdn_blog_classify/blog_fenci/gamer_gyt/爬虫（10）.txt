python模拟登录网易邮箱

python模拟登录网易邮箱


推荐一篇不错的文章：http://www.pythonclub.org/python-network-application/observer-spider


python模拟浏览器登录人人网，并使用代理IP和发送表单数据

Python模拟登录人人网，并使用代理IP



上述代码中同时使用了代理ip和模拟登录，若不使用代理ip只需修改如下


为：

即可


python模拟用户登录某某网

python模拟用户登录某某网：





python爬虫爬取csdn博客专家所有博客内容

python爬虫爬取csdn博客专家所有博客内容：

全部过程采取自动识别与抓取，抓取结果是将一个博主的所有 文章存放在以其名字命名的文件内，代码如下





结果如下：





Python爬取CSDN博客专家系列——移动开发

注明：小编亲测，只要把第一部分里面的url修改，即可抓取博客专家里边所有的专家的博客内容和标题，后续小编还会对此代码改进，敬请期待

文章分为两部分：Python爬虫爬取移动开发专家的姓名和博客首页地址，爬取每个专家的所有博客存放在已该专家名字命名的txt文件中
说明：本爬虫主要是采用BeautifulSoup和少量的正则匹配，在第一部分抓取完毕后需要将文件格式改为ANSI，代码如下：
第一部分：

第二部分：


结果如下图：




新浪明星日志推荐系统——爬虫爬取数据（2）

由于之前的改造，现在将爬虫的功能做了一些改变，具体实现的功能是将推荐的日志全部抓取下来，并存放在以文章标题命名的文件中，代码如下：

import urllib
import os,re
import sys
from bs4 import BeautifulSoup
reload(sys)
sys.setdefaultencoding("utf-8")


def if_str(str_t):
if re.search(r"^.*[a-zA-Z].*",str_t)== None:
print " saf"


def get_blog(url):
page = urllib.urlopen(url).read()
if len(page)!=0:
if BeautifulSoup(page).title != None:
str_title = BeautifulSoup(page).title.string #获取title名称，并作为文件名称
if re.search(r"^.*[a-zA-Z|\s\"，<>].*",str_title) == None:

fp = file("%s.txt" % str_title,"w")
page_js = r"<!-- 正文开始 -->[\s\S]*<!-- 正文结束 -->" # 正则匹配文章正文部分
if re.search(page_js,page):

soup = BeautifulSoup(re.search(page_js,page).group(0),from_encoding="gb18030")
for div in soup.find_all("div"):
fp.write(div.get_text().lstrip())
fp.close()


if "__main__"==__name__:
i = 1
if i<7:
for j in range(1,140):
url = "http://roll.ent.sina.com.cn/blog/star/index_" + str(i) +".shtml"
fp = file("EveryPageHref.txt","a")
fp.write(url)
fp.write("\n")
fp.close()

i+=1

page = urllib.urlopen(url).read()
soup = BeautifulSoup(page,from_encoding = "gb18030")
list_ul = soup.find_all("ul",class_="list_009")
list_li = list_ul[0].find_all("li")

for li in list_li:
l 
ist_a = li.find_all("a")
one_link = list_a[1].get("href") #获取连接
str_title = list_a[0].get_text()
if one_link != "http://blog.sina.com.cn/s/blog_4a6c545e0102vgwe.html":
get_blog(one_link)

print "OK!"
另外附上一张成果图：

新浪明星日志推荐系统——爬虫爬取数据（1）

今天有了一个想法，想自己用Python写一个新浪明星日志推荐系统
 那么第一步要完成的工作就是获得新浪明星日志的数据，于是自己写了一个爬虫，实现的功能是爬取新浪明星日志的作者，推荐的文章链接，以及作者日志列表或者首页链接，具体程序如下：


# -*- coding: utf-8 -*-
"""
Created on Wed May 20 13:55:00 2015


@author: Administrator
"""
import urllib
import os,re
import sys
from bs4 import BeautifulSoup
reload(sys)
sys.setdefaultencoding("utf-8")




if "__main__"==__name__:
i = 1
for j in range(1,140):
url = "http://roll.ent.sina.com.cn/blog/star/index_" + str(i) +".shtml"
fp = file("EveryPageHref.txt","a")
fp.write(url)
fp.write("\n")
fp.close()

i+=1

page = urllib.urlopen(url).read()
soup = BeautifulSoup(page,from_encoding = "gb18030")
list_ul = soup.find_all("ul",class_="list_009")
list_li = list_ul[0].find_all("li")
for li in list_li:
list_a = li.find_all("a")
one_link = list_a[1].get("href") #获取连接
print list_a[0].get_text()
print one_link
if len(one_link)>10:
page = urllib.urlopen(one_link).read()
if len(page)!=0:
href=r'<a class="on" href=.*?>'
link = re.findall(href,page,re.M|re.S)
if link:
a_soup = BeautifulSoup(link[0],from_encoding= "gb18030")
a_href = a_soup.find_all('a')
href = a_href[0].get('href')
print a_href[0].get('href')

fp = file("title.txt","a")
fp.write(list_a[0].get_text())
fp.write("\n")
fp.write(one_link)
fp.write("\n")
fp.write(href)
fp.write("\n")
fp.close()
else:
pass 

print "OK!"



python 爬虫爬取腾讯新闻科技类的企鹅智酷系列（1）

废话不多说，直接贴代码，主要采用BeautifulSoup写的

# -*- coding: utf-8 -*-
"""
Created on Mon May 18 19:12:06 2015


@author: Administrator
"""


import urllib
import os
from bs4 import BeautifulSoup
import sys
reload(sys)
sys.setdefaultencoding("utf-8")


i = 0 
j = 0
list_a = []


def gettext(href):
global j,list_a
page = urllib.urlopen(href).read()
soup = BeautifulSoup(page,from_encoding="gb18030")
div = soup.find_all("div",class_="content")
p_text = div[0].find_all("p")
for p in p_text:
fp = file("%s.txt" % list_a[j],"a")
fp.write(' ')
fp.write(p.get_text())
fp.write(" \n")
j+=1


def gethref(url): #获得所有链接
global i,list_a
fp = file("AllTitle.txt","w+")
page = urllib.urlopen(url).read()
soup = BeautifulSoup(page,from_encoding="gb18030")
ul = soup.find_all("ul",class_="row1")
li = ul[0].find_all("li")
for lia in li:
list_a.append(("%s、" % (i+1))+lia.h3.get_text())
href = lia.a.get('href')
# 将标题简介和链接有规则的写入文件中
fp.write("%s、" % (i+1))
i+=1
fp.write("标题：")
fp.write(lia.h3.get_text())
fp.write("\n 简介：")
fp.write(lia.p.get_text())
fp.write("\n 链接：")
fp.write(lia.a.get("href"))
fp.write("\n")

gettext(href)


if "__main__"==__name__:
url ="http://re.qq.com/biznext/zkht.htm"

gethref(url)
print "All Is OK!"


BeautifulSoup中解决乱码问题

由于初步学习Python爬取网页文本内容，在存储文本时出现乱码问题
我的代码如下：

import urllib
from bs4 import BeautifulSoup
import sys
reload(sys)
sys.setdefaultencoding('utf-8')


fp = file("test.txt","wb+")
page=urllib.urlopen('http://tech.qq.com/a/20150518/031741.htm').read()
soup = BeautifulSoup(page)
div = soup.find_all('div',id="Cnt-Main-Article-QQ")
pp = div[0].find_all('p')
for p in pp:
fp.write(p.get_text())


print "Write Over!!!"
fp.flush()
fp.close()


但打开文本时内容为乱码，上网搜索了好多资料，找到一篇文章，非常简洁的而又完美的解决乱码问题
方法如下：
将soup = BeautifulSoup(page)改成soup=BeautifulSoup(page,from_encoding"gb18030")


当你再次打开文本时会惊奇的发现不会乱码了

Python爬虫抓取图片，网址从文件中读取



利用python抓取网络图片的步骤：

1.根据给定的网址获取网页源代码

2.利用正则表达式把源代码中的图片地址过滤出来

3.根据过滤出来的图片地址下载网络图片


import urllib
import re
import os
                                                            #urllib,re,os均为Python模块
def gethtml(outline):
page = urllib.urlopen(outline)              #抓取网页内容获得图片链接
html = page.read()
return html

def getimg(html):                                #下载图片保存在同目录下的pictures文件夹下
reg=r'src="(.+?\.jpg)" pic_ext'
imgre=re.compile(reg)
imglist=imgre.findall(html)
if not imglist:
print "not found"
else:
filepath=os.getcwd() +'\pictures'
print filepath
if os.path.exists(filepath) is False:
os.mkdir(filepath)
global x
for imgurl in imglist:
temp = filepath + '\%s.jpg' % x
print imgurl
urllib.urlretrieve(imgurl,temp)
x=x+1

x = 0
fp =file("img_path.txt")                          #所有网址都放在这个文件里
while True:
outline = fp.readline().strip('\n')
if len(outline)==0:
break
print outline
html=gethtml(outline)
getimg(html)

fp.close()



