hbase遇到问题及解决方法
hbase遇到问题及解决方法
1.zookeeper启动报错
错误日志
启动zookeeper报错信息如下：
java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)
        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)
        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)
        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)
2015-05-19 10:26:26,983 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@849] - Notification time out: 800
解决方法
此问题产生的主要原因是因为zookeeper集群未关闭防火墙。 
执行下面命令后仍然报上面的错误： 
systemctl start iptables.service 
经过仔细查找后发现，CentOS 7.0默认使用的是firewall作为防火墙，需要执行如下命令关闭防火墙: 
systemctl stop firewalld.service  #停止firewall 
systemctl disable firewalld.service  #禁止firewall开机启动 
关闭各个节点防火墙后，重启zookeeper进程，就可以解决上述问题了。
2.RegionServer进程挂掉
错误日志
Caused by: java.io.IOException: Couldn't set up IO streams
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:786)
    at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
    at org.apache.hadoop.ipc.Client.call(Client.java:1438)
    ... 60 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
    at java.lang.Thread.start0(Native Method)
    at java.lang.Thread.start(Thread.java:713)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)

解决方法
/etc/security/limits.conf
/etc/security/limits.d/90-nproc.conf
*       hard    nproc   65536
*       soft    nproc   65536
*       hard    nofile  65536
*       soft    nofile  65536
3.增加thrift server的线程数
日志信息
2015-06-05 12:46:37,756 INFO  [thrift-worker-6] client.AsyncProcess: #79, waiting for 72000  actions to finish
2015-06-05 12:46:37,756 INFO  [thrift-worker-9] client.AsyncProcess: #79, waiting for 48908  actions to finish
2015-06-05 12:46:37,855 INFO  [thrift-worker-8] client.AsyncProcess: #79, waiting for 72000  actions to finish
2015-06-05 12:46:38,198 INFO  [thrift-worker-2] client.AsyncProcess: #1, waiting for 78000  actions to finish
2015-06-05 12:46:38,762 INFO  [thrift-worker-13] client.AsyncProcess: #79, waiting for 72000  actions to finish
2015-06-05 12:46:39,547 INFO  [thrift-worker-0] client.AsyncProcess: #17, waiting for 78000  actions to finish
2015-06-05 12:47:55,612 INFO  [thrift-worker-9] client.AsyncProcess: #79, waiting for 108000  actions to finish
2015-06-05 12:47:55,912 INFO  [thrift-worker-6] client.AsyncProcess: #79, waiting for 114000  actions to finish
解决方法
增加thriftServer线程数
hbase-daemon.sh start thrift --threadpool -m 200 -w 500
在hbase_home目录下的logs目录中可以看到启动日志信息如下：
INFO  [main] thrift.ThriftServerRunner: starting TBoundedThreadPoolServer on /0.0.0.0:9090; min worker threads=200, max worker threads=500, max queued requests=1000

hbase-daemon.sh start thrift –threadpool 
                       -m

4. zookeeper使用内存大的问题
日志信息

jps 查看 QuorumPeerMain 进程IP
jmap  -heap PID 查看进程使用内存情况,具体情况如下：

Attaching to process ID 6801, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.71-b01

using thread-local object allocation.
Parallel GC with 18 thread(s)

Heap Configuration:
   MinHeapFreeRatio = 0
   MaxHeapFreeRatio = 100
   MaxHeapSize      = 32126271488 (30638.0MB)
   NewSize          = 1310720 (1.25MB)
   MaxNewSize       = 17592186044415 MB
   OldSize          = 5439488 (5.1875MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 21757952 (20.75MB)
   MaxPermSize      = 85983232 (82.0MB)
   G1HeapRegionSize = 0 (0.0MB)

Heap Usage:
PS Young Generation
Eden Space:
   capacity = 537919488 (513.0MB)
   used     = 290495976 (277.0385513305664MB)
   free     = 247423512 (235.9614486694336MB)
   54.00361624377513% used
From Space:
   capacity = 89128960 (85.0MB)
   used     = 0 (0.0MB)
   free     = 89128960 (85.0MB)
   0.0% used
To Space:
   capacity = 89128960 (85.0MB)
   used     = 0 (0.0MB)
   free     = 89128960 (85.0MB)
   0.0% used
PS Old Generation
   capacity = 1431306240 (1365.0MB)
   used     = 0 (0.0MB)
   free     = 1431306240 (1365.0MB)
   0.0% used
PS Perm Generation
   capacity = 22020096 (21.0MB)
   used     = 9655208 (9.207923889160156MB)
   free     = 12364888 (11.792076110839844MB)
   43.847256615048366% used

3259 interned Strings occupying 265592 bytes.
解决方法
方法一
Heap Configuration配置中我们可以看到，配置的heap内存很大，现在我们修改zkServer.sh脚本减小MaxHeapSize，具体步骤如下： 
* 打开zookeeper安装目录下bin文件夹中的zkServer.sh 
* 在 zkServer.sh文件的49行处加入 JVMPARAM="-Xms1000M -Xmx1000M -Xmn512M" 
* 然后修改zkServer.sh 109~110行出的内容。
修改前如下：
nohup "$JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}" \
 -cp "$CLASSPATH" $JVMFLAGS $ZOOMAIN "$ZOOCFG" > "$_ZOO_DAEMON_OUT" 2>&1 < /dev/null &
修改后如下(将在49上添加的JVMPARAM参数项添加在JVMFLAGS后面)：
 nohup "$JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}" \
-cp "$CLASSPATH" $JVMFLAGS $JVMPARAM $ZOOMAIN "$ZOOCFG" > "$_ZOO_DAEMON_OUT" 2>&1 < /dev/null &

重启zookeeper进程
jmap  -heap PID 查看修改后进程使用内存情况：

Attaching to process ID 6207, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.71-b01

using thread-local object allocation.
Parallel GC with 18 thread(s)

Heap Configuration:
   MinHeapFreeRatio = 0
   MaxHeapFreeRatio = 100
   MaxHeapSize      = 1048576000 (1000.0MB)
   NewSize          = 536870912 (512.0MB)
   MaxNewSize       = 536870912 (512.0MB)
   OldSize          = 5439488 (5.1875MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 21757952 (20.75MB)
   MaxPermSize      = 85983232 (82.0MB)
   G1HeapRegionSize = 0 (0.0MB)

Heap Usage:
PS Young Generation
Eden Space:
   capacity = 402653184 (384.0MB)
   used     = 104690912 (99.84103393554688MB)
   free     = 297962272 (284.1589660644531MB)
   26.000269254048664% used
From Space:
   capacity = 67108864 (64.0MB)
   used     = 0 (0.0MB)
   free     = 67108864 (64.0MB)
   0.0% used
To Space:
   capacity = 67108864 (64.0MB)
   used     = 0 (0.0MB)
   free     = 67108864 (64.0MB)
   0.0% used
PS Old Generation
   capacity = 511705088 (488.0MB)
   used     = 0 (0.0MB)
   free     = 511705088 (488.0MB)
   0.0% used
PS Perm Generation
   capacity = 22020096 (21.0MB)
   used     = 8878832 (8.467514038085938MB)
   free     = 13141264 (12.532485961914062MB)
   40.321495419456845% used

2697 interned Strings occupying 216760 bytes.
方法二
打开 zookeeper/bin/zkEnv.sh 文件,在zkEvn.sh中49~52行处有如下内容：
if [ -f "$ZOOCFGDIR/java.env" ]
then
    . "$ZOOCFGDIR/java.env"
fi
该文件已经明确说明有独立JVM内存的设置文件，路径是zookeeper/conf/java.env 
安装的时候这个路径下没有有java.env文件，需要自己新建一个： 
* vim  java.env 
* java.env文件内容如下：
#!/bin/sh
# heap size MUST be modified according to cluster environment
export JVMFLAGS="-Xms512m -Xmx1024m $JVMFLAGS"

重启zookeeper进程
jmap  -heap PID 查看修改后进程使用内存情况：

[hadoop@hadoop202 conf]$ jmap -heap  10151
Attaching to process ID 10151, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.71-b01

using thread-local object allocation.
Parallel GC with 18 thread(s)

Heap Configuration:
   MinHeapFreeRatio = 0
   MaxHeapFreeRatio = 100
   MaxHeapSize      = 1073741824 (1024.0MB)
   NewSize          = 1310720 (1.25MB)
   MaxNewSize       = 17592186044415 MB
   OldSize          = 5439488 (5.1875MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 21757952 (20.75MB)
   MaxPermSize      = 85983232 (82.0MB)
   G1HeapRegionSize = 0 (0.0MB)

Heap Usage:
PS Young Generation
Eden Space:
   capacity = 135266304 (129.0MB)
   used     = 43287608 (41.28227996826172MB)
   free     = 91978696 (87.71772003173828MB)
   32.00176741725715% used
From Space:
   capacity = 22020096 (21.0MB)
   used     = 0 (0.0MB)
   free     = 22020096 (21.0MB)
   0.0% used
To Space:
   capacity = 22020096 (21.0MB)
   used     = 0 (0.0MB)
   free     = 22020096 (21.0MB)
   0.0% used
PS Old Generation
   capacity = 358088704 (341.5MB)
   used     = 0 (0.0MB)
   free     = 358088704 (341.5MB)
   0.0% used
PS Perm Generation
   capacity = 22020096 (21.0MB)
   used     = 8887040 (8.475341796875MB)
   free     = 13133056 (12.524658203125MB)
   40.358770461309526% used

2699 interned Strings occupying 217008 bytes.

5.hosts文件导致hbase集群无法启动
错误日志
hmaster 错误日志
2015-08-27 02:23:40,681 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 3149329 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2015-08-27 02:23:42,186 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 3150833 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2015-08-27 02:23:43,690 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 3152338 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2015-08-27 02:23:45,195 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 3153843 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2015-08-27 02:23:46,699 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 3155347 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2015-08-27 02:23:48,204 INFO  [hadoop115:16020.activeMasterManager] master.ServerManager: Waiting f
regionserver 
2015-08-27 14:21:22,770 WARN  [regionserver/hadoop116/127.0.0.1:16020] regionserver.HRegionServer: error telling master we are up
com.google.protobuf.ServiceException: java.net.SocketException: Invalid argument
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:231)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:300)
        at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerStartup(RegionServerStatusProtos.java:8277)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:2132)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:826)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Invalid argument
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:465)
        at sun.nio.ch.Net.connect(Net.java:457)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupConnection(RpcClientImpl.java:403)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:709)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:880)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:849)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1173)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:216)
解决方法
检查/etc/hosts文件,如下文,产生此问题的原因由hadoop116引起:

127.0.0.1  hadoop116 localhost.localdomain localhost4 localhost4.localdomain4

改成如下内容后重启集群,问题解决

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4

此问题在stackoverflow中有这样的描述: 
check your /etc/hosts file,if there is something like

127.0.0.1 localhost yourhost

change it to

127.0.0.1 localhost 
  192.168.1.1 yourhost

HBase RegionServer: error telling master we are up 

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Spark入门实战系列--6.SparkSQL（上）--SparkSQL简介
【注】该系列文章以及使用到安装包/测试数据 可以在《倾情大奉送–Spark入门实战系列》获取
1 SparkSQL的发展历程
1.1   Hive and Shark
SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，Hive应运而生，它是当时唯一运行在Hadoop上的SQL-on-Hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，大量的SQL-on-Hadoop工具开始产生，其中表现较为突出的是：

MapR的Drill
Cloudera的Impala
Shark

其中Shark是伯克利实验室Spark生态环境的组件之一，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在Spark引擎上，从而使得SQL查询的速度得到10-100倍的提升。

1.2   Shark和SparkSQL  
但是，随着Spark的发展，对于野心勃勃的Spark团队来说，Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等），制约了Spark的One Stack Rule Them All的既定方针，制约了Spark各个组件的相互集成，所以提出了SparkSQL项目。SparkSQL抛弃原有Shark的代码，汲取了Shark的一些优点，如内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码；由于摆脱了对Hive的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。

数据兼容方面  不但兼容Hive，还可以从RDD、parquet文件、JSON文件中获取数据，未来版本甚至支持获取RDBMS数据以及cassandra等NOSQL数据；
性能优化方面  除了采取In-Memory Columnar Storage、byte-code generation等优化技术外、将会引进Cost Model对查询进行动态评估、获取最佳物理计划等等；
组件扩展方面  无论是SQL的语法解析器、分析器还是优化器都可以重新定义，进行扩展。

2014年6月1日Shark项目和SparkSQL项目的主持人Reynold Xin宣布：停止对Shark的开发，团队将所有资源放SparkSQL项目上，至此，Shark的发展画上了句话，但也因此发展出两个直线：SparkSQL和Hive on Spark。

其中SparkSQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive；而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎。
1.3   SparkSQL的性能
Shark的出现，使得SQL-on-Hadoop的性能比Hive有了10-100倍的提高：

那么，摆脱了Hive的限制，SparkSQL的性能又有怎么样的表现呢？虽然没有Shark相对于Hive那样瞩目地性能提升，但也表现得非常优异：

为什么SparkSQL的性能会得到怎么大的提升呢？主要SparkSQL在下面几点做了优化：
A：内存列存储（In-Memory Columnar Storage）
SparkSQL的表数据在内存中存储不是采用原生态的JVM对象存储方式，而是采用内存列存储，如下图所示。

该存储方式无论在空间占用量和读取吞吐率上都占有很大优势。 
对于原生态的JVM对象存储方式，每个对象通常要增加12-16字节的额外开销，对于一个270MB的TPC-H lineitem table数据，使用这种方式读入内存，要使用970MB左右的内存空间（通常是2～5倍于原生数据空间）；另外，使用这种方式，每个数据记录产生一个JVM对象，如果是大小为200B的数据记录，32G的堆栈将产生1.6亿个对象，这么多的对象，对于GC来说，可能要消耗几分钟的时间来处理（JVM的垃圾收集时间与堆栈中的对象数量呈线性相关）。显然这种内存存储方式对于基于内存计算的Spark来说，很昂贵也负担不起。 
对于内存列存储来说，将所有原生数据类型的列采用原生数组来存储，将Hive支持的复杂数据类型（如array、map等）先序化后并接成一个字节数组来存储。这样，每个列创建一个JVM对象，从而导致可以快速的GC和紧凑的数据存储；额外的，还可以使用低廉CPU开销的高效压缩方法（如字典编码、行长度编码等压缩方法）降低内存开销；更有趣的是，对于分析查询中频繁使用的聚合特定列，性能会得到很大的提高，原因就是这些列的数据放在一起，更容易读入内存进行计算。
B：字节码生成技术（bytecode generation，即CG）
在数据库查询中有一个昂贵的操作是查询语句中的表达式，主要是由于JVM的内存模型引起的。比如如下一个查询：
SELECT a + b FROM table 
在这个查询里，如果采用通用的SQL语法途径去处理，会先生成一个表达式树（有两个节点的Add树，参考后面章节），在物理处理这个表达式树的时候，将会如图所示的7个步骤： 
1.调用虚函数Add.eval()，需要确认Add两边的数据类型 
2.调用虚函数a.eval()，需要确认a的数据类型 
3.确定a的数据类型是Int，装箱 
4.调用虚函数b.eval()，需要确认b的数据类型 
5.确定b的数据类型是Int，装箱 
6.调用Int类型的Add 
7.返回装箱后的计算结果 
其中多次涉及到虚函数的调用，虚函数的调用会打断CPU的正常流水线处理，减缓执行。 
Spark1.1.0在catalyst模块的expressions增加了codegen模块，如果使用动态字节码生成技术（配置spark.sql.codegen参数），SparkSQL在执行物理计划的时候，对匹配的表达式采用特定的代码，动态编译，然后运行。如上例子，匹配到Add方法： 
 
然后，通过调用，最终调用：

最终实现效果类似如下伪代码：
val a: Int = inputRow.getInt(0)
val b: Int = inputRow.getInt(1)
val result: Int = a + b
resultRow.setInt(0, result)
对于Spark1.1.0，对SQL表达式都作了CG优化，具体可以参看codegen模块。CG优化的实现主要还是依靠scala2.10的运行时放射机制（runtime reflection）。对于SQL查询的CG优化，可以简单地用下图来表示：

C：Scala代码优化
另外，SparkSQL在使用Scala编写代码的时候，尽量避免低效的、容易GC的代码；尽管增加了编写代码的难度，但对于用户来说，还是使用统一的接口，没受到使用上的困难。下图是一个Scala代码优化的示意图：

2 SparkSQL运行架构
类似于关系型数据库，SparkSQL也是语句也是由Projection（a1，a2，a3）、Data Source（tableA）、Filter（condition）组成，分别对应sql查询过程中的Result、Data Source、Operation，也就是说SQL语句按Result–>Data Source–>Operation的次序来描述的。
 
当执行SparkSQL语句的顺序为： 
1.对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范； 
2.将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的； 
3.一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）； 
4.计划执行（Execute），按Operation–>Data Source–>Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。
2.1   Tree和Rule
SparkSQL对SQL语句的处理和关系型数据库对SQL语句的处理采用了类似的方法，首先会将SQL语句进行解析（Parse），然后形成一个Tree，在后续的如绑定、优化等处理过程都是对Tree的操作，而操作的方法是采用Rule，通过模式匹配，对不同类型的节点采用不同的操作。在整个sql语句的处理过程中，Tree和Rule相互配合，完成了解析、绑定（在SparkSQL中称为Analysis）、优化、物理计划等过程，最终生成可以执行的物理计划。
2.1.1 Tree

Tree的相关代码定义在sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees
Logical Plans、Expressions、Physical Operators都可以使用Tree表示
Tree的具体操作是通过TreeNode来实现的
SparkSQL定义了catalyst.trees的日志，通过这个日志可以形象的表示出树的结构
TreeNode可以使用scala的集合操作方法（如foreach, map, flatMap, collect等）进行操作
有了TreeNode，通过Tree中各个TreeNode之间的关系，可以对Tree进行遍历操作，如使用transformDown、transformUp将Rule应用到给定的树段，然后用结果替代旧的树段；也可以使用transformChildrenDown、transformChildrenUp对一个给定的节点进行操作，通过迭代将Rule应用到该节点以及子节点。
TreeNode可以细分成三种类型的Node：
UnaryNode 一元节点，即只有一个子节点。如Limit、Filter操作
BinaryNode 二元节点，即有左右子节点的二叉节点。如Jion、Union操作
LeafNode 叶子节点，没有子节点的节点。主要用户命令类操作，如SetCommand

2.1.2 Rule

Rule的相关代码定义在sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules
Rule在SparkSQL的Analyzer、Optimizer、SparkPlan等各个组件中都有应用到
Rule是一个抽象类，具体的Rule实现是通过RuleExecutor完成
Rule通过定义batch和batchs，可以简便的、模块化地对Tree进行transform操作
Rule通过定义Once和FixedPoint，可以对Tree进行一次操作或多次操作（如对某些Tree进行多次迭代操作的时候，达到FixedPoint次数迭代或达到前后两次的树结构没变化才停止操作，具体参看RuleExecutor.apply）

2.2   sqlContext和hiveContext的运行过程
SparkSQL有两个分支，sqlContext和hiveContext，sqlContext现在只支持SQL语法解析器（SQL-92语法）；hiveContext现在支持SQL语法解析器和hivesql语法解析器，默认为hiveSQL语法解析器，用户可以通过配置切换成SQL语法解析器，来运行hiveSQL不支持的语法，
2.2.1 sqlContext的运行过程
sqlContext总的一个过程如下图所示： 
1.SQL语句经过SqlParse解析成UnresolvedLogicalPlan； 
2.使用analyzer结合数据数据字典（catalog）进行绑定，生成resolvedLogicalPlan； 
3.使用optimizer对resolvedLogicalPlan进行优化，生成optimizedLogicalPlan； 
4.使用SparkPlan将LogicalPlan转换成PhysicalPlan； 
5使用prepareForExecution()将PhysicalPlan转换成可执行物理计划； 
6.使用execute()执行可执行物理计划； 
7.生成SchemaRDD。 
在整个运行过程中涉及到多个SparkSQL的组件，如SqlParse、analyzer、optimizer、SparkPlan等等

2.2.2 hiveContext的运行过程
hiveContext总的一个过程如下图所示： 
1.SQL语句经过HiveQl.parseSql解析成Unresolved LogicalPlan，在这个解析过程中对hiveql语句使用getAst()获取AST树，然后再进行解析； 
2.使用analyzer结合数据hive源数据Metastore（新的catalog）进行绑定，生成resolved LogicalPlan； 
3.使用optimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan，优化前使用了ExtractPythonUdfs(catalog.PreInsertionCasts(catalog.CreateTables(analyzed)))进行预处理； 
4.使用hivePlanner将LogicalPlan转换成PhysicalPlan； 
5.使用prepareForExecution()将PhysicalPlan转换成可执行物理计划； 
6.使用execute()执行可执行物理计划； 
7.执行后，使用map(_.copy)将结果导入SchemaRDD。

2.3   catalyst优化器
SparkSQL1.1总体上由四个模块组成：core、catalyst、hive、hive-Thriftserver：

core处理数据的输入输出，从不同的数据源获取数据（RDD、Parquet、json等），将查询结果输出成schemaRDD；
catalyst处理查询语句的整个处理过程，包括解析、绑定、优化、物理计划等，说其是优化器，还不如说是查询引擎；
hive对hive数据的处理
hive-ThriftServer提供CLI和JDBC/ODBC接口

在这四个模块中，catalyst处于最核心的部分，其性能优劣将影响整体的性能。由于发展时间尚短，还有很多不足的地方，但其插件式的设计，为未来的发展留下了很大的空间。下面是catalyst的一个设计图：
 
其中虚线部分是以后版本要实现的功能，实线部分是已经实现的功能。从上图看，catalyst主要的实现组件有：

sqlParse，完成sql语句的语法解析功能，目前只提供了一个简单的sql解析器；
Analyzer，主要完成绑定工作，将不同来源的Unresolved LogicalPlan和数据元数据（如hive metastore、Schema catalog）进行绑定，生成resolved LogicalPlan；
optimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan；
Planner将LogicalPlan转换成PhysicalPlan；
CostModel，主要根据过去的性能统计数据，选择最佳的物理执行计划 
这些组件的基本实现方法：
先将sql语句通过解析生成Tree，然后在不同阶段使用不同的Rule应用到Tree上，通过转换完成各个组件的功能。
Analyzer使用Analysis Rules，配合数据元数据（如hive metastore、Schema catalog），完善Unresolved LogicalPlan的属性而转换成resolved LogicalPlan；
optimizer使用Optimization Rules，对resolved LogicalPlan进行合并、列裁剪、过滤器下推等优化作业而转换成optimized LogicalPlan；
Planner使用Planning Strategies，对optimized LogicalPlan

3 SparkSQL CLI
CLI（Command-Line Interface，命令行界面）是指可在用户提示符下键入可执行指令的界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后予以执行。Spark CLI指的是使用命令界面直接输入SQL命令，然后发送到Spark集群进行执行，在界面中显示运行过程和最终的结果。 
Spark1.1相较于Spark1.0最大的差别就在于Spark1.1增加了Spark SQL CLI和ThriftServer，使得Hive用户还有用惯了命令行的RDBMS数据库管理员较容易地上手，真正意义上进入了SQL时代。 
**【注】**Spark CLI和Spark Thrift Server实验环境为第二课《Spark编译与部署（下）–Spark编译安装》所搭建
3.1   运行环境说明
3.1.1 硬软件环境

主机操作系统：Windows 64位，双核4线程，主频2.2G，10G内存
虚拟软件：VMware® Workstation 9.0.0 build-812388
虚拟机操作系统：CentOS 64位，单核
虚拟机运行环境： 
JDK：1.7.0_55 64位
Hadoop：2.2.0（需要编译为64位）
Scala：2.11.4
Spark：1.1.0（需要编译）
Hive：0.13.1

3.1.2 机器网络环境
集群包含三个节点，节点之间可以免密码SSH访问，节点IP地址和主机名分布如下：

3.2   配置并启动
3.2.1 创建并配置hive-site.xml
在运行Spark SQL CLI中需要使用到Hive Metastore，故需要在Spark中添加其uris。具体方法是在SPARK_HOME/conf目录下创建hive-site.xml文件，然后在该配置文件中，添加hive.metastore.uris属性，具体如下：
<configuration>  
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://hadoop1:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  </property>
</configuration>

3.2.2 启动Hive
在使用Spark SQL CLI之前需要启动Hive Metastore（如果数据存放在HDFS文件系统，还需要启动Hadoop的HDFS），使用如下命令可以使Hive Metastore启动后运行在后台，可以通过jobs查询：
$nohup hive --service metastore > metastore.log 2>&1 &

3.2.3 启动Spark集群和Spark SQL CLI
通过如下命令启动Spark集群和Spark SQL CLI：
$cd /app/hadoop/spark-1.1.0
$sbin/start-all.sh
$bin/spark-sql --master spark://hadoop1:7077 --executor-memory 1g
在集群监控页面可以看到启动了SparkSQL应用程序： 

这时就可以使用HQL语句对Hive数据进行查询，另外可以使用COMMAND，如使用set进行设置参数：默认情况下，SparkSQL Shuffle的时候是200个partition，可以使用如下命令修改该参数：
SET spark.sql.shuffle.partitions=20;
运行同一个查询语句，参数改变后，Task（partition）的数量就由200变成了20。 

3.2.4 命令参数
通过bin/spark-sql –help可以查看CLI命令参数：
 

其中[options] 是CLI启动一个SparkSQL应用程序的参数，如果不设置–master的话，将在启动spark-sql的机器以local方式运行，只能通过http://机器名:4040进行监控；这部分参数，可以参照Spark1.0.0 应用程序部署工具spark-submit 的参数。 
[cli option]是CLI的参数，通过这些参数CLI可以直接运行SQL文件、进入命令行运行SQL命令等等，类似以前的Shark的用法。需要注意的是CLI不是使用JDBC连接，所以不能连接到ThriftServer；但可以配置conf/hive-site.xml连接到Hive的Metastore，然后对Hive数据进行查询。
3.3   实战Spark SQL CLI
3.3.1 获取订单每年的销售单数、销售总额
第一步 设置任务个数，在这里修改为20个
spark-sql>SET spark.sql.shuffle.partitions=20;

第二步 运行SQL语句
spark-sql>use hive;

spark-sql>select c.theyear,count(distinct a.ordernumber),sum(b.amount) from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber join tbDate c on a.dateid=c.dateid group by c.theyear order by c.theyear;

第三步 查看运行结果


3.3.2 计算所有订单每年的总金额
第一步 执行SQL语句
spark-sql>select c.theyear,count(distinct a.ordernumber),sum(b.amount) from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber join tbDate c on a.dateid=c.dateid group by c.theyear order by c.theyear;

第二步 执行结果 
使用CLI执行结果如下： 
 

3.3.3 计算所有订单每年最大金额订单的销售额
第一步 执行SQL语句
spark-sql>select c.theyear,max(d.sumofamount) from tbDate c join (select a.dateid,a.ordernumber,sum(b.amount) as sumofamount from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber group by a.dateid,a.ordernumber ) d  on c.dateid=d.dateid group by c.theyear sort by c.theyear;

第二步 执行结果 
使用CLI执行结果如下：


4 Spark Thrift Server
ThriftServer是一个JDBC/ODBC接口，用户可以通过JDBC/ODBC连接ThriftServer来访问SparkSQL的数据。ThriftServer在启动的时候，会启动了一个SparkSQL的应用程序，而通过JDBC/ODBC连接进来的客户端共同分享这个SparkSQL应用程序的资源，也就是说不同的用户之间可以共享数据；ThriftServer启动时还开启一个侦听器，等待JDBC客户端的连接和提交查询。所以，在配置ThriftServer的时候，至少要配置ThriftServer的主机名和端口，如果要使用Hive数据的话，还要提供Hive Metastore的uris。 
【注】Spark CLI和Spark Thrift Server实验环境为第二课《Spark编译与部署（下）–Spark编译安装》所搭建
4.1   配置并启动
4.1.1 创建并配置hive-site.xml
第一步 创建hive-site.xml配置文件 
在$SPARK_HOME/conf目录下修改hive-site.xml配置文件（如果在Spark SQL CLI中已经添加，可以省略）：
$cd /app/hadoop/spark-1.1.0/conf
$sudo vi hive-site.xml

第二步 修改配置文件 
设置hadoop1为Metastore服务器，hadoop2为Thrift Server服务器，配置内容如下：
<configuration>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://hadoop1:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  </property>

  <property>
    <name>hive.server2.thrift.min.worker.threads</name>
    <value>5</value>
    <description>Minimum number of Thrift worker threads</description>
  </property>

  <property>
    <name>hive.server2.thrift.max.worker.threads</name>
    <value>500</value>
    <description>Maximum number of Thrift worker threads</description>
  </property>

  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    <description>Port number of HiveServer2 Thrift interface. Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT</description>
  </property>

  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>hadoop2</value>
    <description>Bind host on which to run the HiveServer2 Thrift interface.Can be overridden by setting$HIVE_SERVER2_THRIFT_BIND_HOST</description>
  </property>
</configuration>

4.1.2 启动Hive
在hadoop1节点中，在后台启动Hive Metastore（如果数据存放在HDFS文件系统，还需要启动Hadoop的HDFS）：
$nohup hive --service metastore > metastore.log 2>&1 & 

4.1.3 启动Spark集群和Thrift Server
在hadoop1节点启动Spark集群
$cd /app/hadoop/spark-1.1.0/sbin 
$./start-all.sh
在hadoop2节点上进入SPARK_HOME/sbin目录，使用如下命令启动Thrift Server
$cd /app/hadoop/spark-1.1.0/sbin 
$./start-thriftserver.sh --master spark://hadoop1:7077 --executor-memory 1g

注意：Thrift Server需要按照配置在hadoop2启动！ 
在集群监控页面可以看到启动了SparkSQL应用程序： 

4.1.4 命令参数
使用sbin/start-thriftserver.sh –help可以查看ThriftServer的命令参数：
$sbin/start-thriftserver.sh --help Usage: ./sbin/start-thriftserver [options] [thrift server options] 
    Thrift server options: Use value for given property

 
其中[options] 是Thrift Server启动一个SparkSQL应用程序的参数，如果不设置–master的话，将在启动Thrift Server的机器以local方式运行，只能通过http://机器名:4040进行监控；这部分参数，可以参照Spark1.0.0 应用程序部署工具spark-submit 的参数。在集群中提供Thrift Server的话，一定要配置master、executor-memory等参数。 
[thrift server options]是Thrift Server的参数，可以使用-dproperty=value的格式来定义；在实际应用上，因为参数比较多，通常使用conf/hive-site.xml配置。
4.2   实战Thrift Server 
4.2.1 远程客户端连接
可以在任意节点启动bin/beeline，用!connect jdbc:hive2://hadoop2:10000连接ThriftServer，因为没有采用权限管理，所以用户名用运行bin/beeline的用户hadoop，密码为空：
$cd /app/hadoop/spark-1.1.0/bin
$./beeline
beeline>!connect jdbc:hive2://hadoop2:10000

4.2.2 基本操作
第一步 显示hive数据库所有表
beeline>show database;
beeline>use hive;
beeline>show tables; 

第二步 创建表testThrift
beeline>create table testThrift(field1 String , field2 Int);
beeline>show tables;

第三步 把tbStockDetail表中金额大于3000插入到testThrift表中
beeline>insert into table testThrift select ordernumber,amount from tbStockDetail  where amount>3000;
beeline>select * from testThrift;

第四步 重新创建testThrift表中，把年度最大订单插入该表中
beeline>drop table testThrift;
beeline>create table testThrift (field1 String , field2 Int);
beeline>insert into table testThrift select c.theyear,max(d.sumofamount) from tbDate c join (select a.dateid,a.ordernumber,sum(b.amount) as sumofamount from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber group by a.dateid,a.ordernumber ) d  on c.dateid=d.dateid group by c.theyear sort by c.theyear;
beeline>select * from testThrift;

4.2.3 计算所有订单每年的订单数
第一步 执行SQL语句
spark-sql>select c.theyear, count(distinct a.ordernumber) from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber join tbDate c on a.dateid=c.dateid group by c.theyear order by c.theyear;
第二步 执行结果 

Stage监控页面： 
 
查看Details for Stage 28 

4.2.4 计算所有订单月销售额前十名
第一步 执行SQL语句
spark-sql>select c.theyear,c.themonth,sum(b.amount) as sumofamount from tbStock a join tbStockDetail  b on a.ordernumber=b.ordernumber join tbDate c on a.dateid=c.dateid group by c.theyear,c.themonth order by sumofamount desc limit 10;
第二步 执行结果 

Stage监控页面： 

在其第一个Task中，从本地读入数据 

在后面的Task是从内存中获取数据 

4.2.5 缓存表数据
第一步 缓存数据
beeline>cache table tbStock;
beeline>select count(*) from tbStock;

第二步 运行4.2.4中的“计算所有订单月销售额前十名”
beeline>select count(*) from tbStock;

本次计算划给11.233秒，查看webUI，数据已经缓存，缓存率为100%： 

第三步 在另外节点再次运行 
在hadoop3节点启动bin/beeline，用!connect jdbc:hive2://hadoop2:10000连接ThriftServer，然后直接运行对tbStock计数（注意没有进行数据库的切换）： 

用时0.343秒，再查看webUI中的stage： 

Locality Level是PROCESS，显然是使用了缓存表。 
从上可以看出，ThriftServer可以连接多个JDBC/ODBC客户端，并相互之间可以共享数据。顺便提一句，ThriftServer启动后处于监听状态，用户可以使用ctrl+c退出ThriftServer；而beeline的退出使用!q命令。
4.2.6 在IDEA中JDBC访问
有了ThriftServer，开发人员可以非常方便的使用JDBC/ODBC来访问SparkSQL。下面是一个scala代码，查询表tbStockDetail，返回amount>3000的单据号和交易金额： 
第一步 在IDEA创建class6包和类JDBCofSparkSQL 
参见《Spark编程模型（下）–IDEA搭建及实战》在IDEA中创建class6包并新建类JDBCofSparkSQL。该类中查询tbStockDetail金额大于3000的订单： 
import java.sql.DriverManager

object JDBCofSparkSQL {
  def main(args: Array[String]) {
    Class.forName("org.apache.hive.jdbc.HiveDriver")
    val conn = DriverManager.getConnection("jdbc:hive2://hadoop2:10000/hive", "hadoop", "")
    try {
      val statement = conn.createStatement
val rs = statement.executeQuery("select ordernumber,amount from tbStockDetail  where amount>3000")
      while (rs.next) {
        val ordernumber = rs.getString("ordernumber")
        val amount = rs.getString("amount")
        println("ordernumber = %s, amount = %s".format(ordernumber, amount))
      }
    } catch {
      case e: Exception => e.printStackTrace
    }
    conn.close
  }
}
第二步 查看运行结果 
在IDEA中可以观察到，在运行日志窗口中没有运行过程的日志，只显示查询结果 

第三步 查看监控结果 
从Spark监控界面中观察到，该Job有一个编号为6的Stage，该Stage有2个Task，分别运行在hadoop1和hadoop2节点，获取数据为NODE_LOCAL方式。 



在hadoop2中观察Thrift Server运行日志如下：


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Ceph radosgw 安装配置
Ceph radosgw对象存储的接口，研究配置了许久，现分享如下。首先配置radosgw的前提，是你已经成功的配置好了Ceph集群，通过ceph –s 查看ceph集群，处于health状态。在这里，ceph集群的auth的配置为none，所以有关auth的部分，也就是访问权限相关的keying部分的配置在这里省略。
1)创建rgw相关的pool 
ceph osd pool create .rgw 128 128 
ceph osd pool create .rgw.root 128 128 
ceph osd pool create .rgw.control 128 128 
ceph osd pool create .rgw.gc 128 128 
ceph osd pool create .rgw.buckets 128 128 
ceph osd pool create .rgw.buckets.index 128 128 
ceph osd pool create .log 128 128 
ceph osd pool create .intent-log 128 128 
ceph osd pool create .usage 128 128 
ceph osd pool create .users 128 128 
ceph osd pool create .users.email 128 128 
ceph osd pool create .users.swift 128 128 
ceph osd pool create .users.uid 128 128
2）配置 ceph.conf 
这里配置在ceph.conf里添加有个radowgw的配置。radosgw有两种方式运行，一种是直接用civetweb的方式，其内置了一个比较小巧的http服务器mongoose,这种方式配置比较简单，不需要配置Apache httpd服务器，其配置如下：
[client.radosgw.gateway] 
host = node1 
log file = /var/log/ceph/client.radosgw.gateway.log 
rgw_frontends =civetweb port=80 
rgw print continue = false 
特别说明的是civetweb默认的端口是7480, 这里因为用s3cmd, 所以改为80，这里的node1是需要改成你自己的host名字
另一种方式就是 apache httpd + factcgi的方式，其配置如下： 
 [client.radosgw.gateway] 
host = node1 
log file = /var/log/ceph/client.radosgw.gateway.log 
rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0 
rgw print continue = false
注意，这里的node1，要替换成你自己的服务器 
3)启动 radosgw 
radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway 
主要，这里的-n后跟的名字，是ceph.conf 里配置的。如果出错，可以查看日志/var/log/ceph/client.radosgw.gateway.log的提示，做相应的处理。
4) 配置  httpd 
如果用的是civetweb方式，这一步可以省略。
在目录/etc/httpd/conf.d 里添加文件rgw.conf
<VirtualHost *:80>
ServerName 182.92.171.199
DocumentRoot /var/www/html

ErrorLog /var/log/httpd/rgw_error.log
CustomLog /var/log/httpd/rgw_access.log combined

RewriteEngine On
RewriteRule .* - [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

SetEnv proxy-nokeepalive 1

ProxyPass / fcgi://182.92.171.199:9000/
</VirtualHost>
这样这里的ServerName是http server name，如果有多个ip的情况，最好直接写ip，这里182.92.171.199的IP就是node1的ip，这里的ProxyPass 的fcgi 需要改成在ceph.conf里配置的ip和port
5) 启动httpd服务器 
如果用的是civetweb方式，这一步可以省略 
 service httpd restart
6）创建一个用户 
radosgw-admin user create –uid=cephtest –display-name=”ceph test” –email=ceph.test@chinacache.com 
显示： 
{ “user_id”: “cephtest”, 
 “display_name”: “ceph test”, 
 “email”: “ceph.test@chinacache.com”, 
 “suspended”: 0, 
 “max_buckets”: 1000, 
 “auid”: 0, 
 “subusers”: [], 
 “keys”: [ 
 { “user”: “cephtest”, 
 “access_key”: “8JGTP1714JRRAPRDLBI4“, 
 “secret_key”: “i4O+yJMNii87ruLMwcIIYbjLmKIUaoSO1svQOoB9“}], 
 “swift_keys”: [], 
 “caps”: [], 
 “op_mask”: “read, write, delete”, 
 “default_placement”: “”, 
 “placement_tags”: [], 
 “bucket_quota”: { “enabled”: false, 
 “max_size_kb”: -1, 
 “max_objects”: -1}, 
 “user_quota”: { “enabled”: false, 
 “max_size_kb”: -1, 
 “max_objects”: -1}, 
 “temp_url_keys”: []} 
注意：如果生成的key中有”\”,最后把这个user删掉，再重新生成直到不含”\”：
7）配置s3cmd 
$vim ~/.s3cfg  
[default] 
access_key = PPA789F4W5ANH3COC51O 
bucket_location = US 
cloudfront_host = cloudfront.amazonaws.com 
cloudfront_resource = /2010-07-15/distribution 
default_mime_type = binary/octet-stream 
delete_removed = False 
dry_run = False 
encoding = UTF-8 
encrypt = False 
follow_symlinks = False 
force = False 
get_continue = False 
gpg_command = /usr/bin/gpg 
gpg_decrypt = %(gpg_command)s -d –verbose –no-use-agent –batch –yes –passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s 
gpg_encrypt = %(gpg_command)s -c –verbose –no-use-agent –batch –yes –passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s 
gpg_passphrase = 
guess_mime_type = True 
host_base = node1 
host_bucket = %(bucket)s.node1 
human_readable_sizes = False 
list_md5 = False 
log_target_prefix = 
preserve_attrs = True 
progress_meter = True 
proxy_host = 
proxy_port = 0 
recursive = False 
recv_chunk = 4096 
reduced_redundancy = False 
secret_key = kqHLxnI14WqSl0Eh5akr73evrqmFZjfxYxUmG04h 
send_chunk = 4096 
simpledb_host = sdb.amazonaws.com 
skip_existing = False 
socket_timeout = 10 
urlencoding_mode = normal 
use_https = False 
verbosity = WARNING
在这里，access_key 和 secret_key 就是 radosgw-admin创建用户获得的，也可以用以下命令获取。hostbase  和 hostbucket 里的 node1都需要修改成你自己的server name 
radosgw-admin user info –uid=cephtest
8）测试 
首先用命令检查以下http 服务器是否正常工作： 
curl node1 
如果正确，应该返回：
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
如果不正确，那么http的服务器有问题。如果是apache http的服务器，那么查看服务器的配置的日志，查找相应的问题。 
/etc/httpd/logs/rgw_error.log
一般的常遇见问题都是: 
proxy: FCGI: attempt to connect to 182.92.171.199:9000 (182.92.171.199) failed 
显示连不上FCGI，那么看以下ceph.conf配置: 
rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0 
host和 port是否一致。
如果是civetweb方式启动，可以查看你在ceph.conf配置的日志 /var/log/ceph/client.radosgw.gateway.log来解决问题。
如果上述没有问题，可以用命令 
s3cmd la 
这么命令是list 所有的bucket下的对象
s3cmd mb s3://BUCKET  创建一个bucket，目前遇到的一个问题是，只能创建“BUCKET”为前缀的bucket，这个不知道为啥，还需要进一步研究和配置。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

手动安装OpenStack-Murano和Murano-dashboard
一、 环境准备
搭建Murano和Murano-dashboard是在Red Hat Enterprise Linux Server release 7.1 (Maipo)上搭建的，之前这个环境已经搭建了Openstack的环境，因此一些系统的工具包也安装过了，例如：gcc,python-pip,mysql。


二、在Devstack上配置安装Murano
1.首先需要下载dev的包，可以直接从github上clone


git clone https://github.com/openstack-dev/devstack.git
2.接着下载Murano需要的文件,从https://github.com/openstack/murano/tree/master/contrib/devstack处下载extras.d和lib中的文件，并将相应的文件放到devstack相应的目录下：


cp lib/murano ${DEVSTACK_DIR}/lib
cp lib/murano-dashboard {DEVSTACK_DIR}/lib
cp extras.d/70-murano.sh {DEVSTACK_DIR}/extras.d

3.最重要的就是编写localrc文件，这里给出一个示例，可以根据需要自行修改：


DATABASE_PASSWORD=123456
    RABBIT_PASSWORD=123456
    SERVICE_TOKEN=123456
    SERVICE_PASSWORD=123456
    ADMIN_PASSWORD=123456
    disable_service n-net
    enable_service q-svc
    enable_service q-agt
    enable_service q-dhcp
    enable_service q-l3
    enable_service q-meta
    enable_service q-lbaas
    enable_service q-vpn

    DEST=/home/OpenStack/workspace

    DATA_DIR=$DEST/data
    SERVICE_DIR=$DEST

    LOGDIR=/home/OpenStack/workspace/logs
    LOGFILE=$LOGDIR/stack.sh.log
    VERBOSE=True
    LOG_COLOR=True
    SCREEN_LOGDIR=$LOGDIR/screens

    Q_PLUGIN=ml2
    ENABLE_TENANT_VLANS=True

LOGDAYS=1

    #RECLONE=True
    #OFFLINE=True

GIT_BASE="http://github.com"
IMAGE_URLS=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img

    # Enable Heat
    enable_service heat h-api h-api-cfn h-api-cw h-eng

    # Enable Murano
    enable_service murano murano-api murano-engine



4.修改完localrc文件我们执行./stack.sh即可进行一键部署带有Murano的Devstack。

三、手动安装Murano
现在来尝试在上文中提到的环境中手动搭建Murano，之前的OpenStack已经部署上去了，所以我们不需要在devstack的环境中进行部署了。

1.首先安装murano-client，在shell中执行命令：

pip install python-muranoclient



2.安装murano API和Engine

下载Murano代码：
git clone git://git.openstack.org/openstack/murano

3.配置Murano的localrc文件，这里同样给出一个示例：

[DEFAULT]
debug = true
verbose = true
rabbit_host = %RABBITMQ_SERVER_IP%
rabbit_userid = %RABBITMQ_USER%
rabbit_password = %RABBITMQ_PASSWORD%
rabbit_virtual_host = %RABBITMQ_SERVER_VIRTUAL_HOST%
notification_driver = messagingv2

...

[database]
backend = sqlalchemy
connection = sqlite:///murano.sqlite

...

[keystone]
auth_url = 'http://%OPENSTACK_HOST_IP%:5000/v2.0'

...

[keystone_authtoken]
auth_uri = 'http://%OPENSTACK_HOST_IP%:5000/v2.0'
auth_host = '%OPENSTACK_HOST_IP%'
auth_port = 5000
auth_protocol = http
admin_tenant_name = %OPENSTACK_ADMIN_TENANT%
admin_user = %OPENSTACK_ADMIN_USER%
admin_password = %OPENSTACK_ADMIN_PASSWORD%

...

[murano]
url = http://%YOUR_HOST_IP%:8082

[rabbitmq]
host = %RABBITMQ_SERVER_IP%
login = %RABBITMQ_USER%
password = %RABBITMQ_PASSWORD%
virtual_host = %RABBITMQ_SERVER_VIRTUAL_HOST%



4.给Murano创建endpoint


keystone endpoint-create [--region <endpoint-region>] --service-id
                           <service-id> [--publicurl <public-url>]
                          [--adminurl <admin-url>]
                          [--internalurl <internal-url>]


5.安装API

install murano-api --config-file ./etc/murano/murano.conf
6.安装engine


install murano-engine –config-file ./etc/murano/murano.conf
7.然后可以一下murano的命令观察murano是否安装成功。



四、安装murano-dashboard

1.下载murano-dashboard和horizon的代码

git clone git://git.openstack.org/openstack/murano-dashboard
git clone git://git.openstack.org/openstack/horizon




2.复制murano的plugin文件到horizon文件夹中

cp  ../murano-dashboard/muranodashboard/local/_50_murano.py openstack_dashboard/local/enabled/
3.准备配置文件，可以直接将example文件中的配置拿过来使用：


cp  openstack_dashboard/local/local_settings.py.example openstack_dashboard/local/local_settings.py
4.正常启动django服务


python  manage.py  runserver  <IP:PORT>



5.在浏览器中输入<IP:PORT>，登录进去就可以在页面左侧看到Murano的项目了。











版权声明：本文为博主原创文章，未经博主允许不得转载。

Exactly-once Spark Streaming from Apache Kafka
这篇文章我已经看过两遍了，收获颇多，抽个时间翻译下，先贴个原文链接吧，也给自己留个任务
http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/


版权声明：本文为博主原创文章，未经博主允许不得转载。

Docker/compose分析
.. 
声明： 
本博客欢迎转发，但请保留原作者信息! 
博客地址：http://blog.csdn.net/halcyonbaby 
新浪微博：@寻觅神迹
内容系本人学习、研究和总结，如有雷同，实属荣幸！   


以下内容基于Compose1.4。

Docker/ompose分析

Compose是什么？
Compose是Docker公司提供的开源的编排部署工具。Compose的前身是Fig，使用Python代码编写。 
License为Apache2.0，对商业友好。
为什么需要Compose？
因为用户的实际应用是复杂的，可能由多个容器组成，容器间存在关系，使用上可能需要重复多次部署。 
因此，需要一定的编排部署能力来简化这个操作。
Compose的架构
Compose目前只提供命令行工具，没有Daemon存在。Compose与Docker紧密结合，目前只支持Docker。 
当然Compose也可以支持Swarm。 
Compose运行需要有对应的yml文件以及Dockerfile。 
通过调用Docker/Swarm的API进行容器应用的编排。     
Compose的现状
Compose目前版本为1.4。官方推荐在开发、测试、持续集成等非生成环境使用。并不适合生产环境使用。 
Compose的模型
service：实际上是一个包含某种功能的容器。  
Compose的所有操作对象均为service。  
Compose的功能

build(构建yml中某个服务的镜像) 
如上，web这个服务是依赖于镜像build的。在本地 也存在Dockerfile文件。 
可以使用docker-compose build来构建服务的镜像。  
ps(查看已经启动的服务状态）
kill(停止某个服务）
logs(可以查看某个服务的log）
port(打印绑定的public port）  
pull(pull服务镜像)  
up(启动yml定义的所有服务）
stop(停止yml中定义的所有服务）
start(启动被停止的yml中的所有服务）  
kill(强行停止yml中定义的所有服务）
rm（删除yml中定义的所有服务）
restart(重启yml中定义的所有服务）  
scale(扩展某个服务的个数，可以向上或向下）
migrate-to-labels(这个没有实际尝试。根据介绍是将服务从1.2迁移到1.3带labels的版本。docker之前不支持label）
version（查看compose的版本）

Compose的缺点

没有Daemon
没有Deaemon，也就没有高可用、HA之说。 
但是同时没有Deamon，所用动作需要用户自己触发。AutoScaling、self healing等也就没有办法提供。
模型不完整
模型相对简单，只有service。 
缺乏诸如网络、存储之类的资源抽象和管理。 
也缺乏诸如kubernetes中Pod、RC、service proxy之类的抽象，由于servie本身粒度太细，操作管理起来相对麻烦。  
使用Python代码编写
由于Docker社区大部分项目是Go编写的，Compose使用python不利于项目间代码共享。 
所幸的是，Compose社区目前已经开始着手此事。
与Docker紧密结合
是优点也是缺点。一方面可以很好的支持Docker的最新特性，另一方面与Docker绑死。
Compose 的未来发展
目前社区已经开始着手使用Go语言重新Compose，并以lib方式提供。 
Docker的理念是便于上层集成，也许未来会在kitematic中集成Compose的功能。  
其他第三方也可以通过各种方式进行集成。  
Compose演示

yml范例
[root@localhost sleep]# cat docker-compose.yml 
sleep1:
  command: sleep 8888888
  image: busybox
  restart: always
sleep2:
  command: sleep 9999999
  image: busybox
  restart: always
  
docker-compose up
根据yml启动应用。
[root@localhost sleep]# docker-compose up
Creating sleep_sleep1_1...
Creating sleep_sleep2_1...
Attaching to sleep_sleep1_1, sleep_sleep2_1
[root@localhost sleep]# docker-compose ps
     Name           Command      State   Ports 
'----------------------------------------------'
sleep_sleep1_1   sleep 8888888   Up            
sleep_sleep2_1   sleep 9999999   Up   






#### docker-compose scale
对应用的实例个数进行扩展。
[root@localhost sleep]# docker-compose scale sleep1=2
Creating and starting 2... done
[root@localhost sleep]# docker-compose ps
     Name           Command      State   Ports 
'----------------------------------------------
sleep_sleep1_1   sleep 8888888   Up            
sleep_sleep1_2   sleep 8888888   Up            
sleep_sleep2_1   sleep 9999999   Up    

docker-compose kill
强制终止某个服务。
[root@localhost sleep]# docker-compose kill sleep1
Killing sleep_sleep1_2... done
Killing sleep_sleep1_1... done
[root@localhost sleep]# docker-compose ps
     Name           Command       State     Ports 
'-------------------------------------------------
sleep_sleep1_1   sleep 8888888   Exit 137         
sleep_sleep1_2   sleep 8888888   Exit 137         
sleep_sleep2_1   sleep 9999999   Up

docker-compose stop
正常停止某个服务。
[root@localhost sleep]# docker-compose stop sleep2
[root@localhost sleep]# docker-compose ps
     Name           Command       State     Ports 
'-------------------------------------------------
sleep_sleep1_1   sleep 8888888   Exit 137         
sleep_sleep1_2   sleep 8888888   Exit 137         
sleep_sleep2_1   sleep 9999999   Exit 137    

docker-compose start
启动已经定义的服务。  
[root@localhost sleep]# docker-compose stop sleep2
[root@localhost sleep]# docker-compose ps
     Name           Command       State     Ports 
'-------------------------------------------------
sleep_sleep1_1   sleep 8888888   Exit 137         
sleep_sleep1_2   sleep 8888888   Exit 137         
sleep_sleep2_1   sleep 9999999   Exit 137    

…其他命令不再赘述。
Compose代码分析
Compose参考资料

Compose文档 
http://docs.docker.com/compose/
Github地址 
https://github.com/docker/compose


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

贝叶斯分类器
1、条件概率
P(A|B)=P(AB)P(B)P(A|B)=\frac{P(AB)}{P(B)}
即：在事件B发生的条件下事件A发生的频率，等于事件A、B同时发生的频率处于事件B发生的频率，可以通过文氏图来理解条件概率。
由条件概率可以得到乘法公式：
P(AB)=P(A|B)P(B)P(AB)=P(A|B)P(B)，同理：P(AB)=P(B|A)P(A)P(AB)=P(B|A)P(A)
2、全概率公式
设B1,B2,...,BnB_{1},B_{2},...,B_{n}为一完备事件组，即相互之间交集为空，且总的并集为1，则对事件A有：
P(A)=∑ni=1P(A|Bi)P(Bi)P(A)=\sum_{i=1}^{n}P(A|B_{i})P(B_{i})，其中i=1,2,…,n
3、贝叶斯公式
设B1,B2,...,BnB_{1},B_{2},...,B_{n}为一完备事件组，即相互之间交集为空，且总的并集为1，则有：
P(Bj|A)=P(A|Bj)P(Bj)∑ni=1P(A|Bi)P(Bi)P(B_{j}|A)=\frac{P(A|B_{j})P(B_{j})}{\sum_{i=1}^{n}P(A|B_{i})P(B_{i})}，其中i、j=1,2,…,n
4、贝叶斯分类
假设有n个类别，分别为 C1,C2,...,CnC_{1},C_{2},...,C_{n}
各个类别的概率，即 P(C1),P(C2),...,P(Cn)P(C_{1}),P(C_{2}),...,P(C_{n})，是好求的，称为先验概率
有k个特征，分别为t1,t2,...,tkt_{1},t_{2},...,t_{k}，贝叶斯分类假设各个特征之间是相互独立的
各个类别中每个特征的概率也可以求出来，即求
P(t1|C1),P(t2|C1),...,P(tk|C1)P(t_{1}|C_{1}), P(t_{2}|C_{1}), ..., P(t_{k}|C_{1}) 
P(t1|C2),P(t2|C2),...,P(tk|C2)P(t_{1}|C_{2}), P(t_{2}|C_{2}), ..., P(t_{k}|C_{2}) 
…. 
P(t1|Cn),P(t2|Cn),...,P(tk|Cn)P(t_{1}|C_{n}),P(t_{2}|C_{n}),...,P(t_{k}|C_{n})
假设有一特征向量为t′1,t′2,...,t′kt_{1}^{'},t_{2}^{'},...,t_{k}^{'}，现在要对其分类，即在特征为t′1,t′2,...,t′kt_{1}^{'},t_{2}^{'},...,t_{k}^{'}的情况下，看看那个类别的概率最大，最大的那个类别的概率即为贝叶斯分类的结果，求：
P(C1|(t′1,t′2,...,t′k)),...,P(Cn|(t′1,t′2,...,t′k))P(C_{1}|(t_{1}^{'},t_{2}^{'},...,t_{k}^{'})),...,P(C_{n}|(t_{1}^{'},t_{2}^{'},...,t_{k}^{'}))
以求P(C1|(t′1,t′2,...,t′k))P(C_{1}|(t_{1}^{'},t_{2}^{'},...,t_{k}^{'}))为例：
P(C1|(t′1,t′2,...,t′k))=P((t′1,t′2,...,t′k)C1)P(t′1,t′2,...,t′k)=P((t′1,t′2,...,t′k)|C1)P(C1)P(t′1,t′2,...,t′k)P(C_{1}|(t_{1}^{'},t_{2}^{'},...,t_{k}^{'}))=\frac{P((t_{1}^{'},t_{2}^{'},...,t_{k}^{'})  C_{1})}{P(t_{1}^{'},t_{2}^{'},...,t_{k}^{'})}=\frac{P((t_{1}^{'},t_{2}^{'},...,t_{k}^{'})|C_{1})P(C_{1})}{P(t_{1}^{'},t_{2}^{'},...,t_{k}^{'})}
因为分母P(t′1,t′2,...,t′k)P(t_{1}^{'},t_{2}^{'},...,t_{k}^{'})对每个类别是一样的，所以可以忽略不求，所以只需要求上式中分子的最大值，现在求P((t′1,t′2,...,t′k)|C1)P(C1)P((t_{1}^{'},t_{2}^{'},...,t_{k}^{'})|C_{1})P(C_{1})，因为各个特征之间相互独立，所以P((t′1,t′2,...,t′k)|C1)=P(t1|C1)P(t2|C1)...P(tk|C1)P((t_{1}^{'},t_{2}^{'},...,t_{k}^{'})|C_{1})=P(t_{1}|C_{1})P(t_{2}|C_{1})...P(t_{k}|C_{1})，因为每个分类中各个特征的概率我们已经求出来了，所以现在就好计算了。
完， 
参考链接： 
（阮一峰）http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html 
（比较详细，没看）http://www.cnblogs.com/leoo2sk/archive/2010/09/17/1829190.html

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

近似直径算法的 GraphChi 实现
1. GraphChi
1.1 简介
　　GraphChi 是由卡耐基梅隆大学设计， 可以在单机上进行高效大规模计算的框架， 区别于将图的信息全部存储在内存当中， GraphChi 利用单机计算机海量的硬盘进行存储， 由于硬盘与内存的访问速度差距很大， 为了弥补使用硬盘存储带来的缺陷， 他们设计出并行滑动窗口技术， 用来减少硬盘的随机读写。
1.2 并行滑动窗口技术
　　将整个图谱划按照点的顺序分成不同的分片，每个分片能够完全在内存中进行 
处理。 如下图所示： 

　　对每个分片中的入边按照源点进行排序， 基于这种原则出边分布在所有的分片 
上， 而且占据一段连续的空间。 这样对于数据的更新， 首先在内存中进行计算和 
存储， 随后连续的写入其他分片中， 这样能够很好解决随机读写带来的高延迟 
问题， 如下图所示。 

1.3 编程模型
　　GraphChi 延续了 GraphLab 中采用的以点为中心的编程模型， 针对图谱中的节点和边都携带有用户自定义的数据。 在每一次迭代中， 同一个分片中被标记过的点并行调用update 函数进行数据更新： 获取该点入边携带的信息，出边携带的信息， 该点携带的信息，通过用户自定义的计算逻辑，对边和点携带的信息进行更新。 如下图所示：

２. 近似直径算法: Multiple BFS
　　用精确求解图的直径，需要通过从每一个点出发，进行一次 BFS 找到离该点最远的距离，最终所得值当中的最大值即为图的直径。 在图很大的情况下，这种方法带来的时间开销是无法容忍的，因此容易想到的一个方法是： 选取其中 K 个节点， 分别求出对应的最远距离，然后取最大值近似作为图的直径。
　　这种想法很直观： 从 K 个节点分别做 K 次 BFS， 但是这种做法的时间开销比较大，从 K个节点出发， 有一部分点集到这 K 个节点的距离是一样的， 从另一个角度来讲， 从 K 个节点出发形成的 BFS 搜索树之间可能存在相同的路径，对于这些相同的路径其实只需一次遍历就行。因此论文中提出的方法，其实是对每个节点进行状态标记， 用 k 位的二进制数标记哪些源点出发的路径已经访问过该点， 每次迭代过程，其实是对每个节点进行状态间的转移，对于那些从源点已经经过该点的路径，不在对该点进行更新。
3. 基于 GraphChi 的 Multiple BFS 实现设计

K points 的选取： 选取编号为 0,1,2,4,8 …… 的点，共选取 log(nPoints)个点。
点权： 用来存储该节点的状态， 从哪些源点出发的路径，已经访问过该点。
边权： 将源节点的状态通过边权传递给目的节点。
update 函数
访问该点所有的入边， 将入边的所有权值取或作为 new_state， 该点以前的权值作为 old_state, 判断 old_state 是否完全包含 new_state,即判断是否有从新的源点出发的路径来访问该节点，如果是，则更新该节点的状态。
如果该节点的状态被更新， 将该节点所有的出边的权值都设为该节点的状态，并且将该边所指向的节点加入下一次迭代的队列。
入边权值与出边权值共用带来的问题 
　　通过上述方式实现后，发现求出的图的直径与实际值差距较大，通过分析源代码之后，发现： 对于同一条边对应的入边和出边， 它们所对应的权值指向通一块存储地址，也就是说：入边和出边的权值始终都是一样的，这在异步执行的情况下就引入了一个问题：在同一次迭代中， 对于同一条边，如果出现某个节点把出边的权值修改， 而有节点需要使用入边的权值， 此时使用的权值不是上一次迭代传来的权值，而是新的权值。
　　为了解决这个问题， 对于边的权值， 引入了两个权值的概念： “当前权值” 和“下一轮权值”， 在一次迭代中， 对入边的访问，使用“当前权值”，而对于出边的修改， 使用“下一轮权值”。


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【云快讯】《“云计算”也要防雷，20%数据中心故障和雷击有关》




2015-08-24 张晓东 东方云洞察


点击上面的链接文字，可以快速关注“东方云洞察”公众号







上周四，比利时布鲁塞尔西南郊的St.Ghislaina小镇遭遇了雷电天气。结果，Google位于该镇的数据中心所存储的数据，遭到了“严重”的损失。大约百分之0.000001的数据受到了影响。











看起来这么小比例的数据量丢失媒体有点小题大做，不过谁让谷歌是全球互联网界的宠儿呢，也是云计算领域的先驱。 因为，雷电等原因导致数据中心的数据丢失，对于谷歌号称永不丢失的数据而言实在是一个教训。




Google的数据中心架构工作人员正在着手解决这个问题。目前，大部分的服务器已经使用更换了具备更好抗灾性的存储硬件。另外，Google也重申，Google云服务的弹性计算架构遍布全球各地，用户可以设定在灾难发生时自动切换到备用的弹性架构上。目前，Google Compute Engine在全球共有三个云计算数据中心，分别位于比利时、台湾的彰化和美国的爱荷华州。




闪电击中了数据中心所用的电网，导致主要电力系统供电中断。数据中心随即切换到了两种备用系统：备用供电线路和服务器内置电池。一般来说，两种灾备方案对于数据中心肯定是够了，但谁曾想到，内置的电池这次出现了问题。




根据Google透露的情况，在从上周四到昨天的时间里，由于部分数据中心服务器的电池耗尽时间太长(extended or repeated battery drain)，这些涉事服务器存储的I/O出现了零星的存取失败情况：大约百分之0.000001的数据受到了影响。




这些服务器位于Google Compute Engine的北欧区域的europe-west1-b，受影响的服务器在该数据中心比例约为5%。虽然看起来数据损失比例极小，但对于拥有海量数据的Google来说，本次数据损失并不是一件小事……行业人士估计，Google拥有10-15 exabyte(EB)的数据，每exabyte等于100万TB，而每TB等于1000GB。如果把这个数字带入到计算里，百分之0.000001大约等于100多GB的数据。当然，Google虽然没有公布比利时数据中心的数据存储量具体有多少，但我们可以估计，至少有数GB到数十GB的数据，在本次雷暴中丢失。







这一事件之后，谷歌的工程师们对该公司的数据中心技术进行了“广泛的审查”，包括电气分布等发现了一些需要改进的领域。它们包括升级硬件以提高，在断电瞬间高速缓存数据保存能力，”以及“改善系统工程师的响应程序和能力”。




谷歌并不是独自面对这个问题。在2011年亚马逊在爱尔兰数据中心也早到了终端。




谷歌称其可靠性和准备是不可想象的，包括地震、甚至公共健康危机，“假设人和服务不可用长达30天。”（这是一个流行的做法。）




经此一事，说明谷歌还需要在数据中心方案方面有更多的改进，以保证将来做得更好。









扫描二维码关注【东方云洞察】公众号

实时了解深度的公有云市场分析和洞察结果！点击右上角，在弹出的菜单中发送给朋友、分享到朋友圈。请在公众号搜索并关注：DongCloudInsight
 或 东方云洞察。需要点对点交流请加微信：jackyzhang523



帮助您了解公有云相关的深度洞察结果。带来极具深度和最新鲜的：云市场分析、云机会洞察分析、云重大事件快评、云杂谈、云论坛资讯，以及公有云领域最高端的CEO面对面深度研讨。

--- 最专注、专业的“公有云洞察”分享；关注全球，聚焦中国。





版权声明：本文为博主原创文章，未经博主允许不得转载。

storm-kafka数据读取问题
在storm的bolt中，接受kafka会出现数据读不到的问题：

 
控制台不报错，但是有如上的提示，提醒有Fetched 31 messages from:和 Added 31 messages from:，但是并未对数据进行处理和接受。
产生的原因是这样的，下面分两种情况讨论：

1.bolt只接受一个spout：

如：
builder.setSpout("readlog", new KafkaSpout(dataConfig),1);
builder.setBolt("FPGrowth", new FPGrowthBolt(), 1).shuffleGrouping("readlog");
在FPGrowthBolt中无需画蛇添足地加上
 if(input.getFields().contains("readlog")) 
 {
 String line =    (String)input.getValueByField("readlog");
}
否则就会读不到kafka传来的数据 
所以可以直接写上
    String line = input.getString(0);

2.bolt只接受两个spout： 
如：

    builder.setSpout("systemin", new KafkaSpout(inConfig),1);
    builder.setBolt("PatternTree", new PatternTreeBolt(),1).shuffleGrouping("FPGrowth").shuffleGrouping("systemin");

对于PatternTreeBolt来说，接受了两个输入，FpGrowth和systemin，而systemin是由kafka传入数据的，对于Bolt来说，接受kafka传入的数据，与从spout和bolt接受数据有所不同，需要区别对待。 
当写下如下代码时：
if (input.getFields().contains("FPGrowth"))
        {
            SecPackData mesg = (SecPackData) input.getValueByField("FPGrowth");
        }
         else if (input.getFields().contains("systemin")){   
                String str;
                str = (String)input.getValueByField("systemin");
                }
就会读取不到systemin通过kafka传来的数据，所以应改成如下代码：
if (input.getFields().contains("FPGrowth"))
        {
            SecPackData mesg = (SecPackData) input.getValueByField("FPGrowth");
        }
         else {  
                String str;
                str = input.getString(0);
                }

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Stanford机器学习课程笔记(1) Supervised Learning and Unsupervised Learning
最近跟完了Andrew Ng的Machine Learning前三周的课，主要讲解了机器学习中的线性回归（Linear Regression）和逻辑回归（Logistic Regression）模型。在这里做一下记录。 
另外推荐一本统计学习的书，《统计学习方法》李航，书短小精悍，才200多页，但是内容基本上覆盖了机器学习中的理论基础。
笔记<1> 主要了解一下监督学习和无监督学习

机器学习：是关于计算机基于数据 构建概率统计模型 并运用模型对数据进行预测与分析的一门学科。
机器学习算法分类： 监督学习（supervised learning）、无监督学习（unsupervised learning）、半监督学习（semi-supervised learing）和强化学习（reinforcement learning）。
监督学习

监督学习从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。
监督学习的目的在于学习一个由输入到输出的映射，即找到这样一个模型（又称为假设hypothesis）属于由输入空间到输出空间的映射的集合。 

当输入变量和输出变量均为连续变量的预测问题称为回归（regression）问题；输出变量为有限个离散变量的预测问题称为分类（classification）问题；输入变量与输出变量均为变量序列的预测问题称为标注(tagging)问题。

（课程中的Housing price prediction 和 Breast cancer例子分别介绍回归问题和分类问题）
无监督学习

无监督式学习(Unsupervised Learning )是人工智能网络的一种算法(algorithm)，其目的是去对原始资料进行分类，以便了解资料内部结构。有别于监督式学习网络，无监督式学习网络在学习时并不知道其分类结果是否正确，亦即没有受到监督式增强(告诉它何种学习是正确的)。其特点是仅对此种网络提供输入范例，而它会自动从这些范例中找出其潜在类别规则。当学习完毕并经测试后，也可以将之应用到新的案例上。
无监督学习里典型的例子就是聚类了。聚类的目的在于把相似的东西聚在一起，而我们并不关心这一类是什么。因此，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了。

半监督学习
＜https://en.wikipedia.org/wiki/Semi-supervised_learning＞
强化学习
＜https://en.wikipedia.org/wiki/Reinforcement_learning＞

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

HBase性能优化完全版
近期在处理HBase的业务方面常常遇到各种瓶颈，一天大概一亿条数据，在HBase性能调优方面进行相关配置和调优后取得了一定的成效，于是，特此在这里总结了一下关于HBase全面的配置，主要参考我的另外两篇文章：
（1）http://blog.csdn.net/u014297175/article/details/47975875
（2）http://blog.csdn.net/u014297175/article/details/47976909
在其基础上总结出来的性能优化方法。
1.垃圾回收优化
Java本身提供了垃圾回收机制，依靠JRE对程序行为的各种假设进行垃圾回收，但是HBase支持海量数据持续入库，非常占用内存，因此繁重的负载会迫使内存分配策略无法安全地依赖于JRE的判断：需要调整JRE的参数来调整垃圾回收策略。有关java内存回收机制的问题具体请参考：http://my.oschina.net/sunnywu/blog/332870。
（1）HBASE_OPTS或者HBASE_REGIONSERVER_OPT变量来设置垃圾回收的选项，后面一般是用于配置RegionServer的，需要在每个子节点的HBASE_OPTS文件中进行配置。
1）首先是设置新生代大小的参数，不能过小，过小则导致年轻代过快成为老生代，引起老生代产生内存随便。同样不能过大，过大导致所有的JAVA进程停止时间长。-XX:MaxNewSize=256m-XX:NewSize=256m 这两个可以合并成为-Xmn256m这一个配置来完成。
2）其次是设置垃圾回收策略：-XX:+UseParNewGC -XX:+UseConcMarkSweepGC也叫收集器设置。
3）设置CMS的值，占比多少时，开始并发标记和清扫检查。-XX:CMSInitiatingOccupancyFraction=70
4）打印垃圾回收信息：-verbose:gc -XX: +PrintGCDetails -XX:+PrintGCTimeStamps 
-Xloggc:$HBASE_HOME/logs/gc-$(hostname)-hbase.log
最终可以得到：HBASE_REGIONSERVER_OPT="-Xmx8g -Xms8g –Xmn256m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC  \
-XX:CMSInitiatingOccupancyFraction=70   -verbose:gc \
-XX:+PrintGCDetails -XX:+PrintGCTimeStamps \ 
-Xloggc:$HBASE_HOME/logs/gc-$(hostname)-hbase.log
（2）hbase.hregion.memstore.mslab.enabled默认值：true，这个是在hbase-site.xml中进行配置的值。
说明：减少因内存碎片导致的Full GC，提高整体性能。
2.启用压缩，详情自行搜索，暂时未曾尝试，后面持续更新。
3.优化Region拆分合并以及与拆分Region
（1）hbase.hregion.max.filesize默认为256M（在hbase-site.xml中进行配置），当region达到这个阈值时，会自动拆分。可以把这个值设的无限大，则可以关闭HBase自动管理拆分，手动运行命令来进行region拆分，这样可以在不同的region上交错运行，分散I/O负载。
（2）预拆分region
用户可以在建表的时候就制定好预设定的region，这样就可以避免后期region自动拆分造成I/O负载。
4.客户端入库调优
（1）用户在编写程序入库时，HBase的自动刷写是默认开启的，即用户每一次put都会提交到HBase server进行一次刷写，如果需要高速插入数据，则会造成I/O负载过重。在这里可以关闭自动刷写功能，setAutoFlush(false)。如此，put实例会先写到一个缓存中，这个缓存的大小通过hbase.client.write.buffer这个值来设定缓存区，当缓存区被填满之后才会被送出。如果想要显示刷写数据，可以调用flushCommits()方法。
此处引申：采取这个方法要估算服务器端内存占用则可以：hbase.client.write.buffer*hbase.regionserver.handler.count得出内存情况。
（2）第二个方法，是关闭每次put上的WAL（writeToWAL(flase)）这样可以刷写数据前，不需要预写日志，但是如果数据重要的话建议不要关闭。
（3）hbase.client.scanner.caching：默认为1
这是设计客户端读取数据的配置调优，在hbase-site.xml中进行配置，代表scanner一次缓存多少数据（从服务器一次抓取多少数据来scan）默认的太小，但是对于大文件，值不应太大。
（4）hbase.regionserver.lease.period默认值：60000


说明：客户端租用HRegion server 期限，即超时阀值。

调优：这个配合hbase.client.scanner.caching使用，如果内存够大，但是取出较多数据后计算过程较长，可能超过这个阈值，适当可设置较长的响应时间以防被认为宕机。
（5）还有诸多实践，如设置过滤器，扫描缓存等，指定行扫描等多种客户端调优方案，需要在实践中慢慢挖掘。
5.HBase配置文件
上面涉及到的调优内容或多或少在HBase配置文件中都有所涉及，因此，下面的配置不涵盖上面已有的配置。
(1) zookeeper.session.timeout（默认3分钟）
ZK的超期参数，默认配置为3分钟，在生产环境上建议减小这个值在1分钟或更小。 
设置原则：这个值越小，当RS故障时Hmaster获知越快，Hlog分裂和region 部署越快，集群恢复时间越短。 但是，设置这个值得原则是留足够的时间进行GC回收，否则会导致频繁的RS宕机。一般就做默认即可
（2）hbase.regionserver.handler.count（默认10）
对于大负载的put（达到了M范围）或是大范围的Scan操作，handler数目不易过大，易造成OOM。 对于小负载的put或是get，delete等操作，handler数要适当调大。根据上面的原则，要看我们的业务的情况来设置。（具体情况具体分析）。
（3）HBASE_HEAPSIZE（hbase-env.sh中配置）
我的前两篇文章Memstoresize40%（默认） blockcache 20%（默认）就是依据这个而成的，总体HBase内存配置。设到机器内存的1/2即可。
（4）选择使用压缩算法，目前HBase默认支持的压缩算法包括GZ，LZO以及snappy（hbase-site.xml中配置）
（5）hbase.hregion.max.filesize默认256M
上面说过了，hbase自动拆分region的阈值，可以设大或者无限大，无限大需要手动拆分region，懒的人别这样。
（6）hbase.hregion.memstore.flush.size
单个region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。
（7）hbase.hstore.blockingStoreFiles  默认值：7
说明：在flush时，当一个region中的Store（CoulmnFamily）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。
调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。
（8）hbase.hregion.memstore.block.multiplier默认值：2
说明：当一个region里总的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。
虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。
调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。
（9）hbase.regionserver.global.memstore.upperLimit：默认40%
当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。
hbase.regionserver.global.memstore.lowerLimit：默认35%
同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flushthread woke up with memory above low water.”。
调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。
（10）hfile.block.cache.size：默认20%
  这是涉及hbase读取文件的主要配置，BlockCache主要提供给读使用。读请求先到memstore中查数据，查不到就到blockcache中查，再查不到就会到磁盘上读，并把读的结果放入blockcache。由于blockcache是一个LRU,因此blockcache达到上限(heapsize * hfile.block.cache.size)后，会启动淘汰机制，淘汰掉最老的一批数据。对于注重读响应时间的系统，应该将blockcache设大些，比如设置blockcache=0.4，memstore=0.39，这会加大缓存命中率。
（11）hbase.regionserver.hlog.blocksize和hbase.regionserver.maxlogs
之所以把这两个值放在一起，是因为WAL的最大值由hbase.regionserver.maxlogs*hbase.regionserver.hlog.blocksize (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。
提示：最好将hbase.regionserver.hlog.blocksize* hbase.regionserver.maxlogs 设置为稍微大于hbase.regionserver.global.memstore.lowerLimit* HBASE_HEAPSIZE。
6.HDFS优化部分
HBase是基于hdfs文件系统的一个数据库，其数据最终是写到hdfs中的，因此涉及hdfs调优的部分也是必不可少的。
（1）dfs.replication.interval:默认3秒
可以调高，避免hdfs频繁备份，从而提高吞吐率。
（2）dfs.datanode.handler.count:默认为10
可以调高这个处理线程数，使得写数据更快
（3）dfs.namenode.handler.count：默认为8
同上，可以调高，提高整体速度与性能。
 
以上就是hbase整体性能调优攻略，仍然会有遗漏的地方或补充，在实际中会慢慢完善，有不足的也请各位同仁指出！

版权声明：本文为博主原创文章，未经博主允许不得转载。

物联网环境下的大吞吐量下消息服务集群设计
1、基于IBM MQ产品来实施JMS技术的消息服务应用服务器。2、物联网消息采用MQTT协议，WebSphere MQ Telemetry Transport (MQTT)  是一项专为受限设备和受限网络设计的异步消息通信协议，以轻量、精简、开放和易于实现为主要特点。3、MQTT 规范是开放并且免版税使用的，这有助于更好地推广。提供开源的实现，在 http://eclipse.org/paho/上有各种客户端的开源实现4、发布 - 订阅的消息通信协议，允许一条消息只发布一次，便可被多个消费端（应用程序 / 设备）所接收5、提供多种消息服务质量，包括 MQ 的黄金准则 -- 保证传递且仅有一次传递  0 ：消息最多被传递一次 1 ：消息会被传递但可能会重复传递 2 ：消息保证传递且仅有一次传递6、为受限的设备所设计 :预期客户端应用程序 / 设备有可能仅具备非常有限的处理能力和资源占用空间极小的 MQTT 客户端 ( 和服务器 ) 类库7、易于使用（和实现）简单的动词集合，包括 connect, publish, subscribe 和 disconnect内建结构支持处理客户端和服务器之间的连接丢失如果客户端意外掉线，使用“遗愿和遗嘱”发布一条消息8、WebSphere MQ Telemetry 由 Telemetry 服务和 Telemetry 客户端组成。其中 Telemetry 服务作为 Queue Manager 的一部分，可作为 MQTT 连接的服务器，Telemetry 客户端可用来测试 MQTT 连接的可用性。9、在传统的开放平台 WebSphere MQ 应用架构中，每个队列管理器都是独立的。当一个 QM 给另一个 QM 发送消息时，需要定义一个传输队列（transmission queue）, 一个连接到目的端 QM 的通道，并且需要在发送消息的客户端上定义远程队列定义（remote queue definition）。为了简化 MQ 系统配置，可以通过 MQ 集群的使用，减少队列管理器上的对象数量，使得不同的 QM 可以互相通信而不需要定义众多的传输队列、通道以及远程队列定义。当集群中含有一个以上的同一队列实例时，WebSphere® MQ 会根据负载均衡算法选择最佳的队列进行消息路由。10、MQ 集群中的完全存储仓库存储集群中队列管理器的元数据信息，一个集群不建议使用超过两个完全存储仓库11、完全存储仓库建议不做业务应用，具体业务应用使用不完全存储仓库12、在 MQ 集群中使用 MQTT Telemetry 服务时，只需要在集群中建立集群主题（Cluster Topic），并且只需要在集群中的一个队列管理器创建，不需要创建共享队列，默认使用 SYSTEM.MQTT.TRANSMIT.QUEUE13、使用 MQ Telemetry 不需要手动创建订阅对象（Subscriptions），MQXR 服务默认使用 client ID :topic string 为名字自动创建订阅对象14、完整的MQTT协议规范pdf下载：http://public.dhe.ibm.com/software/dw/webservices/ws-mqtt/MQTT_V3.1_Protocol_Specific.pdf15、 java -Xms50M -Xmx50M -Djava.ext.dirs=/root/mq/lib -cp mqttperf.jar SingleTopicSub -b 9.119.154.235 -c 1000 -m 50000 -t TestTopic -s 1 即一共创建了 1000 个订阅者，无差错情况下会接收到 50000 条消息。命令中参数 -Xms 指程序的初始化内存大小，-Xmx 指程序占用的最大内存，-Djava.ext.dirs 指引用包路径，该路径文件夹中应该包含有 org.eclipse.paho.client.mqttv3.jar。注意：其中 -m 参数主要用来标记所有客户端应该收到的消息总数，其值为所有客户端数与发布程序发布的消息数之乘积，用来和实际接收到的消息总数做比较，判断所有消息是否被可靠传输。

版权声明：本文为博主原创文章，未经博主允许不得转载。

Spark修炼之道（基础篇）——Linux大数据开发基础：第五节：vi、vim编辑器（一）
本节主要内容

vim编辑器的三种模式
移动光标
输入模式
修改文本

作者：周志湖 
微信号：zhouzhihubeyond 
网名：摇摆少年梦
1. vi编辑器的三种模式
学会使用vi编辑器是学习linux系统的必备技术之一，因为一般的linux服务器是没有GUI界面的，linux运维及开发人员基本上都是通过命令行的方式进行文本编辑或程序编写的。vi编辑器是linux内置的文本编辑器，几乎所有的类unix系统中都内置了vi编辑器，而其它编辑器则不一定，另外很多软件会调用vi编辑进行内容编写，例如crontab定时任务。较之于其它编辑器或GUI编辑器，vi编辑速度是最快的。vim编辑器可以看作vi的高级版本，它实现了用颜色来进行特殊信息的显示，例如在进行java程序开发是，它会对某些关键字用颜色显示。 
vi编辑器： 

vim编辑器： 

vi编辑器有三种模式，分别是一般模式、编辑模式及命令行模式 
1 一般模式，采用vi命令直接进入一般模式 
例如
root@ubuntu:/home/xtwy/compresse_demo# vi /etc/profile

 
在一般模式下，可以进行上下左右的光标移动、删除字符、行，还可以进行复制和粘贴操作
2 编辑模式，在一般模式中按”i,l,o,O,a,A,r,R”等做任意一个字符后，将进入编辑模式，下面给出的是在一般模式下输入”i”后，得到的编辑模式窗口。 
 
窗口最底下会显示– INSERT–,此时可以对文本内容进行编辑模式，注意因为机器上已经安装了vim，所以显示时有颜色。
3 命令行模式 
 在编辑模式中，按”ESC”鍵可以回到一般模式。在一般模式中，输入”:,/,?”做任意一个字符，光标将移动到窗口底部，此时可以保存编辑好的文件或离开vi编辑器等，下图给出的是输入”:”之后得到的命令行模式窗口 

2. 移动光标
1 单个字符移动 
  在一般模式化中，采用Up Arrow, Down Arrow键可以左右进行字符移动，也可以采用键盘上的 H、L键进行字符左右移动 
 空格键也可以单个字符地向右移动。
2 移动到某个特定字符 
   采用fx命令进行字符定位，例如fs可以定位到同一行s下一次现出的位置 
 
 按下fs后光标位置 

采用Fx命令，可以将光标定位到x字符在同一行上一次出现的位置
3 按word移动
w命令将光标移动到下一下word的首字母，标点符号也算一个字。例如有下列文本：
【光标在这】class Student(name:String,age:Int,val studentNo:String) extends Person

在一般模式下，按1w之后
class 【光标在这】Student(name:String,age:Int,val studentNo:String) extends Person

W命令利用空格移动光标，例如
class 【光标在这】Student(name:String,age:Int,val studentNo:String) extends Person

在一般模式下，按1W之后
class Student(name:String,age:Int,val 【光标在这】studentNo:String) extends Person
B命令利用空格向后移动光标，例如
class Student(name:String,age:Int,val 【光标在这】studentNo:String) extends Person

在一般模式下，按1W之后
class 【光标在这】Student(name:String,age:Int,val studentNo:String) extends Person
b命令利用word包括标点向后移动光标，例如
class 【光标在这】Student(name:String,age:Int,val studentNo:String) extends Person
在一般模式下，按1b之后
【光标在这】class Student(name:String,age:Int,val studentNo:String) extends Person

e命令将光标移动到下一word的尾部，例如
class Student(name:String,age:Int,val studentNo:String)【光标在这】 extends Person

在一般模式下，按1e之后

class Student(name:String,age:Int,val studentNo:String) extends【光标在这】 Person
E命令将光标移动到下一个空格分隔字的尾部，例如
class 【光标在这】Student(name:String,age:Int,val studentNo:String) extends Person

在一般模式下，按1E之后
class Student(name:String,age:Int,val【光标在这】 studentNo:String) extends Person
4 按行移动
k键、Up Arrow键移动到上一行 
j鍵、Down Arrow键移动到下一行
5 句子、段落移动 
( 移动句子的开始，）移动到句子的结束位置，下面只给出（的演示
Apache Spark is a fast and general-purpose cluster computing system. 【光标在这】It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
Downloading

一般模式下，输入(后

【光标在这】Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
Downloading

｛移动到段落的开始，｝移动到段落的结束，下面给出}的演示
Spark Overview
【光标在这】
Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
Downloading

一般模式下，输入(后

Spark Overview

Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
Downloading
【光标在这】
5 屏幕位置移动 
H(home）移动到屏幕最顶部，下面的图是没有按H键时光标位置 
 
按完键后， 
 
按L键可以将光标移动到屏幕最底部，下面的图是上图按L键之后的效果 
 
按M键可以将光标移动到屏幕中间，下图是上图按M鍵后的效果 

CTR+D(Down）向下翻屏移动光标，CTR+U(UP)向上翻屏移动光标，命令前面还可以加数字表示翻屏多少行
CTR+F(Forward）与PageDown键，显示下一屏文本；CTR+B（Backward）与PageUp显示上一屏改文本。
G(Globalize）命令将光标移动到指定行号，例如1G，移动到文本第一行
3. 编辑模式
1 插入文本 
  在一般模式下，按”I、i、a或A”进行文本插入，其中i命令用于在光标之前插入文本内容，I表示在行开始插入，a表示在光标之后插入，A则表示在行尾插入。
（I在这插入）Downloa(i在这插入）【光标在这】（a在这插入）ding（A在这插入）
2 新行输入
o表示在当行下的下方创建一个新行，O表示在当前行的上方打开一个新行
//O在光标所在行的上方插入新行
【光标在这】Downloading
//o在光标所在行的下方插入新行
3 文本替换 
命令r替换单个字符，替换完成后直接返回一般模式，命令R则连接已经文本替换，手动按ESC鍵后返回一般模式
4. 修改文本
在一般模式下进行文本的修改 
1 撤消修改
u撤消上一次修改
//修改前
Downloading
//修改后
DWDDDDDDDD
//在一般模式下，按u后
Downloading
U连续执行可以执行若干次撤消
2 删除字符
x删除光标右边的字符，每次删除一个，如果指定3x，则每次删除三个 
X删除光标左边的字符，每次删除一个，如果指定3X，则每次删除三个
3 批量删除 
常用删除命令有： 
  dd命令删除一行 
  dl删除字符，与x命令相同 
  d0从行首开始删除，到光标处为止 
  d^从行首第一个字符开始删除，到光标处为止 
  dw从当前光标处开始，删除到word的末尾 
  d3w从当前光标处开始，删除到第三个字符的末尾 
  d)从当前光标开始，删除到句子的末尾 
  d}从当前光标开始，删除到段落的末尾 
  d(从当前光标开始，删除到句子的开头 
  d{从当前光标开始，删除到段落的开头 
  D从当前光标开始，删除到行的末尾 
  d$与D等同
4 批量修改
常用命令： 
  cl修改当前字符 
  cw从光标处开始，修改到字的末尾 
  cb从word开始处修改，直到光标处 
  c)从当前光标开始，修改到句子的末尾 
  c}从当前光标开始，修改到段落的末尾 
  c(从当前光标开始，修改到句子的开头 
  c{从当前光标开始，修改到段落的开头 
  C从当前光标开始，修改到行的末尾 
  cc修改当前行 
  ncc修改从当前行开始的n行
5 文本替换
s 先删除当前字符，再输入替换字符 
   S 先删除当前行，再输入替换字符作为当前行
6 大小写转换 
    ~ 如果字符是大写，则自动转换成小写，如果是小写，则自动转换成大写
添加公众微信号，可以了解更多最新Spark、Scala相关技术资讯  


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

[置顶]
        常见的机器学习与数据挖掘知识点之常见分布
常见的机器学习与数据挖掘知识点之常见分布
Common Distribution(常见分布)：
Discrete Distribution(离散型分布)：

0-1 Distribution(0-1分布) 
定义：若随机变量XX只取00和11两个值，且其分布律为 
P{X=k}=pk(1−p)1−k,k=0,1P\{X=k\}=p^k(1-p)^{1-k}, k=0,1 
其中XX服从参数为p的(0−1)(0-1)分布，记作X∼(0−1)X \sim (0-1). 如抛掷硬币一次便服从两点分布. 
  两点分布的期望与方差分别为：p,1−pp, 1-p.
Geometric Distribution(几何分布) 
定义：若随机变量XX的可能取值为1,2,3,...1,2,3,...且它的分布律为 
P{X=k}=(1−p)k−1p=qk−1p,k=1,2,3,...P\{X=k\}=(1-p)^{k-1}p=q^{k-1}p, k=1,2,3,... 
则称随机变量XX服从参数p的几何分布，记作X∼G(p)X \sim G(p). 
  几何分布具有无记忆性，即： 
P{X>m+n|X>m}=P{X>n}P\{X>m+n|X>m\}=P\{X>n\} 
指几何分布对过去的m次失败的信息在后面的计算中被遗忘了. 
  几何分布对应于：XX为独立重复的贝努利试验这种，“首次成功”时的试验次数. 
  几何分布的期望与方差分别为：1p,1−pp2\frac{1}{p}, \frac{1-p}{p^2}.
Hypergeometric Distribution(超几何分布) 
定义：若随机变量XX的可能取值为0,1,2,....,n0,1,2,....,n，而且其分布律为 
P{X=m}=CmMCn−mN−MCnNP\{X=m\}=\frac{C_M^mC_{N-M}^{n-m}}{C_{N}^{n}} 
其中n,M,Nn,M,N都是正整数，且n≤N,M≤Nn \leq N, M \leq N. 上式中当m>Mm>M或n−m>N−Mn-m>N-M时，显然有PX=m=0P{X=m}=0，称这种分布为超几何分布，记作X∼H(n,M,N)X \sim H(n,M,N). 
  超几何分布对应与不返回抽样模型：NN个产品中有MM个不合格产品，从中抽取nn个，那么不合格的产品个数为XX. 
  超几何分布的期望与方差分别为：nMN,nMNN−MNN−nN−1\frac{nM}{N}, \frac{nM}{N}\frac{N-M}{N}\frac{N-n}{N-1}.
Bernoulli Distribution/Binomial Distribution(贝努利分布/二项分布) 
定义：设随机变量XX的可能取值为0,1,2,...,n0,1,2,...,n，其它的分布律为 
P{X=k}=Cknpk(1−p)n−kP\{X=k\}=C_n^kp^k(1-p)^{n-k} 
则称随机变量XX服从参数为n,pn,p的二项分布，记作X∼B(n,p)X \sim B(n,p)，它是nn重独立贝努利试验分布成功kk次，当n=1n=1时，其退化成0−10-1分布. 
  设随机变量X∼H(n,M,N) X \sim H(n,M,N)，则当N→∞N \to \infty时，XX近似地服从二项分布B(n,p)B(n,p)，即下面的近似等式成立. 
  二项分布的期望与方差分别为：np,np(1−p)np, np(1-p).
Negative Binomial Distribution(负二项分布，又称Pascal 帕斯卡分布) 
定义：若随机变量XX的可能取值为r,r+1,...r,r+1,...，而且其分布律为 
P{X=k}=Cr−1k−1(1−p)k−rpr,k=r,r+1,...P\{X=k\}=C_{k-1}^{r-1}(1-p)^{k-r}p^r, k=r,r+1,... 
其中,r,pr, p都是常数，那么称随机变量XX服从参数r,pr,p的负二项分布，记作X∼NB(r,p)X \sim NB(r,p). 
  负二项分布是：X为独立重复的贝努利试验中，“第rr次成功“时的试验次数.  
  负二项分布的期望与方差分别为：rp,r(1−p)p2\frac{r}{p}, \frac{r(1-p)}{p^2}. 
  二项随机变量时独立0−10-1随机变量之和. 
  在n重贝努利试验可看作由nn个相同的，独立进行的贝努利试验组成，若将第ii个贝努利试验中成功的次数记为Xi∼B(1,p),i=1,...,nX_i \sim B(1,p), i=1,...,n，nn重贝努利试验成功的总次数X=X1+X2+...+XnX=X_1+X_2+...+X_n，它服从B(n,p)B(n,p). 
  负二项随机变量时独立几何随机变量之和. 
  做一系列的贝努利试验，如果将首个成功出现的试验次数记为X1X_1，第二个成功出现时的试验次数(从第一次成功之后算起)记为X2X_2，……，第rr个成功出现时的试验次数记为XrX_r，则XiX_i独立同分布，且Xi∼G(p)X_i \sim G(p). 此时有X=X1+X2+...+Xn∼NB(r,p)X=X_1+X_2+...+X_n \sim NB(r,p).
Multinomial Distribution(多项分布) 
定义:：若mm维随机变量(X1,X2,...,Xm)(X_1,X_2,...,X_m)可能取值为(k1,K2,...,Km)(k_1,K_2,...,K_m)，而且其分布律为 
P{X1=k1,X2=k2,...,Xm=km}=n!k1!k2!...km!pk11pk22...pkmmP\{X_1=k_1,X_2=k_2,...,X_m=k_m\}=\frac{n!}{k_1!k_2!...k_m!}p_1^{k_1}p_2^{k_2}...p_m^{k_m} 
其中，∑mi=1ki=n\sum_{i=1}^m k_i=n，pip_i>0为试验结果是xix_i的概率，kik_i表示试验结果是xix_i的次数. 那么称随机变量(X1,X2,...,Xm)(X_1,X_2,...,X_m)服从多项分布，记作(X1,X2,...,Xm)∼M(n,p1,p2,...,pm)(X_1,X_2,...,X_m) \sim M(n, p_1, p_2,...,p_m). 
  通俗地说，假设一次随机试验取值范围可能为Spark修炼之道（基础篇）——Linux大数据开发基础：第六节：vi、vim编辑器（二）
本节主要内容

缓冲区的使用
文件的存盘与读盘
文本查找
文本替换

作者：周志湖  
微信号：zhouzhihubeyond  
网名：摇摆少年梦
1. 缓冲区的使用
在利用vim进行文本编辑时，编辑修改后的文本不会立即保存到硬盘上，而是保存在缓冲区中，如果没有把缓冲区里的文件存盘，原始文件不会被更改。vim在打开文件时将文本内容读到缓冲区中，在进行文本编辑时，修改的文本保存在缓冲区，此时硬盘上的原文件不变。下面让我们来演示一下缓冲区的使用。 
假设采用vim 同时打开两个文本文件：
root@ubuntu:/home/xtwy# vim test2.txt test1.txt
//打开文件后，默认打开的是test2.txt
//此时我们使用:buffers命令可以看查缓冲区
//得到如下结果
:buffers
  1 %a   "test2.txt"                    line 1
  2      "test1.txt"                    line 0


:buffers命令给出的是当前编辑中所有的缓冲区状态，前面的数字是缓冲区的数字标记，第二个标记就是缓冲区当前的状态，紧接着是与缓冲区所关联的文件名。缓冲区的状态有以下几种：

- （非活动的缓冲区）
a （激活缓冲区）
h （隐藏的缓冲区）
% （当前的缓冲区）
# （交换缓冲区）
= （只读缓冲区）
+ （已经更改的缓冲区）
在命令模式输入：open test1.txt进入test1.txt编辑界面，然后再输入:buffers查看缓冲区状态，得到如下结果
:buffers
  1 #    "test2.txt"                    line 1
  2 %a   "test1.txt"                    line 1


可以看到此时test1.txt加载为活动缓冲区，而test2.txt则被加载到交换缓冲区。此时利用:bprevious命令可以切换test2.txt为活动缓冲区，
 
执行后得到： 
 
可以看到，此时已经切换回到text2.txt，即将text2.txt加载到当前活动缓冲区当中，采用:buffers命令得到如下结果： 

更多缓冲区操作命令如下：
:buffers    电焊工缓冲区状态
:buffer 编辑指定缓冲区
:ball   编辑所有缓冲区
:bnext  到下一缓冲区
:bprevious  到前一缓冲区
:blast  到最后一个缓冲区
:bfirst 到第一个缓冲区
:badd   增加缓冲区
:bdelete    删除缓冲区
:bunload    卸载缓冲区
2. 文件的存盘与读盘
（一)保存并退出
在编辑模式中，如果文本编辑任务已经完成，想直接保存退出，返回到Linux CLI命令行的话，直接按ZZ即可。
（二)读取文件内容到缓冲区
在编辑模式中，采用:r命令读取文件内容到当前缓冲区， 
 
:r test1.txt可以test1.txt文件内容写到缓冲区 

（三)将缓冲区内容写到文件
在编辑模式中，采用：w命令将修改后的文件写到磁盘，也可以使用:wq命令将修改的文件写到磁盘上后退出vim返回inux CLI，如果不想保存直接退出，则使用:q!命令直接退出vim，返回到CLI命令行。
3. 文本查找
（1）一般搜索
使用?或/进行字符串查找，例如： 
 
回车之后，光标将定位到下一个Spark上，如果还想往下搜索，则按n（next），如果想往上搜索，则按N
（2）正则表达式搜索
正则表达式搜索是指加入了像”^,$,.”等特殊匹配字符，它们的作用如下表：



搜索字符串
搜索描述
举例



:/^Spark
搜索以Spark为开头的行
Spark is ….


:/YARN$
搜索以YARN为结尾的行
…Hadoop YARN


:/Ha…p
搜索Ha开头，中间有三个字符且以p结尾的字符串
Hadoop、Hadaap


:/e>
查找以e结尾的字符串，其中>符号是字符串结束指示符号，这里\不是转义字符，而是与>组合到一起，来表示特殊意义
like、source


:/\<Had
查找以Had作为开始的字符串，\< 同样具有特殊意义
Hadoop、Hadoo


:/Spa*
查看字符串中出现至少一次Spar的字符串，\< 同样具有特殊意义
Spark、SpaSpark


:/Sp[ae]rk
匹配Spark或Sperk
Spark、Sperk


4. 文本替换
文本替换使用以下语法格式：
:[g][address]s/search-string/replace-string[/option]
其中address用于指定替换范围，下表给出的是常用示例：
//将当前缓冲区的第一行中的Downloading替换为Download
: 1 s/Downloading/Download

//将当前缓冲区中的第一行到第五行中的Spark替换为spark
:1,5 s/Spark/spark

//将当前缓冲区中的第一行到当前光标所在行的Spark替换为spark
:1,. s/Spark/spark

//将当前光标所在行到缓冲区最后一行的Spark替换为spark
:.,$ s/Spark/spark

//将整个缓冲区中的Spark替换为spark
:% s/Spark/spark

//当前行中第一次搜索到的Spark替换为spark
: s/Spark/spark

//将当前行中所有的Spark替换为spark
:s/Spark/spark/g  

//将所有的and转换成And，不包括theta这种字符串，只会作用于the这种单独存在的字符串
:% s/\<the\>/The/g  

添加公众微信号，可以了解更多最新Spark、Scala相关技术资讯  


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Java Web提交任务到Spark
相关软件版本：Spark1.4.1 ，Hadoop2.6，Scala2.10.5 , MyEclipse2014，intelliJ IDEA14，JDK1.8，Tomcat7机器：windows7 （包含JDK1.8，MyEclipse2014，IntelliJ IDEA14，TOmcat7）；centos6.6虚拟机（Hadoop伪分布式集群，Spark standAlone集群，JDK1.8）；centos7虚拟机（Tomcat，JDK1.8）；1. 场景：1. windows简单java程序调用Spark，执行Scala开发的Spark程序，这里包含两种模式：    1> 提交任务到Spark集群，使用standAlone模式执行；    2> 提交任务到Yarn集群，使用yarn-client的模式；2. windows 开发java web程序调用Spark，执行Scala开发的Spark程序，同样包含两种模式，参考1.3. linux运行java web程序调用Spark，执行Scala开发的Spark程序，包含两种模式，参考1.2. 实现：1. 简单Scala程序，该程序的功能是读取HDFS中的log日志文件，过滤log文件中的WARN和ERROR的记录，最后把过滤后的记录写入到HDFS中，代码如下：import org.apache.spark.{SparkConf, SparkContext}


/**
 * Created by Administrator on 2015/8/23.
 */
object Scala_Test {
  def main(args:Array[String]): Unit ={
    if(args.length!=2){
      System.err.println("Usage:Scala_Test <input> <output>")
    }
    // 初始化SparkConf
    val conf = new SparkConf().setAppName("Scala filter")
    val sc = new SparkContext(conf)

    //  读入数据
    val lines = sc.textFile(args(0))

    // 转换
    val errorsRDD = lines.filter(line => line.contains("ERROR"))
    val warningsRDD = lines.filter(line => line.contains("WARN"))
    val  badLinesRDD = errorsRDD.union(warningsRDD)

    // 写入数据
    badLinesRDD.saveAsTextFile(args(1))

    // 关闭SparkConf
    sc.stop()
  }
}
使用IntelliJ IDEA 并打成jar包备用（lz这里命名为spark_filter.jar）;2.  java调用spark_filter.jar中的Scala_Test 文件，并采用Spark standAlone模式，java代码如下：package test;

import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.spark.deploy.SparkSubmit;
/**
 * @author fansy
 *
 */
public class SubmitScalaJobToSpark {

	public static void main(String[] args) {
		SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd-hh-mm-ss"); 
		String filename = dateFormat.format(new Date());
		String tmp=Thread.currentThread().getContextClassLoader().getResource("").getPath();
		tmp =tmp.substring(0, tmp.length()-8);
		String[] arg0=new String[]{
				"--master","spark://node101:7077",
				"--deploy-mode","client",
				"--name","test java submit job to spark",
				"--class","Scala_Test",
				"--executor-memory","1G",
//				"spark_filter.jar",
				tmp+"lib/spark_filter.jar",//
				"hdfs://node101:8020/user/root/log.txt",
				"hdfs://node101:8020/user/root/badLines_spark_"+filename
		};
		
		SparkSubmit.main(arg0);
	}
}
具体操作，使用MyEclipse新建java web工程，把spark_filter.jar 以及spark-assembly-1.4.1-hadoop2.6.0.jar（该文件在Spark压缩文件的lib目录中，同时该文件较大，拷贝需要一定时间） 拷贝到WebRoot/WEB-INF/lib目录。（注意：这里可以直接建立java web项目，在测试java调用时，直接运行java代码即可，在测试web项目时，开启tomcat即可）java调用spark_filter.jar中的Scala_Test 文件，并采用Yarn模式。采用Yarn模式，不能使用简单的修改master为“yarn-client”或“yarn-cluster”，在使用Spark-shell或者spark-submit的时候，使用这个，同时配置HADOOP_CONF_DIR路径是可以的，但是在这里，读取不到HADOOP的配置，所以这里采用其他方式，使用yarn.Clent提交的方式，java代码如下：package test;

import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.spark.SparkConf;
import org.apache.spark.deploy.yarn.Client;
import org.apache.spark.deploy.yarn.ClientArguments;

public class SubmitScalaJobToYarn {

	public static void main(String[] args) {
		SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd-hh-mm-ss"); 
		String filename = dateFormat.format(new Date());
		String tmp=Thread.currentThread().getContextClassLoader().getResource("").getPath();
		tmp =tmp.substring(0, tmp.length()-8);
		String[] arg0=new String[]{
				"--name","test java submit job to yarn",
				"--class","Scala_Test",
				"--executor-memory","1G",
//				"WebRoot/WEB-INF/lib/spark_filter.jar",//
				"--jar",tmp+"lib/spark_filter.jar",//
				
				"--arg","hdfs://node101:8020/user/root/log.txt",
				"--arg","hdfs://node101:8020/user/root/badLines_yarn_"+filename,
				"--addJars","hdfs://node101:8020/user/root/servlet-api.jar",//
				"--archives","hdfs://node101:8020/user/root/servlet-api.jar"//
		};
		
//		SparkSubmit.main(arg0);
		Configuration conf = new Configuration();
		String os = System.getProperty("os.name");
		boolean cross_platform =false;
		if(os.contains("Windows")){
			cross_platform = true;
		}
		conf.setBoolean("mapreduce.app-submission.cross-platform", cross_platform);// 配置使用跨平台提交任务
		conf.set("fs.defaultFS", "hdfs://node101:8020");// 指定namenode
		conf.set("mapreduce.framework.name","yarn"); // 指定使用yarn框架
		conf.set("yarn.resourcemanager.address","node101:8032"); // 指定resourcemanager
		conf.set("yarn.resourcemanager.scheduler.address", "node101:8030");// 指定资源分配器
		conf.set("mapreduce.jobhistory.address","node101:10020");
		
		 System.setProperty("SPARK_YARN_MODE", "true");

		 SparkConf sparkConf = new SparkConf();
		 ClientArguments cArgs = new ClientArguments(arg0, sparkConf);
		
		new Client(cArgs,conf,sparkConf).run();
	}
}
3. java web测试 任务提交到Spark的两种模式，这里采用最简单的方式，直接配置servlet，其web.xml文件如下：<?xml version="1.0" encoding="UTF-8"?>
<web-app version="3.0"
    xmlns="http://java.sun.com/xml/ns/javaee"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd">
  <servlet>
    <description>This is the description of my J2EE component</description>
    <display-name>This is the display name of my J2EE component</display-name>
    <servlet-name>SparkServlet</servlet-name>
    <servlet-class>servlet.SparkServlet</servlet-class>
  </servlet>
  <servlet>
    <description>This is the description of my J2EE component</description>
    <display-name>This is the display name of my J2EE component</display-name>
    <servlet-name>YarnServlet</servlet-name>
    <servlet-class>servlet.YarnServlet</servlet-class>
  </servlet>


  <servlet-mapping>
    <servlet-name>SparkServlet</servlet-name>
    <url-pattern>/servlet/SparkServlet</url-pattern>
  </servlet-mapping>
  <servlet-mapping>
    <servlet-name>YarnServlet</servlet-name>
    <url-pattern>/servlet/YarnServlet</url-pattern>
  </servlet-mapping>

</web-app>SparkServlet如下：package servlet;

import java.io.IOException;
import java.io.PrintWriter;

import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import test.SubmitScalaJobToSpark;

public class SparkServlet extends HttpServlet {

	/**
	 * Constructor of the object.
	 */
	public SparkServlet() {
		super();
	}

	/**
	 * Destruction of the servlet. <br>
	 */
	public void destroy() {
		super.destroy(); // Just puts "destroy" string in log
		// Put your code here
	}

	/**
	 * The doGet method of the servlet. <br>
	 *
	 * This method is called when a form has its tag value method equals to get.
	 * 
	 * @param request the request send by the client to the server
	 * @param response the response send by the server to the client
	 * @throws ServletException if an error occurred
	 * @throws IOException if an error occurred
	 */
	public void doGet(HttpServletRequest request, HttpServletResponse response)
			throws ServletException, IOException {

		this.doPost(request, response);
	}

	/**
	 * The doPost method of the servlet. <br>
	 *
	 * This method is called when a form has its tag value method equals to post.
	 * 
	 * @param request the request send by the client to the server
	 * @param response the response send by the server to the client
	 * @throws ServletException if an error occurred
	 * @throws IOException if an error occurred
	 */
	public void doPost(HttpServletRequest request, HttpServletResponse response)
			throws ServletException, IOException {
		System.out.println("开始SubmitScalaJobToSpark调用......");
		SubmitScalaJobToSpark.main(null);
		//YarnServlet也只是这里不同
		System.out.println("完成SubmitScalaJobToSpark调用！");
		response.setContentType("text/html");
		PrintWriter out = response.getWriter();
		out.println("<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">");
		out.println("<HTML>");
		out.println("  <HEAD><TITLE>A Servlet</TITLE></HEAD>");
		out.println("  <BODY>");
		out.print("    This is ");
		out.print(this.getClass());
		out.println(", using the POST method");
		out.println("  </BODY>");
		out.println("</HTML>");
		out.flush();
		out.close();
	}

	/**
	 * Initialization of the servlet. <br>
	 *
	 * @throws ServletException if an error occurs
	 */
	public void init() throws ServletException {
		// Put your code here
	}

}
这里只是调用了java编写的任务调用类而已。同时，SparServlet和YarnServlet也只是在调用的地方不同而已。在web测试时，首先直接在MyEclipse上测试，然后拷贝工程WebRoot到centos7，再次运行tomcat，进行测试。3. 总结及问题1. 测试结果：   1> java代码直接提交任务到Spark和Yarn，进行日志文件的过滤，测试是成功运行的。可以在Yarn和Spark的监控中看到相关信息：同时，在HDFS可以看到输出的文件：2> java web 提交任务到Spark和Yarn，首先需要把spark-assembly-1.4.1-hadoop2.6.0.jar中的javax.servlet文件夹删掉，因为会和tomcat的servlet-api.jar冲突。    a. 在windows和linux上启动tomcat，提交任务到Spark standAlone，测试成功运行；    b. 在windows和linux上启动tomcat，提交任务到Yarn，测试失败；2. 遇到的问题：    1> java web 提交任务到Yarn，会失败，失败的主要日志如下：15/08/25 11:35:48 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoClassDefFoundError: javax/servlet/http/HttpServletResponse
java.lang.NoClassDefFoundError: javax/servlet/http/HttpServletResponse这个是因为javax.servlet的包被删掉了，和tomcat的冲突。同时，在日志中还可以看到：15/08/26 12:39:27 INFO Client: Setting up container launch context for our AM
15/08/26 12:39:27 INFO Client: Preparing resources for our AM container
15/08/26 12:39:27 INFO Client: Uploading resource file:/D:/workspase_scala/SparkWebTest/WebRoot/WEB-INF/lib/spark-assembly-1.4.1-hadoop2.6.0.jar -> hdfs://node101:8020/user/Administrator/.sparkStaging/application_1440464833795_0012/spark-assembly-1.4.1-hadoop2.6.0.jar
15/08/26 12:39:32 INFO Client: Uploading resource file:/D:/workspase_scala/SparkWebTest/WebRoot/WEB-INF/lib/spark_filter.jar -> hdfs://node101:8020/user/Administrator/.sparkStaging/application_1440464833795_0012/spark_filter.jar
15/08/26 12:39:33 INFO Client: Uploading resource file:/C:/Users/Administrator/AppData/Local/Temp/spark-46820caf-06e0-4c51-a479-3bb35666573f/__hadoop_conf__5465819424276830228.zip -> hdfs://node101:8020/user/Administrator/.sparkStaging/application_1440464833795_0012/__hadoop_conf__5465819424276830228.zip
15/08/26 12:39:33 INFO Client: Source and destination file systems are the same. Not copying hdfs://node101:8020/user/root/servlet-api.jar
15/08/26 12:39:33 WARN Client: Resource hdfs://node101:8020/user/root/servlet-api.jar added multiple times to distributed cache.
这里在环境初始化的时候，上传了两个jar，一个就是spark-assembly-1.4.1-hadoop2.6.0.jar 还有一个就是我们自定义的jar。上传的spark-assembly-1.4.1-hadoop2.6.0.jar 里面没有javax.servlet的文件夹，所以会报错。在java中直接调用（没有删除javax.servlet的时候）同样会看到这样的日志，同样的上传，那时是可以的，也就是说这里确实是删除了包文件夹的关系。那么如何修复呢？上传servlet-api到hdfs，同时在使用yarn.Client提交任务的时候，添加相关的参数，这里查看参数，发现两个比较相关的参数，--addJars以及--archive 参数，把这两个参数都添加后，看到日志中确实把这个jar包作为了job的共享文件，但是java web提交任务到yarn 还是报这个类找不到的错误。所以这个办法也是行不通！使用yarn.Client提交任务到Yarn参考http://blog.sequenceiq.com/blog/2014/08/22/spark-submit-in-java/ 。分享，成长，快乐脚踏实地，专注转载请注明blog地址：http://blog.csdn.net/fansy1990

版权声明：本文为博主原创文章，未经博主允许不得转载。

探索中国软件的智造之路
﻿﻿
    1973年，瓦特制造出第一台实用性蒸汽机，第一次工业革命的大幕就此拉开。它改变了每一个人甚至是每一个国家的命运，凭借着瓦特们提供的能量，英国开始了对世界的统治。1866年德国人西门子制成发电机，1870年比利时人格拉姆发明电动机，电力开始用于带动机器，也将人类拉入电气这个全新的时代。20世纪中期，人类历史的舞台上又诞生了一个足以颤动世界的伟大发明，这就是互联网，它使人类真正进入信息共享时代，也有人将它称为第三次工业革命。而人类在探索继续向前发展的脚步从未停止……2013年，德国率先提出工业4.0战略，以智能制造主导第四次工业革命，而中国也第一次与发达国家站在了同一起跑线。2015年5月，中国版的工业4.0规划--《中国制造2025》出台。这是我国实施制造强国战略的第一个十年行动纲领。《中国制造2025》以工业化、信息化两化融合为主线，这也是未来中国制造业的基础所在。
    工信部软件司司长陈伟表示：“软件业作为信息技术产业中增长最快、创新最活跃的部分，成为两化深度融合的核心。中国作为全球IT产业大国和重要的IT市场，要充分发挥软件的引领带动作用，推动智能制造的发展，为实现经济高速发展贡献力量。”自《中国制造2025》作为国家战略实施以来，中国制造转型升级成为经济社会发展中的一大热点。在中国制造由大变强的进程中，软件业将迎来巨大发展机遇。中国制造业依靠资源要素投入的发展模式难以为继，亟待向智能制造发展。从某种程度上说，软件业是智能制造的突破口，对智能制造具有核心驱动作用。
    近年来，中国软件产业取得了长足发展。去年，我国软件和信息技术服务业实现软件业务收入3.7万亿元，同比增长20.2%。今年1-5月，实现收入近1.6万亿元，同比增长17.1%，整体保持了平稳增长态势。但是我们也要清醒的认识到，我国软件业自身仍面临大而不强的难题，在助推智能制造过程中的市场空间巨大，任务也很艰巨。
    软件业要对自己的地位和肩负的责任有一个清晰的认识。在智能制造的大环境下，越来越多的企业需要依靠软件技术来实现智能化，将技术注入机器设备中，使其更加智能、自主的进行生产，从而提高技术水平、产品质量和生产效率。在为全行业提供技术支撑的同时，软件行业自身更应率先走向智能制造之路。不久前，华为提出1+2+1的智能制造解决方案，将碎片化的市场聚合，发挥物联网的无限可能。中望软件运用先进的设备和ERP、CAD/CAM、PDM等软件系统进行生产管理和产品设计，用整体解决方案代替单一的产品或服务，满足制造企业在新形势下的应用需求。OCCS软件云工厂则从改变传统软件企业的开发模式入手，将编程人员聚集于平台，通过工厂流水线的作业革新生产方式，使生产、组织、管理更加高效，从而推动软件企业走向智能化。
    借助工业4.0时代按需生产的趋势，宝马等国外知名汽车厂商纷纷提出汽车个性化定制业务。而在《中国制造2025》中也十分重视这一部分，鼓励企业发展个性定制服务。海尔互联工厂为用户提供个性化的定制家电。OCCS软件云工厂则为客户提供定制的软件产品，更具针对性的软件产品可以更加快速和方便的推进企业乃至行业的智能化进程。
    《中国制造2025》下，智能制造成为大势所趋，而在这其中软件行业的地位更是举足轻重，它的道路能否走好与各个行业息息相关。

版权声明：本文为博主原创文章，未经博主允许不得转载。

在 Databricks  可获得 Spark 1.5 预览版

我们兴奋地宣布,从今天开始,Apache Spark1.5.0的预览数据砖是可用的。我们的用户现在可以选择提供集群与Spark 1.5或先前的火花版本准备好几个点击。

正式,Spark 1.5预计将在数周内公布,和社区所做的QA测试的版本。鉴于火花的快节奏发展,我们觉得这是很重要的,使我们的用户尽快开发和利用新特性。与传统的本地软件部署,它可以需要几个月,甚至几年,从供应商收到软件更新。数据砖的云模型,我们可以在几小时内更新,让用户试他们的火花版本的选择。




What’s New?

The last few releases of Spark focus on making data science more accessible, through high-level programming APIs such as DataFrames, machine
 learning pipelines, and R
 language support. A large part of Spark 1.5, on the other hand, focuses on under-the-hood changes to
 improve Spark’s performance, usability, and operational stability.

Spark 1.5 delivers the first phase of Project Tungsten,
 a new execution backend for DataFrames/SQL. Through code generation and cache-aware algorithms, Project Tungsten improves the runtime performance with out-of-the-box configurations. Through explicit memory management and external operations, the new backend
 also mitigates the inefficiency in JVM garbage collection and improves robustness in large-scale workloads.



Over the next few weeks, we will be writing about Project Tungsten. To give you a sneak peek, the above chart compares the out-of-the-box (i.e. no configuration changes) performance of an aggregation query (16 million records and 1 million composite keys) using
 Spark 1.4 and Spark 1.5 on my laptop.

Streaming workloads typically run 24/7 and have stringent stability requirements. In this release, Typesafe has introduced Backpressure in
 Spark Streaming. With this feature, Spark Streaming can dynamically control the data ingest rates to adapt to unpredictable variations in processing load. This allows streaming applications to be more robust against bursty workloads and downstream delays.

Of course, Spark 1.5 is the work of more than 220 open source contributors from over 80 organizations, and includes a lot more than the above two. Some examples include:

New machine learning algorithms: multilayer perceptron classifier, PrefixSpan for sequential pattern mining, association rule generation, etc.Improved R language support and GLMs with R formula.Better instrumentation and reporting of memory usage in web UI.

Stay tuned for future blog posts covering the release as well as deep dives into specific improvements.

How do I use it?

Launching a Spark 1.5 cluster is as easy as selecting Spark 1.5 experimental version in the cluster creation interface in Databricks.



Once you hit confirm, you will get a Spark cluster ready to go with Spark 1.5.0 and start testing the new release. Multiple
 Spark version support in Databricks also enables users to run Spark 1.5 canary clusters side-by-side with existing production Spark clusters.

You can find the work-in-progress documentation for Spark 1.5.0
 here. Please be aware that just like any other preview software, Spark 1.5.0 support is experimental. There will be bugs and quirks that we find and fix in the next couple of weeks. The good news is that you don’t have to worry about following the development
 or upgrading yourself. As we discover and fix bugs in the open source project, the Spark 1.5 option in Databricks will also be updated automatically. If you encounter a bug, please report it by filing
 a JIRA ticket.

To try Databricks, sign up for a free 30-day trial.

在上一次北京sparkmeetup技术分享会上，一个spark commiter就说他们忙着Spark 1.5（核心工作就说Tungsten），一个新的DataFrames / SQL执行后端。项目支持缓存通过代码生成算法,提高运行时性能与Tungsten的开箱即用配置。通过显式的内存管理和外部操作,新的后端也减轻了低效JVM的垃圾收集,提高了鲁棒性在大规模的工作负载

目前来看，spark1.5第一阶段目前是完成，估计后期应该有很多优化和代码修复，但可尝尝甜头，如果想了解1.5版本代码，看github spark1.5 branch，个人感觉 主要还是spark sql的提升吧，因为大多数公司都是 spark on yarn的方式，大多数任务提升希望在spark sql上面

版权声明：本文为博主原创文章，未经博主允许不得转载。

单源最短路径算法的MapReduce实现(Metis版本)
1. Mapreduce框架
1.1 Mapreduce介绍
Mapreduce 是谷歌提出的一个分布式计算框架， 利用该框架， 能够让用户方便地利用多机并行处理数据。 该框架有两个重要的函数： Map 和 Reduce， Map 函数对整个输入数据进行处理， 按照用户定义的处理方式， 从输入的数据中产生中间键值对（ key， value）。Reduce 函数对这些键值对进行处理， 相同 key 的键值对由同一个 Reduce 进程进行处理。最终将处理的结果进行合并。 整个处理流程如下： 

1.2 Phoenix & Metis
Phoenix 是由斯坦福大学基于多核多处理器系统实现的一套 mapreduce 框架， Phoenix框架能够自动管理线程的创建， 动态任务调度， 数据划分和故障容错。 本质上与用户写的多线程程序没有区别， 只不过通过这个框架， 能够更容易编写出能够划分为“map-reduce-merge”模式的数据处理业务。
Metis 框架是在 Phoenix 的基础上改进而来， Phoenix 中在 map 阶段使用哈希表， 对于哈希表中的每条条目使用排序数组进行保存， 而 Metis 采用了 BTree 的方式进行了代替， 以此来提高速度。 


2. Metis 框架
2.1 三种数据处理模式

map_reduce
map_group
map_only

2.2 框架使用
struct SPFA: public map_reduce{
    bool split(split_t *out, int ncores);
    void map_function(split_t *ma);
    void reduce_function(void *k, void **v, size_t length);
    int key_compare(const void *s1, const void *s2);
};

split： 数据切分函数， 在这里根据 ncores 参数， 用户 自定义如何划分数据， 并将数据切分信息保存在 out 中。
Map_funcion:对每个数据分片进行处理， 产生键值对。
Reduce_function:对键值对进行处理。
Key_compare: 用户自定义键值对的比较函数

2.3 程序执行流程
    SPFA app;
    app.set_reduce_task(reduce_tasks);
    app.set_ncore(nprocs);
    mapreduce_appbase::initialize();
    app.sched_run();
    mapreduce_appbase::deinitialize();

set_reduce_task: 设置 reduce 线程的数量
set_ncore: 设置使用的核数
sched_run: 程序启动， 内部会先后调用数据切分， map， reduce， merge 等操作
map 线程数量： 由数据切分函数间接控制， 数据分块的数量对应 map_tasks。


3. 单源最短路经算法
3.1 SPFA 算法
SPFA 的思想十分简单， 简单说是加队列优化后的 bellman-ford 算法， 利用松弛操作， 更新距离。 通过引入队列， 有以下几个优化：

减少松弛操作： bellman-ford 算法每次迭代中， 不需利用所有的点对其他的点进行松弛操作， 减少了松弛次数。
负环判断： 通过点进入队列的次数， 可以判断是否存在负环， 如果进入队列 n 次， 则说明存在负环。

3.2 Dijkstra 算法
Dijkstra 算法采用贪心的方式， 每次从更新的节点中， 选出最小并且该点的值不可能再由剩下的节点来更新(无负环的情况下)。

4. 实现设计
4.1 SPFA算法实现
图的存储方式： 二维数组 
数据处理模式： map_only 
Mapreduce 处理过程：

数据划分： 将整个图的节点集合按 map_tasks 数量进行均匀划分， 每个 map 处理其中一部分。
Map： 在一个数据分片中， 使用更新队列[1]^{[1]}中所有的点对当前数据分片进行松弛操作， 如果出现更新操作， 在标记数组[2]^{[2]}中对该点进行记录， 并修改 dist[3]^{[3]}数组的值。
替换更新队列： 清空更新队列， 将标记数组中标记过的节点加入更新队列， 用 
于下一次迭代对其他节点进行松弛操作。
如果更新队列为空， 迭代结束， 否则继续下一次迭代。

说明: 
 [1] 更新队列： 节点被更新后放入的队列， 依次使用该队列中的节点对整个图的节点进行松弛操作。 
 [2] 标记数组： 用来标记该节点在当前迭代中是否被更新过， 作为是否放入下一次迭代中更新队列的依据。 
[3]  dist 数组： 源点到其他点的最近距离， 初始值为无穷大。
4.2 Dijkstra 算法实现
图的存储方式： 二维数组 
数据处理模式： map_reduce 
Mapreduce 处理过程：

数据划分： 将整个图的节点集合按 map_tasks 数量进行均匀划分， 每个 map 处理其中一部分。
Map： 在一个数据分片中， 使用当前未使用中离源点最近的点来对数据分片中 
点的距离进行松弛操作， 如果出现更新操作， 发送以(点的编号%reduce_tasks)的值作为 key，点的编号作为 value 的键值对。
Reduce： 在每个 reduce 中， 找出当前离源点最近的点， 发送(点的编号， 点的距离)的键值对。
寻找最近的点： 在最终处理完的结果数组 result_中,挑选出离源点最近的点， 用 
来在下一次迭代中更新其他点。 如果 result_数组为空， 退出迭代。 
说明： 将整个找离源点最近的点分为了两个阶段， 先在 reduce 阶段求出了局部最近， 最后在局部最近的结果集中求出全局最近的点。


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

spark core源码分析3 Master HA
这一节讲解master 选举以及之后的处理流程
上一节说到在Master启动过程中，首先调用了 Akka actor的preStart方法。

override def preStart() {
  logInfo("Starting Spark master at " + masterUrl)
  logInfo(s"Running Spark version ${org.apache.spark.SPARK_VERSION}")
  // Listen for remote client disconnection events, since they don't go through Akka's watch()
  context.system.eventStream.subscribe(self, classOf[RemotingLifecycleEvent])
  webUi.bind()
  masterWebUiUrl = "http://" + masterPublicAddress + ":" + webUi.boundPort
  context.system.scheduler.schedule(0 millis, WORKER_TIMEOUT millis, self, CheckForWorkerTimeOut)

  masterMetricsSystem.registerSource(masterSource)
  masterMetricsSystem.start()
  applicationMetricsSystem.start()
  // Attach the master and app metrics servlet handler to the web ui after the metrics systems are
  // started.
  masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)
  applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)

  //HA的流程从这里开始
  //这里可以选择Master的元数据信息保存在哪里，我们以ZK为例讲解
  //这里用Apache Curator作为zk的client，它包装了zk client 复杂的api
  val (persistenceEngine_, leaderElectionAgent_) = RECOVERY_MODE match {
    case "ZOOKEEPER" =>
      logInfo("Persisting recovery state to ZooKeeper")
      val zkFactory =
        new ZooKeeperRecoveryModeFactory(conf, SerializationExtension(context.system))
      (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(this))
    case "FILESYSTEM" =>
      val fsFactory =
        new FileSystemRecoveryModeFactory(conf, SerializationExtension(context.system))
      (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(this))
    case "CUSTOM" =>
      val clazz = Class.forName(conf.get("spark.deploy.recoveryMode.factory"))
      val factory = clazz.getConstructor(classOf[SparkConf], classOf[Serialization])
        .newInstance(conf, SerializationExtension(context.system))
        .asInstanceOf[StandaloneRecoveryModeFactory]
      (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(this))
    case _ =>
      (new BlackHolePersistenceEngine(), new MonarchyLeaderAgent(this))
  }
  persistenceEngine = persistenceEngine_
  leaderElectionAgent = leaderElectionAgent_
}
上面的persistenceEngine_封装了在zk中读写元数据信息，以及序列化反序列化的接口
leaderElectionAgent_封装了master的选举过程，见下面代码注释中的解释
private[master] class ZooKeeperLeaderElectionAgent(val masterActor: LeaderElectable,
    conf: SparkConf) extends LeaderLatchListener with LeaderElectionAgent with Logging  {

  //依赖zk中的一个节点来判断选主
  val WORKING_DIR = conf.get("spark.deploy.zookeeper.dir", "/spark") + "/leader_election"

  private var zk: CuratorFramework = _
  private var leaderLatch: LeaderLatch = _
  private var status = LeadershipStatus.NOT_LEADER

  //构造这个对象之后就调用了start方法
  start()
  //leaderLatch.start()一旦调用，LeaderLatch会和其它使用相同latch path的其它LeaderLatch交涉，然后随机的选择其中一个作为leader
  private def start() {
    logInfo("Starting ZooKeeper LeaderElection agent")
    zk = SparkCuratorUtil.newClient(conf)
    leaderLatch = new LeaderLatch(zk, WORKING_DIR)
    leaderLatch.addListener(this)
    leaderLatch.start()
  }

  override def stop() {
    leaderLatch.close()
    zk.close()
  }

  //当一个master被选为主时，isLeader方法被回调，说明在这一轮选举中胜出
  override def isLeader() {
    synchronized {
      // could have lost leadership by now.
      if (!leaderLatch.hasLeadership) {
        return
      }

      logInfo("We have gained leadership")
      updateLeadershipStatus(true)
    }
  }

  //当一个master被选为备时，notLeader方法被回调，说明在这一轮选举中落败
  override def notLeader() {
    synchronized {
      // could have gained leadership by now.
      if (leaderLatch.hasLeadership) {
        return
      }

      logInfo("We have lost leadership")
      updateLeadershipStatus(false)
    }
  }
  private def updateLeadershipStatus(isLeader: Boolean) {
    //当一个master之前状态为备，目前被选为主
    if (isLeader && status == LeadershipStatus.NOT_LEADER) {
      status = LeadershipStatus.LEADER
      masterActor.electedLeader()//调用master类的electedLeader方法	
      //当一个master之前状态为主，目前被选为备
    } else if (!isLeader && status == LeadershipStatus.LEADER) {
      status = LeadershipStatus.NOT_LEADER
      masterActor.revokedLeadership()//调用master类的revokedLeadership方法	
    }
  }

  private object LeadershipStatus extends Enumeration {
    type LeadershipStatus = Value
    val LEADER, NOT_LEADER = Value
  }
}继续查看master中的逻辑
override def receiveWithLogging: PartialFunction[Any, Unit] = {
  case ElectedLeader => {
    //既然之前是备，现在想变成主，就需要读取zk中的必要的信息来构造元数据
    val (storedApps, storedDrivers, storedWorkers) = persistenceEngine.readPersistedData()
    state = if (storedApps.isEmpty && storedDrivers.isEmpty && storedWorkers.isEmpty) {
      RecoveryState.ALIVE//如果没有任何元数据需要构造，则直接置为alive状态
    } else {
      RecoveryState.RECOVERING//不然需要置为恢复中
    }
    logInfo("I have been elected leader! New state: " + state)
    if (state == RecoveryState.RECOVERING) {
      beginRecovery(storedApps, storedDrivers, storedWorkers)//见下面介绍
      recoveryCompletionTask = context.system.scheduler.scheduleOnce(WORKER_TIMEOUT millis, self,
        CompleteRecovery)
    }
  }

  case CompleteRecovery => completeRecovery()

  //之前是主，现在被置为备了，不需要额外操作，退出即可
  case RevokedLeadership => {
    logError("Leadership has been revoked -- master shutting down.")
    System.exit(0)
  }开始恢复private def beginRecovery(storedApps: Seq[ApplicationInfo], storedDrivers: Seq[DriverInfo],
    storedWorkers: Seq[WorkerInfo]) {
  for (app <- storedApps) {
    logInfo("Trying to recover app: " + app.id)
    try {
      registerApplication(app)//将读到的app加载到内存
      app.state = ApplicationState.UNKNOWN//状态置为unknown
      app.driver ! MasterChanged(masterUrl, masterWebUiUrl)//向driver发送MasterChanged消息
    } catch {
      case e: Exception => logInfo("App " + app.id + " had exception on reconnect")
    }
  }

  for (driver <- storedDrivers) {
    // Here we just read in the list of drivers. Any drivers associated with now-lost workers
    // will be re-launched when we detect that the worker is missing.
    drivers += driver//将读到的driver加载到内存
  }

  for (worker <- storedWorkers) {
    logInfo("Trying to recover worker: " + worker.id)
    try {
      registerWorker(worker)//将读到的worker信息加载到内存
      worker.state = WorkerState.UNKNOWN//同样状态需要置为unknown，需要等到worker发送消息过来之后才能认为该worker是可用的
      worker.actor ! MasterChanged(masterUrl, masterWebUiUrl)//向worker发送MasterChanged消息
    } catch {
      case e: Exception => logInfo("Worker " + worker.id + " had exception on reconnect")
    }
  }
}看driver端收到MasterChanged消息会发生什么？在AppClient.scala中只有主master会发送MasterChanged消息，所以这里的masterUrl肯定是新的主master的case MasterChanged(masterUrl, masterWebUiUrl) =>
  logInfo("Master has changed, new master is at " + masterUrl)
  //收到这个消息之后，driver需要修改之前保存的master信息，用于之后向新的master通信
  changeMaster(masterUrl)
  alreadyDisconnected = false
  sender ! MasterChangeAcknowledged(appId)//向master反馈MasterChangeAcknowledged消息master这时会收到所有app中driver发来的消息，我们看master收到MasterChangeAcknowledged消息的处理方式，参数为appIdcase MasterChangeAcknowledged(appId) => {
  idToApp.get(appId) match {
    case Some(app) =>
      logInfo("Application has been re-registered: " + appId)
      app.state = ApplicationState.WAITING  //收到消息后将app状态置为WAITING
    case None =>
      logWarning("Master change ack from unknown app: " + appId)
  }

  if (canCompleteRecovery) { completeRecovery() }  //这个只是优先判断消息处理是否都结束了，这样就不用等待worker_timeout的时间间隔再调用completeRecovery了
}看worker端收到MasterChanged消息会发生什么？在Worker.scala中case MasterChanged(masterUrl, masterWebUiUrl) =>
  logInfo("Master has changed, new master is at " + masterUrl)
  changeMaster(masterUrl, masterWebUiUrl)//同上

//master不与Executor交互，所以需要worker来告诉master关于Executor的信息
val execs = executors.values.
    map(e => new ExecutorDescription(e.appId, e.execId, e.cores, e.state))
  sender ! WorkerSchedulerStateResponse(workerId, execs.toList, drivers.keys.toSeq)继续看master中的处理逻辑case WorkerSchedulerStateResponse(workerId, executors, driverIds) => {
  idToWorker.get(workerId) match {
    case Some(worker) =>
      logInfo("Worker has been re-registered: " + workerId)
      worker.state = WorkerState.ALIVE //这时可以将之前worker状态unknown修改为ALIVE，代表该worker可用

      //将接受到的Executor信息更新到相关的app，worker中
      val validExecutors = executors.filter(exec => idToApp.get(exec.appId).isDefined)
      for (exec <- validExecutors) {
        val app = idToApp.get(exec.appId).get
        val execInfo = app.addExecutor(worker, exec.cores, Some(exec.execId))
        worker.addExecutor(execInfo)
        execInfo.copyState(exec)
      }

      //将master中driver信息更新，状态置为RUNNING
      for (driverId <- driverIds) {
        drivers.find(_.id == driverId).foreach { driver =>
          driver.worker = Some(worker)
          driver.state = DriverState.RUNNING
          worker.drivers(driverId) = driver
        }
      }
    case None =>
      logWarning("Scheduler state from unknown worker: " + workerId)
  }

  if (canCompleteRecovery) { completeRecovery() }  //同上
}这一切都处理完毕之后，看master的completeRecovery，这个是在beginRecovery调用之后，在延迟worker_timeout时间之后调用，一般情况下，上面的消息来回发送处理应该都已经结束了private def completeRecovery() {
  // Ensure "only-once" recovery semantics using a short synchronization period.
  synchronized {
    if (state != RecoveryState.RECOVERING) { return }
    state = RecoveryState.COMPLETING_RECOVERY//状态置为恢复完成
  }

  // Kill off any workers and apps that didn't respond to us.
  //清理在这个worker_timeout间隔过后还未处理成功的worker和app
  workers.filter(_.state == WorkerState.UNKNOWN).foreach(removeWorker)
  apps.filter(_.state == ApplicationState.UNKNOWN).foreach(finishApplication)

  // Reschedule drivers which were not claimed by any workers
  //在一番消息通信之后，本应该在driver中更新的worker信息不见了，则重启driver或者删除
  drivers.filter(_.worker.isEmpty).foreach { d =>
    logWarning(s"Driver ${d.id} was not found after master recovery")
    if (d.desc.supervise) {
      logWarning(s"Re-launching ${d.id}")
      relaunchDriver(d)
    } else {
      removeDriver(d.id, DriverState.ERROR, None)
      logWarning(s"Did not re-launch ${d.id} because it was not supervised")
    }
  }

  state = RecoveryState.ALIVE  //这时恢复状态真正结束了
  schedule() //整个选主流程结束时候，重新调度一次
  logInfo("Recovery complete - resuming operations!")
}


版权声明：本文为博主原创文章，未经博主允许不得转载。

hadoop -- setup and configuration


Hadoop Modes
pre-install setup
Creating a user
SSH Setup
installing java


Install Hadoop
Install in Standalone Mode
lets do a test


Install in Pseudo Distributed Mode
Hadoop setup
Hadoop configuration
YARN configuration







本节配置一个基于 linux 的 hadoop 环境。
Hadoop Modes
hadoop 支持三种模式：

Local/Standalone Mode: 默认设置是Standalone 模式，作为一个java 进程运行。
Pseudo Distributed Mode: 在一个机器上模拟分布式。hdfs, YARN, MapReduce  等这些hadoop daemon 都是一个独立的 java 进程。
Fully Distributed Mode: 需要两个或多个机器作为一个集群，实现真正的分布式。


pre-install setup
Creating a user
推荐为 hadoop 建一个独立的用户, 修改目录权限
$ su
    passwd
# useradd hadoop
# passwd hadoop
    New passwd:
    Retype new passwd 
# chown -R hadoop /usr/hadoop
SSH Setup
$ ssh-keygen -t rsa 
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 
$ chmod 0600 ~/.ssh/authorized_keys 
然后在当前 shell 用ssh 链接 localhost 就无需输入密码了。
$ ssh localhost
installing java
$ java -version
如果这个指令可以正确的查看到java版本那么java已经争取安装，如果不能，请务必要先安装好java。

step1: 在这儿下载 java  (jdk-*u**-OS-x64.tar.gz).
setp2: 切换到java所在文件夹并解压。

$ cd Downloads/ 
$ tar zxf jdk-7u71-linux-x64.gz 
$ ls 
jdk1.7.0_71   jdk-7u71-linux-x64.gz 

step3: 使所有用户可以使用java, 移动java到“/usr/local”, 或者其他你希望安装的地方。

$ su 
password: 
# mv jdk1.7.0_71 /usr/local/ 

step4 
在 ~/.bashrc 里面添加以下内容：

export JAVA_HOME=/usr/local/jdk1.7.0_71 
export PATH=$PATH:$JAVA_HOME/bin 
$ source ~/.bashrc

step5 
为方便管理，把java 加入到版本管理器，ubuntu 下是 update-alternatives:

# alternatives --install /usr/bin/java java usr/local/java/bin/java 2

# alternatives --install /usr/bin/javac javac usr/local/java/bin/javac 2

# alternatives --install /usr/bin/jar jar usr/local/java/bin/jar 2

# alternatives --set java usr/local/java/bin/java

# alternatives --set javac usr/local/java/bin/javac

# alternatives --set jar usr/local/java/bin/jar

Install Hadoop
在这找到你需要的版本，下载Hadoop, 并加压. 我下载的是 hadoop-2.7.1
$ su 
password: 
# cd /usr/local 
# wget http://apache.claz.org/hadoop/common/hadoop-2.7.1/hadoop-2.4.1.tar.gz
# tar xzf hadoop-2.7.1.tar.gz 
# chmod -R 777 /usr/local/hadop-2.7.1
Install in Standalone Mode
在这个模式下没有daemons, 并且都在同一个 JVM 里运行。 
把下面的命令写入 ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop 
export PATH=$HADOOP_HOME/bin:$PATH
$ source ~/.bashrc
然后确认一下 Hadoop 是否可以正常工作：
$ hadoop version
如果安装成功会显示类似以下的结果（这是我的输出）：
Hadoop 2.7.1
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a
Compiled by jenkins on 2015-06-29T06:04Z
Compiled with protoc 2.5.0
From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
This command was run using /usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar
let’s do a test
现在，我们用 standalone 模式下的 hadoop 做一个小小 wordcount 的测验
$ cd $HADOOP_HOME
$ mkdir input
$ cp *.txt 
$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar  wordcount input ouput  #请查看你所安装按本的jar文件的名字
$ cat output/*
然后，不出意外的话，你应该就可以看到文件里单词的个数了。
Install in Pseudo Distributed Mode
Hadoop setup
把下面的命令写入 ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_LOG_DIR=$HADOOP_HOME/logs #这个是默认位置，也可以自定义你希望的位置
$ source ~/.bashrc
Hadoop configuration
修改 $HADOOP_HOME/etc/hadoop/hadoop-env.sh 中设置 java 环境变量 JAVA_HOME
#export JAVA_HOME=${JAVA_HOME}
export JAVA_HOME=/opt/jdk1.8.0_81
修改 $HADOOP_HOME/etc/hadoop/core-site.xml 文件：
<configuration>
    <property>
        <name>fs.defaultFS</name> //也可以写作 fs.default.name
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
修改 $HADOOP_HOME/etc/hadoop/core-site.xml 文件：
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>

     <property>
      <name>dfs.name.dir</name>
      <value>file:///home/hadoop/hadoopinfra/hdfs/namenode </value> //这里file后面是三个斜杠，我打成了两个，一直启动不起来NameNode.
   </property>

   <property>
      <name>dfs.data.dir</name> 
      <value>file:///home/hadoop/hadoopinfra/hdfs/datanode </value> 
   </property>
</configuration>
格式化 HDFS 文件系统
$ hdfs namenode -format
启动 NameNode 和 DataNode 的守护进程
$ start-dfs.sh
$ jps  # 查看是否正常启动
    172075 Jps
    169799 SecondaryNameNode
    34918 Nailgun
    169311 NameNode
    169483 DataNode
$ stop-dfs.sh

note: 如果你是按博客里面的内容下载的hadoop-..*.tar.gz的安装包，并且如果你的机器是64位的话，你会出现一个WARNING，如下：
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
这个是因为安装包里面的 $HADOOP_HOME/lib/native/libhadoop.so.1.0.0 这个本地的 Hadoop 库是在32位机器上编译的，可以做以下选择： 
  1.  忽略它，因为只是个WARN，并不会影响HADOOP 的功能 
  2. 担心它会造成不稳定，下载 Hadoop 的源码包 hadoop-..*-src.tar.gz, 重新编译 
  具体可以参考【链接1】【链接2】 

stop之前可以通过浏览器查看 NameNode，其默认端口是50070, DataNode 的默认端口是50030 
+ http://localhost:50070 
+ http://localhost:50030 
 
如果如果没有正常启动可以在 $HADOOP_LOG_DIR 目录下查看对应的log文件里的内容，对应调试。
YARN configuration
修改 $HADOOP_HOME/etc/hadoop/mapred-site.xml 文件：
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
修改 $HADOOP_HOME/etc/hadoop/yarn-site.xml 文件：
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
$ start-yarn.sh
$ jps  # 查看是否正常启动
    152423 JournalNode
    173170 Jps
    34918 Nailgun
    172778 ResourceManager
    172956 NodeManager
$ stop-yarn.sh
ResourceManager默认的浏览器端口是8088，stop之前可以在；浏览器查看： 
+ http://localhost:8088 
 
或者可以用脚本 start/stop-all.sh 进行管理
$ start-all.sh
$ stop-all.sh

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

hadoop -- introduction


Hadoop Architecture
HDFS
MapReduce
Work Stages






Hadoop Architecture
用下图描述 hadoop 的四元素 
 
 


hadoop commom: 其他hadoop modules 所需要的 java 库和工具；
hadoop YARN: 用与做 job scheduling 和 cluster resource management 的 框架；
Hadoop Distributed File System (HDFSTM): 一个基于谷歌文件系统（GFS）的分布式文件系统，可以提供对应用数据高吞吐量的访问。
Hadoop MapReduce: 一个用于并行处理大数据集的系统， YARN-based。


HDFS
hadoop 上最常用的文件系统。
HDFS 用的是 master/slave architecture，包括一个master (NameNode) 管理文件系统的大量数据，和数个slaves(DataNodes)用于真正的存储数据.
HDFS里的一个文件被分成若干个块（Blocks）分别存储在若干个DataNodes里。
NameNode 

管理 Blocks 和DataNodes 的映射关系
管理 client 对文件袋额访问权限。
执行 renaming, closing 和 opening 对文件或文件夹的操作。

DataNode 

执行文件系统的读、写操作
以及按 NameNode 发出的指示对Blocks进行Creation, deletion 和 replication。

HDFS 提供了shell 和一系列的指令用于文件系统的交互。

MapReduce
以一种可靠地、高容错性的方式在集群上并行化的处理大规模数据。

The Map Task
The Reduce Task

MapReduce Framework 的每个 Cluster-node 上包含一个 JobTracker(master)  和一个 TaskTracker(slave):

JobTracker: 负责 
资源管理，resource management，tracking resource consumption/availability  
分配并监督slaves 的任务，scheduling the jobs component tasks on the slaves, monitoring them
重新执行失败的任务，re-executing the failed tasks
TaskTracker:  负责 
执行任务，execute the tasks as directed by the master 
定时向master汇报状态，provide task-status information to the master periodically


Work Stages

Stage 1: 一个应用程序向 Hadoop 提交一项工作(job)的时候需要指定一下几项： 
分布式文件系统中输入输出文件的 location;
包含 Map 和 Reduce 函数的执行语句的 jar file;
此项工作需要单独配置的参数 job  configuration.
Stage 2: 应用程序向 Hadoop 提交工作和配置给 JobTracker。JobTracker 把配置参数发布给slaves, 然后分配任务(task)、监督Slave的工作，并且向应用程序提供运行状态和诊断信息。
Stage 3: 每一次 MapReduce 操作，分布在不同节点上的 TaskTracker 执行一次任务，并且把Reduce 操作的输出保存在文件系统。


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

spark core源码分析4 worker启动流程
源码位置：org.apache.spark.deploy.worker.Worker.scala


首先查看worker的main方法，与master类似，创建sparkConf，参数解析，以及构造worker对象并创建ActorRef用于对外或者本身的信息交互。这里masters参数可以设置多个

def main(argStrings: Array[String]) {
  SignalLogger.register(log)
  val conf = new SparkConf
  val args = new WorkerArguments(argStrings, conf)
  val (actorSystem, _) = startSystemAndActor(args.host, args.port, args.webUiPort, args.cores,
    args.memory, args.masters, args.workDir)
  actorSystem.awaitTermination()
}

程序起来后，同样是先执行akka 的preStart方法
override def preStart() {
  assert(!registered)
  logInfo("Starting Spark worker %s:%d with %d cores, %s RAM".format(
    host, port, cores, Utils.megabytesToString(memory)))
  logInfo(s"Running Spark version ${org.apache.spark.SPARK_VERSION}")
  logInfo("Spark home: " + sparkHome)
  createWorkDir()//创建worker内部工作目录
  //订阅akka生命周期事件
  context.system.eventStream.subscribe(self, classOf[RemotingLifecycleEvent])
  //是否额外的启动一个shuffle服务，确保被executor所读写的shuffle文件在executor退出后被保存，可配
  shuffleService.startIfEnabled()
  webUi = new WorkerWebUI(this, workDir, webUiPort)
  webUi.bind()
  registerWithMaster()//最重要的动作了，见下面

  metricsSystem.registerSource(workerSource)
  metricsSystem.start()
  // Attach the worker metrics servlet handler to the web ui after the metrics system is started.
  metricsSystem.getServletHandlers.foreach(webUi.attachHandler)
}向Master注册自己
private def registerWithMaster() {
  // DisassociatedEvent may be triggered multiple times, so don't attempt registration
  // if there are outstanding registration attempts scheduled.
  registrationRetryTimer match {
    case None =>
      registered = false
      //这里向所有的master actorRef发送RegisterWorker消息，上几节有讲master收到该消息后，如果成功处理会反馈RegisteredWorker消息，不成功会发送RegisterWorkerFailed消息
      tryRegisterAllMasters()
      connectionAttemptCount = 0
      //这里在一定时间之后会进入ReregisterWithMaster，里面会判断是否已注册，如果没有会再次发送注册信息。这个是否注册的状态是由master反馈回来的
      registrationRetryTimer = Some {
        context.system.scheduler.schedule(INITIAL_REGISTRATION_RETRY_INTERVAL,
          INITIAL_REGISTRATION_RETRY_INTERVAL, self, ReregisterWithMaster)
      }
    case Some(_) =>
      logInfo("Not spawning another attempt to register with the master, since there is an" +
        " attempt scheduled already.")
  }
}看worker收到master的RegisteredWorker消息会怎么做？这里要说一点，worker要注册时并不知道哪台是主，哪台是备，所以向所有配置的master都发送注册信息。主备都收到worker的注册信息之后，只有主才会反馈，并带上自己的masterUrl信息，worker以此来认定主master的actorRef用于真正的信息交互
worker要通过心跳来保持与master的时刻连通，所以注册成功之后，有一个connected标记是否连接正常，在changeMaster方法内部设置connected ＝ true
<pre name="code" class="java">case RegisteredWorker(masterUrl, masterWebUiUrl) =>
  logInfo("Successfully registered with master " + masterUrl)
  registered = true //状态设置为已注册，不然的话，一定时间过后，会发起ReregisterWithMaster而重复注册
  changeMaster(masterUrl, masterWebUiUrl)//这里是将主master的信息保存

  //在注册成功之后，才开启定时器向master发送心跳
  context.system.scheduler.schedule(0 millis, HEARTBEAT_MILLIS millis, self, SendHeartbeat)
  //定时器清理workDir下很久都没有更新的且app也不在执行状态的目录
  if (CLEANUP_ENABLED) {
    logInfo(s"Worker cleanup enabled; old application directories will be deleted in: $workDir")
    context.system.scheduler.schedule(CLEANUP_INTERVAL_MILLIS millis,
      CLEANUP_INTERVAL_MILLIS millis, self, WorkDirCleanup)
  }如果收到RegisterWorkerFailed消息，则退出


下面看master接受到worker的心跳之后如何处理

由于worker注册时，master已经将workerId存入idToWorker中，所以这里走Some分支。很简单，只是更新该worker的一个时间戳。这里有必要说明一下None分支，在注册消息到达后，在master 的idToWorker和workers中都会保存，但是当master检测到worker超时时，将worker从idToWorker中删除，这样新的任务就选不了该worker了，但不删除workers中的。workers中的只会在间隔很长一段时间之后仍然没有心跳上来，才说明该worker真正无法再工作了，再从workers中删除。这里的None分支就是应对超时过后，心跳又继续上来了，就向worker发送重新注册的消息ReconnectWorker
case Heartbeat(workerId) => {
  idToWorker.get(workerId) match {
    case Some(workerInfo) =>
      workerInfo.lastHeartbeat = System.currentTimeMillis()
    case None =>
      if (workers.map(_.id).contains(workerId)) {
        logWarning(s"Got heartbeat from unregistered worker $workerId." +
          " Asking it to re-register.")
        sender ! ReconnectWorker(masterUrl)
      } else {
        logWarning(s"Got heartbeat from unregistered worker $workerId." +
          " This worker was never registered, so ignoring the heartbeat.")
      }
  }
}至此，worker启动流程以及主动发送的消息介绍完了，剩下的都是被动接收并处理的流程，在之后结合具体job介绍。。。













版权声明：本文为博主原创文章，未经博主允许不得转载。

Hyper 源码分析------Hyper client创建与执行
一、Hyper总体架构
Hyper是典型的C/S架构.首先需要启动守护进程hyperd,用于接收来自client的请求,从而进行真正的hyper虚拟机创建工作以及对其的一系列操作.而作为client端的hyper命令则更像是一个命令解析器.它的作用仅仅只是对用户的hyper指令进行解析和封装,然后将其通过HTTP请求的形式传递给hyperd进程.下面就通过一条具体的hyper虚拟机启动命令:hyper run ubuntu:14.04 /bin/bash 对hyper client端的运作流程进行分析.


二、Hyper client创建
当用户输入如上所示的一条命令时,程序首先会进入./hyper.go文件中的main函数开始执行.第一步:通过cli := client.NewHyperClient(proto, addr, nil) 创建一个HyperClient的实例.其中两个参数proto和addr分别代表了hyper client和hyper server之间通信所采用的协议以及通信地址,默认的是使用unix socket domain,即proto = "unix",addr="/var/run/hyper.sock".第三个参数代表是否使用https安全传输,默认为nil,即只使用http传输协议.这里需要注意的是,用户每次输入hyper命令都会重新创建一个hyper client实例,每个实例的生命周期只有接受用户命令,解析,发送给hyper daemon并接受hyper daemon的反馈这么长而已.
在NewHyperClient函数中主要操作就是通过net.DialTimeout(proto, addr, timeout)函数与hyperd建立连接,然后再填充HyperClient这个数据结构.由于HyperClient主要的字段就是proto,addr以及传输协议scheme,并且在接下来的用处不是很大,因此这里就不再详细展开了.
 
三、flag参数解析
接着我们再次回到./hyper.go的main函数中.当用户输入hyper命令时,其实会输入两类参数,一类为hyper实际发送给hyper daemon的参数,例如上文中的run ubuntu:14.04等等.另一类则为flag参数,Go语言还专门提供了一个flag包用于解析此类参数.
对于flag参数,我们需要事先进行定义,比如:flHelp := flag.Bool("help", false, "Help Message") 我们就定义了一个名叫flHelp的flag参数,且类型为bool,默认值为false.当接下调用flag.Parse()时,若输入的命令为:hyper help 时,flHelp就被解析为true,并且会转入执行输出帮助文档.在hyper中其实只定义了两个flag参数,一个为flHelp,另一个为flVersion,分别用于输出帮助文档和版本信息.因为我们输入的是创建虚拟机的命令,以上条件都不满足,最后则会进入cli.Cmd(flag.Args()...)执行.其中flag.Args()返回的是一个字符串数组,就是那些非flag类型的参数.后面的省略号则是Go语言中代表不定参数的一种方法。
其实上述内容的核心意思就是过滤出flag参数，若命令中有version或者help字段就输出版本或帮助信息，然后将剩余的参数传输到hyper daemon去执行，仅此而已。
 
四、命令分发
在过滤完flag参数之后，接下来要做的就是需要将命令分发到具体的函数，来发送不同的http请求了，这个任务就是由前文所述的cli.Cmd(flag.Args()...)来完成的。在该函数中我们首先执行如下代码：
method, exists := cli.getMethod(args[:2]...)//Go能够返回两个参数
if exists {
return method(args[2:]...)
}
其中args[:2]代表的是截取args数组，准确的说是切片，的前两个元素传递给getMethod方法。并且返回两个参数：method和exists。其实method是一个函数类型的返回值，exists则是一个bool类型的变量，若exists为true则表示该命令合法，并执行相应的方法。
然后我们进入getMethod()函数观察它是如何由命令行参数路由到具体的方法的。首先，getMethod将传入其中的字符串参数都规范为首字母大写，其余字母小写的形式。然后通过语句methodName := "HyperCmd" + strings.Join(camelArgs, "")形成需要调用的方法的名称。事实上，hyper client执行具体命令的函数的命名方式都是统一的，都是HyperCmd+具体命令的形式。例如，对于hyper run命令则对应的执行函数的名称为HyperCmdRun。在得到了对应的函数名称之后，我们就能找到具体的函数了，主要是通过下面这条语句：
method := reflect.ValueOf(cli).MethodByName(methodName)
这里需要说明的是，此处使用的是GO语言的反射机制，reflect.ValueOf(cli).MethodByName得到的并不是一个函数指针，而是一个reflect.value的类型。然后还需要通过它，反射出一个接口，再通过一个断言，才能找到具体的函数，具体操作如下：
method.Interface().(func(...string) error)//这里使用了Go的高级语法，可能比较难理解
那么现在我们可以知道，如果我们执行hyper run命令时，上述代码片段中的method就是一个指向HyperCmdRun的函数指针。下面我们就对这个函数进行分析。
 
五、HyperCmdRun 函数分析
其实HyperCmdRun主要用于在虚拟机中启动Pod以及container的配置。在HyperCmdRun函数中首先定义了一个叫opts的变量：
var opts struct {
PodFile       string   `short:"p" long:"podfile" value-name:"\"\"" description:"Create and Run a pod  based on the pod file"`
….....
RestartPolicy string   `long:"restart" default:"never" value-name:"\"\"" default-mask:"-"  description:"Restart policy to apply when a container exits (never, onFailure, always)"`
}
其中包含的内容主要是输入hyper run命令时的一些可选选项，例如cpu数目，内核大小等等。接下来则会调用parser = gflag.NewParser(&opts, gflag.Default|gflag.IgnoreUnknown)函数将输入的参数进行解析到opts变量中。然后，会判断opts的PodFile和K8s字段是否为空，若不为空则从相应的pod file（pod的配置文件）进行启动。和之前的flag参数类似，剩下的非可选参数则通过args, err := parser.Parse()导入args变量中。
因为我们并没有Pod file，所以自然需要手动进行配置了。首先定义几个变量：
var (
image   = args[1]
command = []string{}
env     = []pod.UserEnvironmentVar{}
)
其中image指的是启动的docker镜像的名称，对于我们的命令，显然image就是ubuntu:14.04。
command则用于保存在虚拟机的容器中执行的命令，同于对于我们的命令，command就应该是/bin/bash。env则保存pod启动的环境变量，其实也就是上文opts中的那些变量。接下来，根据上文中获取的各类信息对UserContainer和UserPod两个数据结构进行配置，其中包含了要启动的pod和container的信息，然后将其转换成json格式，调用：
podId, err := cli.RunPod(string(jsonString))将相应的命令转换为http请求传送给hyper daemon。
 
六、RunPod函数分析
在RunPod函数中首先定义一个变量v := url.Values{}，其中url.Values是一个map[string][]string类型，即通过一个string类型的键值能够获取一个字符串的数组。接下来通过v.Set("podArgs", podstring)将之前解析出来的pod配置和”podArgs”合成一个键值对。然后在hyper daemon中就能通过v[“podArgs”]解析出相应的命令行参数。最后，通过函数body, _, err := readBody(cli.call("POST", "/pod/run?"+v.Encode(), nil, nil))将url.Values进行转码，打包生成一个http请求发往hyper daemon。由此hyper client的任务就基本完成了。
 
七、总结
其实总的来说，hyper client的任务非常简单，就只是解析用户输入的hyper命令，然后根据不同的命令，进行相应的配置，最后将需要执行的操作打包发送到hyper daemon中即可。

版权声明：本文为博主原创文章，未经博主允许不得转载。

Reduce侧联接
案例分析前提，了解其原理，以及术语
术语部分：
 1.Data Source：基本与关系数据库中的表相似，形式为：（例子中为CSV格式）
2.Tag：由于记录类型（Customers或Orders）与记录本身分离，标记一个Record会确保特殊元数据会一致存在于记录中。在这个目的下，我们将使用每个record自身的Data source名称标记每个record。
3.Group Key：Group Key类似于关系数据库中的链接键（join key），在我们的例子中，group key就是Customer ID（第一列的3）。由于datajoin包允许用户自定义group key，所以其较之关系数据库中的join key更一般、平常。
原理部分： 
 原理：
1、mapper端输入后，将数据封装成TaggedMapOutput类型，此类型封装数据源(tag)和值(value)；
2、map阶段输出的结果不在是简单的一条数据，而是一条记录。记录=数据源(tag)+数据值(value).
3、combine接收的是一个组合：不同数据源却有相同组键的值；
4、不同数据源的每一条记录只能在一个combine中出现； 
如图： 
 


1.利用datajoin包来实现join：
---------------------
Hadoop的datajoin包中有三个需要我们继承的类：DataJoinMapperBase，DataJoinReducerBase，TaggedMapOutput。正如其名字一样，我们的MapClass将会扩展DataJoinMapperBase，Reduce类会扩展DataJoinReducerBase。这个datajoin包已经实现了map()和reduce()方法，因此我们的子类只需要实现一些新方法来设置一些细节。

　　

　　在用DataJoinMapperBase和DataJoinReducerBase之前，我们需要弄清楚我们贯穿整个程序使用的新的虚数据类TaggedMapOutput。

　　

　　根据之前我们在图Advance MapReduce的数据流中所展示的那样，mapper输出一个包（由一个key和一个value(tagged record)组成）。datajoin包将key设置为Text类型，将value设置为TaggedMapOutput类型（TaggedMapOutput是一个将我们的记录使用一个Text类型的tag包装起来的数据类型）。它实现了getTag()和setTag(Text tag)方法。它还定义了一个getData()方法，我们的子类将实现这个方法来处理record记录。我们并没有明确地要求子类实现setData()方法，但我们最好还是实现这个方法以实现程序的对称性（或者在构造函数中实现）。作为Mapper的输出，TaggedMapOutput需要是Writable类型，因此的子类还需要实现readFields()和write()方法。


DataJoinMapperBase：
-------------------

　　回忆join数据流图，mapper的主要功能就是打包一个record使其能够和其他拥有相同group key的记录去向一个Reducer。DataJoinMapperBase完成所有的打包工作，这个类定义了三个虚类让我们的子类实现：

　　protected abstract Text generateInputTag(String inputFile);

　　protected abstract TaggedMapOutput generateTaggedMapOutut(Object value);

　　protected abstract Text generateGroupKey(TaggedMapOutput aRecored);

　　

　　在一个map任务开始之前为所有这个map任务会处理的记录定义一个tag(Text)，结果将保存到DataJoinMapperBase的inputTag变量中，我们也可以保存filename至inputFile变量中以待后用。


　　在map任务初始化之后，DataJoinMapperBase的map()方法会对每一个记录执行。它调用了两个我们还没有实现的虚方法：generateTaggedMapOutput()以及generateGroupKey(aRecord);（详见代码）

DataJoinReducerBase：
--------------------

DataJoinMapperBase将我们所需要做的工作以一个full outer join的方式简化。我们的Reducer子类只需要实现combine()方法来滤除掉我们不需要的组合来得到我们需要的（inner join, left outer join等）。同时我们也在combiner()中将我们的组合格式化为输出格式。

/hadoop-2.6.0/share/hadoop/tools/lib   程序需要自己手动导入Jar包
MapperClass.java
package com.yc.zzg.test;

import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;
import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MapperClass extends DataJoinMapperBase{

    @Override
    protected Text generateGroupKey(TaggedMapOutput arg0) {
        String line = ((Text)arg0.getData()).toString();
        String[] tokens = line.split(",");
        String groupKey = tokens[0];
        return new Text(groupKey);
    }

    @Override
    protected Text generateInputTag(String arg0) {

        return new Text(arg0);
    }

    @Override
    protected TaggedMapOutput generateTaggedMapOutput(Object arg0) {
        TaggedWritable tw = new TaggedWritable((Text)arg0);
        tw.setTag(this.inputTag);
        return tw;
    }

}

TaggedWritable.java
package com.yc.zzg.test;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.ReflectionUtils;

public class TaggedWritable extends TaggedMapOutput {

    private Writable data;

    public TaggedWritable() {  
    }  

    public TaggedWritable(Writable data) {  
         this.tag = new Text("");  
         this.data = data;  
     } 

    @Override
    public Writable getData() {
        return data;
    }



    public void setData(Writable data) {
        this.data = data;
    }

    @Override
    public void readFields(DataInput in) throws IOException {
          this.tag.readFields(in);    
          //加入此部分代码，否则，可能报空指针异常  
          String temp=in.readUTF();  
          if (this.data == null|| !this.data.getClass().getName().equals(temp)) {  
              try {  
                  this.data = (Writable) ReflectionUtils.newInstance(  
                          Class.forName(temp), null);  
              } catch (ClassNotFoundException e) {  
                  e.printStackTrace();  
              }  
          }  
          this.data.readFields(in);    
    }

    @Override
    public void write(DataOutput out) throws IOException {
        this.tag.write(out);    
        //此行代码很重要  
        out.writeUTF(this.data.getClass().getName());  

        this.data.write(out);   
    }

}

Reduce.java
package com.yc.zzg.test;

import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;
import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
import org.apache.hadoop.io.Text;

public class Reduce extends DataJoinReducerBase {

    @Override
    protected TaggedMapOutput combine(Object[] tags, Object[] values) {
         if(tags.length<2)return null;
          StringBuffer joinData = new StringBuffer();
          int count=0;

            for(Object value: values){
                joinData.append(",");
                TaggedWritable tw = (TaggedWritable)value;
                String recordLine = ((Text)tw.getData()).toString();
                String[] tokens = recordLine.split(",",2);
                if(count==0) joinData.append(tokens[0]);
                joinData.append(tokens[1]);
            }

            TaggedWritable rtv = new TaggedWritable(new Text(new String(joinData)));
            rtv.setTag((Text)tags[0]);
            return rtv;
    }
}

Drive.java
package com.yc.zzg.test;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Drive {

    public static void main(String[] args) throws Exception {
         Configuration conf = new Configuration();   
            JobConf job = new JobConf(conf, Drive.class);  

            Path in = new Path("hdfs://localhost:9000/input/inputtest/*");
            Path out = new Path("hdfs://localhost:9000/output/test20");
            FileInputFormat.setInputPaths(job, in);  
            FileOutputFormat.setOutputPath(job, out);  
            job.setJobName("DataJoin");  
            job.setMapperClass(MapperClass.class);  
            job.setReducerClass(Reduce.class);  

            job.setInputFormat(TextInputFormat.class);  
            job.setOutputFormat(TextOutputFormat.class);  
            job.setOutputKeyClass(Text.class);  
            job.setOutputValueClass(TaggedWritable.class);  
            job.set("mapred.textoutputformat.separator", ",");  
            JobClient.runJob(job); 
    }
}

Customers.txt
1,Stephanie Leung,555-555-5555　　　　　　
2,Edward Kim,123-456-7890　　　　　　　　 
3,Jose Madriz,281-330-8004　　　　　　　　 
4,David Stork,408-555-0000　　　　　　　　  
Orders.txt
　3,A,12.95,02-Jun-2008
　1,B,88.25,20-May-2008
　2,C,32.00,30-Nov-2007
　3,D,25.02,22-Jan-2009
所碰到问题有几点，提出来和大家分析一下 
1。第一个问题是DataJoinMapperBase包的问题，前面已经解决了 
2。第二个问题是原来的程序会报一个
java.lang.Exception: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.yc.zzg.test.TaggedWritable.<init>()


Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.yc.zzg.test.TaggedWritable.<init>()
所以你需要给一个构造方法
 public TaggedWritable() {  
    }  
3。第三个问题是我有两个多个文件怎么导入，你将多个文件放入同一个文件夹里然后用
   Path in = new Path("hdfs://localhost:9000/input/inputtest/*");
就可以导入多个文件啦，同理也可以拼file*.txt之类的
4。有的时候我为了测试一个工程，从test1测试到了test20，为了方便我们输出的时候总是要创建一个新的目录，解决方案如下
1。hadoop需要把集群上的core-site.xml和hdfs-site.xml放到当前工程下。eclipse工作目录的bin文件夹下面

2。    FileSystem fs=FileSystem.get(conf); 
            if(fs.exists(out)){  
                fs.delete(out, true);  
                System.out.println("输出路径存在，已删除！");  
            }  

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

