SQLite和MySQL数据库的区别与应用
简单来说，SQLITE功能简约，小型化，追求最大磁盘效率；MYSQL功能全面，综合化，追求最大并发效率。如果只是单机上用的，数据量不是很大，需要方便移植或者需要频繁读/写磁盘文件的话，就用SQLite比较合适；如果是要满足多用户同时访问，或者是网站访问量比较大是使用MYSQL比较合适。


下面详细介绍两者的区别和应用：



SQLite

SQLite是非凡的数据库，他可以进程在使用它的应用中。作为一个自包含、基于文件的数据库，SQLite提供了出色的工具集，可以处理所有类型的数据，没有什么限制，而且比起服务器运行的进程型服务器使用起来轻松许多。

一个应用使用SQLite时，它的功能直接被集成在其中，应用会直接访问包含数据的文件(即SQLite数据库),而不是通过一些端口(port, socket)来交互。感谢这种底层技术，这使SQLite变得非常快速和高效，并且十分强大。

SQLite支持的数据类型

NULL:

NULL值。

INTEGER:

有符号整数，按照设置用1、2、3、4、6或8字节存储。

REAL:

浮点数，使用8字节IEEE浮点数方式存储。

TEXT:

文本字符串，使用数据库编码存储(UTF-8, UTF-16BE 或 UTF-16LE)。

BLOB:

二进制大对象，怎么输入就怎么存储。

注: 想了解更多有关SQLite数据类型的信息，可以查看这一主题的 官方文档 。

SQLite 的优点

基于文件:

整个数据库都包含在磁盘上的一个文件中，因此它有很好的迁移性。

标准化:

尽管它看起来像个“简化版”的数据库，SQLite 确实支持 SQL。它略去了一些功能(RIGHT OUTER JOIN 和 FOR EACH STATEMENT)，但是，又同时增加了一些其他功能。

对开发乃至测试都很棒:

在绝大多数应用的开发阶段中，大部分人都非常需要解决方案能有并发的灵活性。SQLite 含有丰富功能基础，所能提供的超乎开发所需，并且简洁到只需一个文件和一个 C 链接库。

SQLite的缺点

没有用户管理:

高级数据库都能支持用户系统，例如，能管理数据库连接对数据库和表的访问权限。但由于 SQLite 产生的目的和本身性质(没有多用户并发的高层设计)，它没有这个功能。

缺乏额外优化性能的灵活性：

仍然是从设计之初，SQLite 就不支持使用各种技巧来进行额外的性能优化。这个库容易配置，容易使用。既然它并不复杂，理论上就无法让它比现在更快，其实现在它已经很快了。

何时使用 SQLite ?

嵌入式应用:

所有需要迁移性，不需要扩展的应用，例如，单用户的本地应用，移动应用和游戏。

代替磁盘访问:

在很多情况下，需要频繁直接读/写磁盘文件的应用，都很适合转为使用 SQLite ，可以得益于 SQLite 使用 SQL 带来的功能性和简洁性。

测试:

它能秒杀大部分专门针对应用业务逻辑(也就是应用的主要目的：能完成功能)的测试。

何时不用 SQLite ?

多用户应用:

如果你在开发的应用需要被多用户访问，而且这些用户都用同一个数据库，那么相比 SQLite 最好还是选择一个功能完整的关系型数据库(例如 MySQL)。

需要大面积写入数据的应用:

SQLite 的缺陷之一是它的写入操作。这个数据库同一时间只允许一个写操作，因此吞吐量有限。




MySQL

MySQL 在所有大型数据库服务器中最流行的一个. 它的特性丰富，产品的开源性质使得其驱动了线上大量的网站和应用程序. 要入手 MySQL 相对简单，开发人员可以在互联网上面访问到大量有关这个数据库的信息.

注意: 由于这个产品的普及性，大量的第三方应用、工具和集成库对于操作这个RDBCMS的方方面面大有帮助.

Mysql没有尝试去实现SQL标准的全部，而是为用户提供了很多有用的功能. 作为一个独立的数据库服务器，应用程序同Mysql守护进程的交互，告诉它去访问数据库自身 -- 这一点不像 SQLite.

MySQL支持的数据类型

TINYINT:

一个非常小的整数.

SMALLINT:

一个小整数.

MEDIUMINT:

一个中间大小的整数.

INT or INTEGER:

一个正常大小的整数.

BIGINT:

一个大的整数.

FLOAT:

一个小的 (单精度) 浮点数，不能是无符号的那种.

DOUBLE, DOUBLE PRECISION, REAL:

一个正常大小 (双精度) 的浮点数，不能使无符号的那种.

DECIMAL, NUMERIC:

没有被包装的浮点数。不能使无符号的那种.

DATE:

一个日期.

DATETIME:

一个日期和时间的组合.

TIMESTAMP:

一个时间戳.

TIME:

一个时间.

YEAR:

一个用两位或者4位数字格式表示的年份(默认是4位).

CHAR:

一个固定长度的字符串，存储时总是在其固定长度的空间里右对齐.

VARCHAR:

一个可变长度的字符串.

TINYBLOB, TINYTEXT:

一个BLOB或者TEXT列，最大长度255 (2^8 - 1)个字符.

BLOB, TEXT:

一个BLOB或者TEXT列，最大长度 65535 (2^16 - 1)个字符.

MEDIUMBLOB, MEDIUMTEXT:

一个BLOB或者TEXT列，最大长度 16777215 (2^24 - 1)个字符.

LONGBLOB, LONGTEXT:

一个BLOB或者TEXT列，最大长度4294967295 (2^32 - 1) 个字符.

ENUM:

一个枚举类型.

SET:

一个集合.

MySQL的优点

容易使用:

安装MySQL非常容易。第三方库，包括可视化(也就是有GUI)的库让上手使用数据库非常简单。

功能丰富:

MySQL 支持大部分关系型数据库应该有的 SQL 功能——有些直接支持，有些间接支持。

安全:

MYSQL 有很多安全特性，其中有些相当高级。

灵活而强大:

MySQL 能处理很多数据，此外如有需要，它还能“适应”各种规模的数据。

快速:

放弃支持某些标准，让 MySQL 效率更高并能使用捷径，因此带来速度的提升。

MySQL的缺点

已知的局限:

从设计之初，MySQL 就没打算做到全知全能，因此它有一些功能局限，无法满足某些顶尖水平应用的需求。

可靠性问题:

MySQL 对于某些功能的实现方式(例如，引用，事务，数据审核等) 使得它比其他一些关系型数据库略少了一些可靠性。

开发停滞:

尽管 MySQL 理论上仍是开源产品，也有人抱怨它诞生之后更新缓慢。然而，应该注意到有一些基于 MySQL 并完整集成的数据库(如 MariaDB)，在标准的 MySQL 基础上带来了额外价值。

何时使用 MySQL?

分布式操作:

当SQLite所提供的不能满足你的需要时，可以把MySQL包括进你的部署栈，就像任何一个独立的数据库服务器，它会带来大量的操作自由性和一些先进的功能。

高安全性:

MySQL的安全功能，用一种简单的方式为数据访问(和使用)提供了可靠的保护。

Web网站 和 Web应用:

绝大多数的网站(和Web应用程序)可以忽视约束性地简单工作在MySQL上。这种灵活的和可扩展的工具是易于使用和易于管理的——这被证明非常有助于长期运行。

定制解决方案:

如果你工作在一个高度量身定制的解决方案上，MySQL能够很容易地尾随和执行你的规则，这要感谢其丰富的配置设置和操作模式。

何时不用 MySQL?

SQL 服从性:

因为 MySQL 没有[想要]实现 SQL 的全部标准，所以这个工具不完全符合SQL。如果你需要对这样的关系数据库管理系统进行整合，从MySQL进行切换是不容易的。

并发:

即使MySQL和一些存储引擎能够真地很好执行读取操作，但并发读写还是有问题的。

缺乏特色:

再次提及，根据数据库引擎的选择标准，MySQL会缺乏一定的特性，如全文搜索。



版权声明：本文为博主原创文章，未经博主允许不得转载。

Hadoop之Reduce侧的联结
理解其就像关系型数据库中的链接查询一样,数据很多的时候,几个数据文件的数据能够彼此有联系,可以使用Reduce联结。举个很简单的例子来说,一个只存放了顾客信息Customer.txt文件,和一个顾客相关联的Order.txt文件,要进行两个文件的信息组合,原理图如下: 
 
 
这里涉及的几个专业术语:Group key ，datasourde,Tag.前者的话通俗点来说的话就相当于关系型数据库中的主键和外键,通过其id进行的联结依据。datasource，顾名思义,就是数据的来源,那么这里指的就是Custonmers和Orders,Tag的话也比较好理解,就是里面的字段到底是属于哪个文件的。 
操作Reduce的侧联结,要用到hadoop-datajoin-2.6.0.jar包,默认路径: 
E:\hadoop-2.6.0\share\hadoop\tools\lib(hadoop的工作目录)。 
用到的3个类： 
1、DataJoinMapperBase 
2、DataJoinReducerBase 
3、TaggedMapOutput 
比较正式的工作原理: 
 1、mapper端输入后，将数据封装成TaggedMapOutput类型，此类型封装数据源(tag)和值(value)； 
2、map阶段输出的结果不在是简单的一条数据，而是一条记录。记录=数据源(tag)+数据值(value). 
3、combine接收的是一个组合：不同数据源却有相同组键的值； 
4、不同数据源的每一条记录只能在一个combine中出现； 
好,了解了这些我们就进行编码阶段: 
这里的话将几个类写在一起测试，感觉另有一番感觉： 
联结之前的Custmoner.txt文件: 

联结之前的Order.txt文件: 
 
测试代码:
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;
import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;
import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.chain.ChainMapper;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class DataJoin extends Configuration{
    //DataJoinMapperBase默认没导入,路径E:\hadoop-2.6.0\share\hadoop\tools\lib
    public static class MapClass extends DataJoinMapperBase{
        // 设置组键
        @Override
        protected Text generateGroupKey(TaggedMapOutput aRecord) {
            String line=((Text)aRecord.getData()).toString();
            String [] tokens=line.split(",");
            String groupkey=tokens[0];
            return new Text(groupkey);
        }
        /* 
         * 这个在任务开始时调用，用于产生标签
               此处就直接以文件名作为标签
         */
        @Override
        protected Text generateInputTag(String inputFile) {
            return new Text(inputFile);
        }
        // 返回一个任何带任何我们想要的Text标签的TaggedWritable
        @Override
        protected TaggedMapOutput generateTaggedMapOutput(Object value) {
            TaggedWritable retv=new TaggedWritable((Text) value);
            retv.setTag(this.inputTag);
            return retv;
        }

    }

    public static class Reduce extends DataJoinReducerBase{
        // 两个参数数组大小一定相同，并且最多等于数据源个数
        @Override
        protected TaggedMapOutput combine(Object[] tags, Object[] values) {
            if(tags.length<2){
                return null;// 这一步，实现内联结
            }
            String joinedStr="";
            for(int i=0;i<values.length;i++){
                if(i>0){
                    joinedStr+=",";// 以逗号作为原两个数据源记录链接的分割符
                    TaggedWritable tw=(TaggedWritable)values[i];
                    String line=((Text)tw.getData()).toString();
                    String[] tokens=line.split(",",2);// 将一条记录划分两组，去掉第一组的组键名。
                    joinedStr+=tokens[1];
                }
            }
            TaggedWritable retv=new TaggedWritable(new Text(joinedStr));
            retv.setTag((Text)tags[0]);
            return retv;// 这只retv的组键，作为最终输出键。
        }
    }

    /*TaggedMapOutput是一个抽象数据类型，封装了标签与记录内容
     此处作为DataJoinMapperBase的输出值类型，需要实现Writable接口，所以要实现两个序列化方法
     自定义输入类型*/
    public static class TaggedWritable extends TaggedMapOutput{
        private Writable data;
        //如果不给其一个默认的构造方法,Hadoop的使用反射来创建这个对象，需要一个默认的构造函数（无参数）
        public TaggedWritable(){
        }
        public TaggedWritable(Writable data){
            //TODO 这里可以通过setTag()方法进行设置
            this.tag=new Text("");
            this.data=data;
        }

        @Override
        public void readFields(DataInput in) throws IOException {
            this.tag.readFields(in);
            //加入以下的代码.避免出现空指针异常,当时一定要在其写的时候加入out.writeUTF(this.data.getClass().getName());
            //不然会出现readFully错误
            String temp=in.readUTF();
            if(this.data==null||!this.data.getClass().getName().equals(temp)){
                try {
                    this.data=(Writable)ReflectionUtils.newInstance(Class.forName(temp), null);
                } catch (ClassNotFoundException e) {
                    e.printStackTrace();
                }
            }
            this.data.readFields(in);
        }

        @Override
        public void write(DataOutput out) throws IOException {
            this.tag.write(out);
            out.writeUTF(this.data.getClass().getName());
            this.data.write(out);
        }

        @Override
        public Writable getData() {
            return data;
        }
    }


    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration(); //组件配置是由Hadoop的Configuration的一个实例实现
        JobConf job = new JobConf(conf, DataJoin.class); 
        Path in=new Path("hdfs://master:9000/user/input/yfl/*.txt");
        Path out=new Path("hdfs://master:9000/user/output/testfeng1");
        FileSystem fs=FileSystem.get(conf);
        //通过其命令来删除输出目录
        if(fs.exists(out)){
            fs.delete(out,true);
        }
        //TODO这里注意别导错包了
        FileInputFormat.setInputPaths(job, in);
        FileOutputFormat.setOutputPath(job, out);
        job.setJobName("DataJoin");
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);
        job.setInputFormat(TextInputFormat.class);
        job.setOutputFormat(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(TaggedWritable.class);
        job.set("mapred.textoutputformat.separator", ",");
        JobClient.runJob(job);

    }
}

运行的结果：
为了让调试更加的方便,在程序中直接使用delete命令已达到删除输出目录的功能,省去每次都要手动删除的麻烦,这里需要在我们的工程目录下面的bin目录下面添加主机的core-site.xml和hdfs-site.xml文件,然后给对于的目录赋上权限chmod -R 777 xxx，即可。 
 
hadoop很有意思,我希望自己能走的更远！！！坚持,加油！！！

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Spring Data MongoDB实战(上)
Spring Data MongoDB实战(上)
作者：chszs，版权所有，未经同意，不得转载。博主主页：http://blog.csdn.net/chszs
本文会详细展示Spring Data MongoDB是如何访问MongoDB数据库的。MongoDB是一个开源的文档型NoSQL数据库，而Spring Data MongoDB是Spring Data的模块之一，专用于访问MongoDB数据库。Spring Data MongoDB模块既提供了基于方法名的查询方式，也提供了基于注释的查询方式。
1、用Spring Data配置并管理MongoDB
要安装MongoDB数据库，可以从这里下载：https://www.mongodb.org/downloads
安装过程省略。完成MongoDB的安装和运行后，可以开始应用开发了。
首先在Eclipse创建一个简单的Maven项目，并配置pom.xml管理Spring Data MongoDB项目的依赖。内容如下：

pom.xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>SpringDataMongoDBDemo</groupId>
    <artifactId>SpringDataMongoDBDemo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>org.springframework.data</groupId>
            <artifactId>spring-data-mongodb</artifactId>
            <version>1.7.2.RELEASE</version>
        </dependency>
    </dependencies>
</project>

Eclipse会下载所需的JAR包并把依赖关系配置到项目的类路径下。现在项目的依赖关系已经完成导入，可以开始编写实际的代码了。
首先创建需要持久化到MongoDB数据库的实体类。

Person.java
package com.ch.jpa.entity;

import java.util.ArrayList;
import java.util.List;
import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.PersistenceConstructor;
import org.springframework.data.mongodb.core.mapping.DBRef;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "person")
public class Person {

    @Id
    private Long personId;

    private String name;

    private int age;

    @DBRef(db = "address")
    private List<Address> addresses = new ArrayList<>();

    public Person() {
    }

    @PersistenceConstructor
    public Person(Long personId, String name, int age) {
        super();
        this.personId = personId;
        this.name = name;
        this.age = age;
    }

    public Long getPersonId() {
        return personId;
    }

    public void setPersonId(Long personId) {
        this.personId = personId;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public List<Address> getAddresses() {
        return addresses;
    }

    public void setAddresses(List<Address> addresses) {
        this.addresses = addresses;
    }

    @Override
    public String toString() {
        return "Person [personId=" + personId + ", name=" + name + ", age=" + age + ", addresses=" + addresses + "]";
    }
}

注释@Document表示待持久化的数据是一个集合。如果集合没有指定名字，那么默认会使用实体类的类名作为集合名。
注释@Id表示被注解的域被映射到集合中的_id列。如果实体类中未使用此注释，那么默认名为id的域会被映射到集合中的_id列。而且此域的值由MongoDB的驱动包自动产生，它的值在在POJO中是不可用的。
注释@DBRef用于在当前的实体类中引用已有的实体类。然而，与关系数据库的情况不同，如果我们保存当前实体，它不会保存引用的相关实体。引用的相关实体的持久化是分开的。
注释@PersistenceConstructor用于标记从MongoDB数据库服务器取回数据时创建实体的构造方法。
下面是关联的Address实体类：
Address.java
package com.ch.jpa.entity;

import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.PersistenceConstructor;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "address")
public class Address {

    @Id
    private long addressId;

    private String address;

    private String city;

    private String state;

    private long zipcode;

    public Address() {

        System.out.println("CAlling default cons");
    }

    @PersistenceConstructor
    public Address(long addressId, String address, String city, String state, long zipcode) {
        super();
        this.addressId = addressId;
        this.address = address;
        this.city = city;
        this.state = state;
        this.zipcode = zipcode;
    }

    public String getAddress() {
        return address;
    }

    public void setAddress(String address) {
        this.address = address;
    }

    public String getCity() {
        return city;
    }

    public void setCity(String city) {
        this.city = city;
    }

    public String getState() {
        return state;
    }

    public void setState(String state) {
        this.state = state;
    }

    public long getZipcode() {
        return zipcode;
    }

    public void setZipcode(long zipcode) {
        this.zipcode = zipcode;
    }

    @Override
    public String toString() {
        return "Address [address=" + address + ", city=" + city + ", state=" + state + ", zipcode=" + zipcode + "]";
    }
}


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主chszs的原创文章，未经博主允许不得转载。

项目组数据库脚本的维护方案

背景

版本发布密集，为满足客户和领导的要求，项目经理决定每周必须发布并上线一个版本。功能修改频繁，相应的表结构、表数据的变动也比较频繁。产品满足两种部署方案，一是总部部署，由项目组直接来维护，供内部客户使用；二是客户自行安装、升级和维护，项目组提供版本和技术支持。项目选型使用MySQL作为数据存储软件。

方案

针对现状，设计出数据库脚本的维护方案，脚本分为全量脚本和升级脚本两套。
全量脚本，包括

表结构定义，包括表结构定义、列的索引定义。初始化数据，包括系统正常运行时需要的初始化数据。存储过程定义，包括系统运行时使用到的存储过程的集合。函数定义，包括系统运行时使用到的、自定义的MySQL

升级脚本，相比于全量脚本，升级脚本的构成相对简单一些，以天为单位来维护脚本，比如2015年8月10日脚本有变化，那么就创建一个脚本文件，名为2015-08-10.sql，这个文件里保存对原有数据库表对象进行增量修改的语句。

方案的问题

由于团队成员中以刚毕业、工作不满一年的新员工为主，前述方案在实际使用时遇到几个问题：

全量脚本和升级脚本中，经常会出现不一致。比如升级脚本中增加了字段，但全量脚本中没有增加；全量脚本中增加字段时出现了重复增加，导致建失败，等等。数据库脚本中存在语法错误，比如语句末尾的“;”经常忘记增加，导致同一文件中后面的脚本执行失败。数据库脚本中存在乱码，比如书写脚本时经常忘记切换输入法，导致分号、逗号等是中文符号，执行时失败。数据库脚本缺少注释，尤其是升级脚本中缺少注释，更别提场景描述，测试人员验证升级脚本时需要大量时间来确认脚本使用的场景。升级脚本的场景存在设计遗漏，测试经常在版本临上线前突然发现脚本未能覆盖全部场景，此时往往需要安排脚本的开发人员及骨干开发放下手上的工作，临时救场。。。。

针对遇到的问题，项目组在例会上时安排骨干开发人员来讲解脚本的作用及开发说明，但收效不明显。脚本中依然不断出现各式的问题，导致日常特性测试、数据库对象升级测试、性能测试过程中，测试人员和骨干开发人员花费大量时间来排查此类错误，极大的降低了团队的效率，相应的团队自身也很疲惫。

解决方法

经过分析，前述问题主要分为几类：

注释不足，比如注释不全，缺少场景说明等；脚本维护中的低级错误，比如遗漏、语法错误、非法字符等；场景设计遗漏，场景考虑不全面，导致脚本实现不合理，不满足业务需求。

解决方案如下：

注释不足。项目组周例会上宣讲脚本的注释要求，并指定专人负责检查脚本的规范符合度，不合要求的脚本直接要求提交人修改，同时作为关键事件通报批评；脚本维护中的低级错误。由于脚本比较多，代码量比较大，靠人去对比不太现实，因此利用MySQL备份工具mysqldump的能力，开发脚本对比工具来完成数据库表定义差异的对比，简化了人的操作，降低了脚本检查的工作量，人只需要查看报告即可以找到脚本中存在的问题。项目组指定专人负责阅读工具输出的报告，当发现脚本存在低级错误时，则要求提交人修改，同时通报批评；场景设计遗漏。修改当前的Story开发流程，增加专门章节，要求开发人员务必分析当前待开发特性在生产环境上线时的数据库对象升级策略；Story评审时，本章节作为必须评审的主题，如果开发人员未准备或者准备不足，测试人员有权要求Story重新评审，项目经理将此事件作为关键事件记录，迭代总结或者项目总结时将有专门议题要求开发人员作出解释。

脚本对比工具的工作流程比较简单，如下：

导出生产环境的数据库表结构定义，R.sql。本地的数据库。创建本地数据库DB_upgrade。打开DB_upgrade，导入R.sql，同时导入本周内，从周一到周六的升级脚本，如无则直接跳过。导出DB_upgrade的表结构定义，upgrade.sql。创建本地数据库DB_install。打开DB_install，导入全量数据库脚本，包括表定义、索引定义。导出DB_install的表结构定义，install.sql。使用工具对比upgrade.sql和install.sql。

工具开发过程中应用到了mysqldump、mysql、msys、jrunscript。

mysqldump，MySQL数据库自带的备份工具，通过指定选项可以只导出表定义。mysql，MySQL数据库自带的客户端软件，用于执行脚本。msys，有了它就可以在windows环境直接运行shell脚本。jrunscript，JDK1.6版本起自带的js执行器，1.8版本还可以执行其它类型的脚本。shell虽然强大，但仍然有些工作不太方便完成，此时即可编写js代码来访问JDK中的API完成那部分操作，同时不需要引入更多的jar，使用时相当方便。


欢迎访问Jackie的家，http://jackieathome.sinaapp.com/，如需转载文章，请注明出处。


版权声明：本文为博主原创文章，未经博主允许不得转载。

MongoDB(1)--简介以及安装
    前段时间接触了NoSql类型的数据库redis，当时是作为缓存服务器使用的。那么从这篇博客开始学习另一个很出名的NoSql数据库：MongoDb。不过目前还没有在开发当中使用，一步一步来吧。

简介
    MongoDB是一个开源的，基于分布式的，面向文档存储的非关系型数据库。是非关系型数据库当中功能最丰富、最像关系数据库的。
    MongoDB由C++编写，其名字来源于"humongous"这个单词，其宗旨在于处理大量数据。
    MongoDB可以运行在Windows、unix、OSX、Solaris系统上，支持32位和64位应用，提供多种编程语言的驱动程序。
    MongoDB支持的数据结构非常松散，是类似json的BSON格式，通过键值对的形式存储数据，可以存储复杂的数据类型。
    MongoDB支持的数据类型有：null、boolean、String、objectId、32位整数、64位整数、64位浮点数、日期、正则表达式、js代码、二进制数据、数组、内嵌文档、最大值、最小值、未定义类型。
    其中，内嵌文档我理解的并不是.doc.txt等文件，这里所指的文档是mongoDB的一个存储单元(相当于关系型数据当中的记录)，在mongoDB中的表现形式为{key1:value1，key2：value2}，而内嵌文档则是这样的形式{key1:value1,key2:{key2.1:value2.1,key2.2:value2.2}}。
    MongoDB最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。

windows下的安装
    安装
    下载路径：https://www.mongodb.org/downloads
    解压之后安装这没什么好说的，因为连安装路径都不用你选的。我还注意，它就安装完了。我连路径都没找着，还是上网查的。默认安装在了C:\Program Files\MongoDB下。

    启动
     创建数据库目录E:\mongodb，接下来打开命令行窗口：
       



    使用配置文件启动
    官方下载的安装包里面没有默认的配置文件，若想使用配置文件自己动手来吧，使用配置文件怎么着也比敲命令来的高级些吧？使用命令的都是大师！使用配置文件配置数据库文件、日志文件以及其它的一些配置如下：




建立数据库目录 E:\mongodb\data建立日志目录 E:\mongodb\log建立配置文件 E:\mongodb\conf建立.conf配置文件，配置文件内容如下：

dbpath=E:\mongodb\data #数据库路径
logpath=E:\mongodb\log\mongodb.log #日志输出文件路径
logappend=true #错误日志采用追加模式，配置这个选项后mongodb的日志会追加到现有的日志文件，而不是从新创建一个新文件
journal=true #启用日志文件，默认启用
quiet=true #这个选项可以过滤掉一些无用的日志信息，若需要调试使用请设置为false
port=27017 #端口号 默认为27017

    普通启动



    访问：http://localhost:27017/可以看到显示信息如下，就表明启动成功了。
    It looks like you are trying to access MongoDB over HTTP on the native driver port.
    MongoDB安装为Windows服务
    将mongodb安装为windows服务非常简单只需要在上面执行的命令行后添加 --install即可



小结：安装和简介就到此结束了，基本上安装这个部分和redis没有差别。感觉nosql的都相对轻便灵活一些。




版权声明：本文为博主原创文章，未经博主允许不得转载。

ORACLE-016：ora-01720 授权选项对于'xxxx'不存在
报错的情形如下，A用户：视图V_AB用户：视图V_B，并且用到了V_AC用户：需要用V_B，授权过程，A用户下：grant select on V_A to B
B用户下：grant select on V_B to C此时报错：ora-01720 授权选项对于'V_A'不存在。那么是什么原因呢，因为B还需要授权视图给C用户，但是B用到的视图是A下的，所以除了将V_A授权select权限给B外，还要授权操作权限。比如这里就需要在A用户下，授权grant操作权限给B，那么B才能继续授权给C。如下：A用户下：grant select on V_A to B with grant option
B用户下：grant select on V_B to C此时正确。C中能正常使用V_B了。同样如果C还要继续授权则B用户下也要依此进行授权。 

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL Server 6.0完全卸载以及卸载中遇到的问题
第一次卸载Mysql的时候一直没卸载干净，当时也没当回事，直接重装系统解决的。直到今天又遇到了这个问题，悔不当初啊，上网查了各种资料，用了半天时间才解决，到最后发现其实很简单，所以现在就写一篇文章留待下次卸载的时候使用。
1.在服务里把MySQL服务停掉

2.控制面板卸载MySQL

3.到Mysql的安装目录里删掉整个文件夹

4.在C盘删掉MySQL的数据文件，我的是在ProgramData里，这个文件夹是隐藏的，可以在文件夹选项里设置显示隐藏文件。如果删不掉，可以用文件粉碎等软件删掉。

5.到注册表里删掉MySQL的注册文件，当时就是这里没删干净，建议大家仔细找一找，注册表进入方法：开始->运行regedit

打开HKEY_LOCAL_MACHINE->SYSTEM->ControlSet001->services查看目录下是否有与Mysql有关的文件夹，如果有删掉。再进入services->eventlog->Application把与MySQL有关的文件夹删掉。 
同理，在ControlSet002，CurrentControlSet文件夹里重复上述操作。

6.关机重启，查看服务里是否还有Mysql服务，如果有，就先停止服务，然后去cmd执行sc delete mysql ，最好以管理员身份运行cmd，成功后显示[SC] DeleteService SUCCESS 。Mysql最后配置的时候在第三步出现的错误cannot create windows service for mysql.error:0也是同样的解决方法。 
第四步出现错误可能就是上面的步骤没删干净。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-基本概念和术语
数据：对客观事物的符号表示，在计算机学科中指所有能输入到计算机中并被计算机程序处理的符号的总称。 
数据元素：数据的基本单位，在计算机程序中通常作为一个整体进行考虑和处理。 
数据项：数据的不可分割的最小单位。一个数据元素可由若干个数据项组成。 
数据对象：性质相同的数据元素的集合，是数据的子集。 
数据结构：相互之间存在一种或多种特定关系的数据元素的集合。

数据元素都不是孤立存在的，在他们之间存在着某种关系，这种数据元素相互之间的关系称为结构。 
根据数据元素之间关系的不同特点，通常分以下四种基本结构： 
集合：数据元素之间除“同属于一个集合”外，没有其他关系； 
线性结构：数据元素之间存在一对一的关系； 
树形结构：数据元素之间存在一对多的关系； 
网状结构（图状结构）：数据元素之间存在多对多的关系。

上述四种结构描述的是数据元素之间的逻辑关系，因此又称为数据的逻辑结构 
数据结构在计算机中的表示（映像）称为数据的物理结构（存储结构） 
数据元素之间的关系在计算机中有两种不同的表示方式：顺序表示和非顺序表示，由此得到两种不同的存储结构：顺序存储结构和链式存储结构。

算法：对特定问题求解步骤的一种描述，是指令的有限序列。 
算法具有5个特性： 
有穷性：一个算法必须在执行有穷步之后结束，且每一步都可以在有穷时间内完成。 
确定性：算法中每一条指令必须有确切的含义，读者理解是不会产生二义性。 
可行性：算法中描述的操作都可以通过已经实现的基本操作执行有限次来实现。 
输入：一个算法有零个或多个输入。 
输出：一个算法有一个或多个输出。

算法的时间量度（时间复杂度）记作T（n）=O(f(n)) 
它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同。 
算法所需存储空间的量度（空间复杂度）记作S(n)=O(f(n))，其中n表示问题的规模。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

JDBC ORACLE 事物处理

数据库事物：
在数据库中,所谓事务是指一组逻辑操作单元,使数据从一种状态变换到另一种状态。
为确保数据库中数据的一致性,数据的操纵应当是离散的成组的逻辑单元:当它全部
完成时,数据的一致性可以保持,而当这个单元中的一部分操作失败,整个事务应全部视
为错误,所有从起始点以后的操作应全部回退到开始状态。 
事务的操作:先定义开始一个事务,然后对数据作修改操作,这时如果提交(COMMIT),这些修改
就永久地保存下来,如果回退(ROLLBACK),数据库管理系统将放弃所作的所有修
改而回到开始事务时的状态。


事务的ACID(acid)属性
1. 原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，
要么都不发生。
2. 一致性（Consistency）事务必须使数据库从一个一致性状态变换到另外一个一致性状态。
3. 隔离性（Isolation）事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务
内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个
事务之间不能互相干扰。
4. 持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，
  接下来的其他操作和数据库故障不应该对其有任何影响
  
事务：指构成单个逻辑工作单元的操作集合
事务处理：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。
当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，要么整个事务回滚(rollback)到最初状态
        当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据
库自动提交，而不能回滚
为了让多个 SQL 语句作为一个事务执行：
调用 Connection 对象的 setAutoCommit(false); 以取消自动提交事务
在所有的 SQL 语句都成功执行后，调用 commit(); 方法提交事务
在出现异常时，调用 rollback(); 方法回滚事务
若此时 Connection 没有被关闭, 则需要恢复其自动提交状态
常用的代码结构
    public void test(String sql, Object... args)
    {
        
        Connection conn = null;
        PreparedStatement prepareStatement = null;
        
        try
        {
            conn = getConn();
            // 事物处理前，取消Connection的默认提交行为
            conn.setAutoCommit(false);
            prepareStatement = conn.prepareStatement(sql);
            
            for (int i = 0; i < args.length; i++)
            {
                prepareStatement.setObject(i + 1, args[i]);
            }
            prepareStatement.executeUpdate();
            
            // 事物处理：如果事物处理成功则提交事物
            conn.commit();
        }
        catch (Exception e)
        {
            e.printStackTrace();
            try
            {
                // 事物处理：如果出现异常 则在catch块中回滚事物
                conn.rollback();
            }
            catch (SQLException e1)
            {
                e1.printStackTrace();
            }
        }
        finally
        { // 释放数据库资源
            releaseSource(prepareStatement, conn, null);
        }
        
    }

版权声明：本文为博主原创文章，未经博主允许不得转载。

为什么你应该永远不要再使用MongoDB


Sven Slootweg (joepie91)是一名黑客，同时也是CrytoCC的创建者，现在提供Node.js代码评审服务。近日，他在个人博客上发表了一篇博文《为什么你应该永远、永远、永远不要再使用MongoDB》。在文中，他列举了如下理由：

丢失数据（见1、2）；默认忽略错误，假设每次写入都是成功的，在32位系统上，这可能会导致数据无声无息地丢失；即使是在MongoDB宣传的适用场景下，其性能依然不高（见3、4）；几乎在所有的应用场景下，开发者都会被迫养成使用隐式模式的坏习惯（见4）；存在锁问题（见4）；对安全问题响应很慢（见5）；不符合ACID（见6）；扩展和维护困难；JSON存储也不是MongoDB独有的功能，PostgreSQL、CouchDB也支持（见7、8)。
joepie91认为，MongoDB不仅存在诸多问题，而且并无突出之处。如果项目涉及用户账户或者两条记录之间存在某种关系，那么就应该使用关系型数据库，而不是文档存储；如果项目在使用Mongoose，那么也应该使用关系型数据库，因为Mongoose只是使用文档存储模拟了有模式的关系型数据库。因此，大多数情况实际上需要的都是一个关系型数据库。在这些情况下，PostgreSQL是个不错的可选方案。开发者可以使用查询构建器或ORM来简化使用过程，比如，在Node.js中，可以选用Knex、Bookshelf、Sequelize或Waterline。即使真得需要一个文档存储，那么也有比MongoDB更好的选项。另外，他也不认为MongoDB适合于创建原型，因为如果生产环境使用不同的数据库，则还需要重写所有的代码。总之，MongoDB并没有什么适用场景。它在技术上比不上其它可选方案，并没有提供真正有用的独有的特性，而且开发人员也无法确保数据一致性和安全。最后，joepie91指出，流行度并不等同于质量，只能说明产品有一个不错的市场团队：

永远不要因为“其他人那样做”就使用一个数据库，对于一个特定的数据库，要自己研究它的优点和不足。

joepie91的观点在Hack News上得到了广泛的赞同。网友karmakaze也认为，有了PostgreSQL 9.4，就没有任何理由要使用MongoDB了（JSONB比BSON更合用），另外还可以使用CouchDB。对于MongoDB的具体限制，网友giaour建议阅读aphyr的系列文章Call Me Maybe，并指出，虽然存在已知的变通方案，但那大大降低了MongoDB的开发体验。网友Animats认为，如果站点的流量比维基小，那么使用某种关系型数据库就可以了。网友PebblesHD有类似的观点：

作为一个规模较小的部署……，只安装一个基本的MySQL有什么问题吗？在我们的内部维基上，我们每天的访问量已经超过了2万次……

但是也有一些不同的声音。例如，网友threeseed就表示，MongoDB仍然是最容易安装和使用的数据库之一。对此，joepie91回复如下：

以错误的方式做事，想不容易都难——MongoDB恰好就是那么做的。它不需要设置身份验证或表模式，因此才看上去“易于安装”。但实际上，为了节省10分钟，你正在浪费几个小时的时间。因为稍后，你将会遇到入侵（没有身份验证）或数据破坏……

Shodan的报道也佐证了joepie91的这一说法，互联网上有将近3万个MongoDB实例没有启用任何的身份验证。这个问题随处可见，而且已经存在多年。
网友toyg则评论说：

我最近首次使用了MongoDB，是在一个内部项目里。我认为，没有模式确实显著了提升了开发速度……现在项目已经成熟，回过头来，我可以看到为什么关系型数据库会更合适，但如果我从开始就使用RDBMS，那么我可能无法这么快地完成迁移。虽然切换到真正的RDBMS意味着要修改三两个类，但变化不大。所以，我不同意MongoDB不适合原型开发的说法。

joepie91对“修改三两个类，但变化不大”的说法提出了质疑，因为根据自己从事代码审查的经验，迁移到不同的数据库通常需要大量的工作。至于切换速度，joepie91指出，在一个有回滚机制的系统中，可能会更快。
然而，在有些情况下，开发者并没有其它选择。例如，有网友就提出，Meteor就使用而且只能使用MongoDB。而由于同Hadoop的合作伙伴关系，MongoDB同Hadoop有很好的集成，因此，它在大数据分析领域非常流行。
另外，来自SourceGear的软件开发人员Eric Sink在读过的joepie91文章之后表示：

（他所列举的内容）部分（也许全部）确有其事。事实上，现在，就假设他所写的都是正确的。我这里不是要说作者是错的。更确切地说，我这里想指出的是，这种博文只能让我了解很少有关MongoDB的知识，但却让我感受到了写这篇博文的人的许多情感。

他觉得，不能因为那些问题就彻底地否定MongoDB，毕竟：

MongoDB是顶级的NoSQL供应商。每天，成千上万的企业用它为数以百万计的用户提供服务。像所有有大量用户的新生软件一样，它有漏洞和缺陷。但它正稳步改善。任何有关技术缺陷的讨论，如果无助于解决问题，那么很大程度上只能是一种情绪的宣泄。



本文最初发表在infoq，文章内容属作者个人观点，不代表本站立场。


版权声明：本文为博主原创文章，未经博主允许不得转载。

ORACLE-017：SQL优化-is not null和nvl
今天在优化一段sql，原脚本大致如下：select  a.字段n from tab_a a

where

a.字段2 is not null;a.字段2增加了索引的，但是查询速度非常慢，于是做了如下修改：select  a.字段n from tab_a a

where

nvl(a.字段2,'0' ) != '0';速度提升很明显。原因是什么呢？其实很简单，因为is null和is not null使字段的索引失效了。虽然都知道哪些情形下会使索引失效，但是有时难免受业务需求的影响而考虑的不够全面，所以sql优化要时刻进行，随时进行。努力提高sql的执行效率。

版权声明：本文为博主原创文章，未经博主允许不得转载。

Linux 系统中重启数据库
Linux 系统中重启数据库：
1、进入数据库用户：  su - oracle
2、先关闭监听： lsnrctl stop
3、再关闭数据库服务 :  sqlplus shutdown immediate
4、开启数据库服务：sqlplus startup
5、开启监听：lsnrctl start


版权声明：本文为博主原创文章，未经博主允许不得转载。

NoSql-MongoDB+Dos窗口下的增删改
   上篇文章主要介绍了MongoDB的下载http://blog.csdn.net/huo065000/article/details/42806533，本篇主要是来简单了解一下通过Dos窗口如何来完成对MOngoDB的增删改查。
   在进入增删改的环境之前，我们还是要打开运行环境的，由于使用频繁，所以我们可以自己创建一个.bat文件来执行，
以我的mongod.exe的存放路径以及数据的存放路径举例：

D:\MongoDB\Server\3.0\bin\mongod.exe -dbpath=D:\Mongodb\data
pause运行的效果如下：



在运行的情况下，我们直接双击启动mongo.exe即可直接操作增删改查等等。
❶对数据库的操作


创建数据库

    >use DB3  


显示已有的数据库  

    >show dbs
运行效果：


❷对表的增删改操作


增加一条数据

   > db.student.insert({"name":"huo","age":"10"})


循环增加数据
   > for(var i=1;i<5;i++) db.student.insert({"name":"huohuo"+1,"age":i});


查看增加的数据

   >db.student.find()


json字符串查看增加的数据

   > var cursor=db.student.find();
       >while(cursor.hasNext()) printjson(cursor.next())
运行效果：







更新字段的操作

     > db.student.update({"age":"10"},{"name":"huo","age":"10"})


删除数据的操作

     >db.student.remove({"name":"huo"})
删除运行效果：


❸MongoDB的一些常用命令
      >show dbs    -- 查看数据库列表


> use DB3   --创建DB3数据库，如果存在DB3数据库则使用DB3数据库

> db   ---显示当前使用的数据库名称

> db.getName()  ---显示当前使用的数据库名称

> db.dropDatabase()  --删当前使用的数据库

> db.repairDatabase()  --修复当前数据库

> db.version()   --当前数据库版本

> db.getMongo()  --查看当前数据库的链接机器地址 

> db.stats() 显示当前数据库状态，包含数据库名称，集合个数，当前数据库大小 ...

> db.getCollectionNames()   --查看数据库中有那些个集合（表）

> show collections    --查看数据库中有那些个集合（表）

> db.person.drop()  --删除当前集合（表）person

部分操作的图示显示：







   Dos窗口下，对有MongoDb的一些基本操作就介绍这些，其实MongoDB一般都是用于各个项目中，比如Java，C#等常用的语言……

……待续

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL设置数据库表为只读
1、设置命令mysql> use test;
Database changed
mysql> lock table t_depart_info read;
Query OK, 0 rows affected (0.00 sec)2、插入数据3、指令分析     由于设置了t_depart_info为只读，不能向其插入数据，故截图中一直在加载请求中...

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-线性表的顺序表示和实现
线性表：最常用且最简单的一种数据结构，是n个数据元素的优先序列。线性表是一个相当灵活的数据结构，它的长度可以根据需要增长或缩短，即对线性表的数据元素不仅可以访问，还以进行插入和删除等。

线性表的顺序表示：用一组地址连续的存储单元依次存储线性表的数据元素。 
线性表的这种机内存储结构称作线性表的顺序存储结构。 
线性表的顺序存储结构是一种随机存取结构，表中的任一数据元素都可以随机存取。

读取操作：对于采用顺序存储结构的线性表，读取操作是非常容易实现的，知道第一个数据元素的存储地址LOC(a1)和每个数据元素所占用的存储单元个数L求第i个数据元素的存储位置： 
LOC(ai)=LOC(a1)+(i-1)*L
插入操作：在线性表的第i-1个数据元素和第i个数据元素之间插入一个新的数据元素b，使长度为n的线性表：（a1，…ai-1，ai，…an） 
变为长度为n+1的线性表：（a1，…ai-1，b,ai，…an）。 
由于逻辑上相邻的元素在物理位置上也是相邻的，所以第i到n个数据元素必须向后移动才能反映这个逻辑关系的变化。
删除操作：线性表的删除操作使长度为n的线性表（a1，…ai-1，ai，ai+1…an） 
变为长度为n-1的线性表（a1，…ai-1，ai+1，…an）. 
数据元素ai-1,ai,ai+1之间的逻辑关系发生了变化，为了在存储结构上反映这一变化，同样需要移动数据元素。将第i+1到第n个数据元素向前移动。
时间复杂度：在顺序存储的线性表中插入或者删除一个数据元素，其时间主要耗费在移动元素上，插入与删除操作的时间复杂度为O（n）.

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（六）——修改表、数据的备份和恢复
1、修改表


添加一个字段：alter  table  distributors


2、数据的备份和恢复

1）使用企业管理器完成备份和恢复（2种方式）


分离/附加

分离完成后，到sql  server 安装的目录下找两个文件（数据库名.mdf）和（数据库名.ldf）。这两个文件即分离后的文件，数据库分离后，该数据库不可再用。附加：当用户需要重新使用某个分离的数据库时进行的操作，即让sql  server数据库重新关联该数据库。
备份/恢复

备份数据库：把某个数据库文件从sql  server中备份出来，这样用于可以根据需要再使用（用于恢复、复用……）。不会影响源数据库的使用。恢复数据库：当源数据库因为某种原因需要恢复时进行的操作。

2）查询管理器



备份

backup  database  数据库名  to  disk='d:\\xxx.bak'
恢复

restore  database  数据库名  from  disk='d:\\xxx.bak'



版权声明：本文为博主原创文章，未经博主允许不得转载。

[Err] 2006 - MySQL server has gone away
1、错误描述[Err] 2006 - MySQL server has gone away2、错误原因     在将数据库脚本利用MySQL客户端导入时，出现这个错误；结果查明，由于脚本中的insert语句过多，插入数据量过大，导致MySQL客户端和服务器连接断开3、解决办法（1）修改MySQL配置文件my.ini       设置max_allowed_packed参数（2）查看MySQL连接是否超时（3）查看服务是否中断

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL查看运行时间
1、查看MySQL运行多长时间mysql> SHOW GLOBAL STATUS LIKE 'UPTIME';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| Uptime        | 12823 |
+---------------+-------+
1 row in set (0.00 sec)2、查看MySQL连接超时mysql> SHOW GLOBAL VARIABLES LIKE '%TIMEOUT';
+-----------------------------+----------+
| Variable_name               | Value    |
+-----------------------------+----------+
| connect_timeout             | 10       |
| delayed_insert_timeout      | 300      |
| innodb_flush_log_at_timeout | 1        |
| innodb_lock_wait_timeout    | 50       |
| innodb_rollback_on_timeout  | OFF      |
| interactive_timeout         | 28800    |
| lock_wait_timeout           | 31536000 |
| net_read_timeout            | 30       |
| net_write_timeout           | 60       |
| rpl_stop_slave_timeout      | 31536000 |
| slave_net_timeout           | 3600     |
| wait_timeout                | 28800    |
+-----------------------------+----------+
12 rows in set (0.00 sec)3、查看mysql请求链接进程被主动杀死 mysql> SHOW GLOBAL STATUS LIKE 'COM_KILL';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| Com_kill      | 0     |
+---------------+-------+
1 row in set (0.00 sec)4、查看MySQL通信信息包最大值mysql> SHOW GLOBAL VARIABLES LIKE 'MAX_ALLOWED_PACKET';
+--------------------+---------+
| Variable_name      | Value   |
+--------------------+---------+
| max_allowed_packet | 4194304 |
+--------------------+---------+
1 row in set (0.00 sec)

版权声明：本文为博主原创文章，未经博主允许不得转载。

Mysql5.7全新的root密码规则
原创文章，转载请注明出处：服务器非业余研究http://blog.csdn.net/erlib 作者Sunface联系邮箱：cto@188.com 今天在安装mysql5.7.8的时候遇到一些问题，首当其冲便的是初始root密码的变更，特分享解决方法如下： 1.mysql5.7会生成一个初始化密码，而在之前的版本首次登陆不需要登录。 shell> cat /root/.mysql_secret  # Password set for user 'root@localhost' at 2015-04-22 22:13:23  ?G5W&tz1z.cN 2.若第一步成功，则使用该密码继续第7步（笔者由于找不到该文件，只能从第3步开始) 3.修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables 4.service mysqld restart后，即可直接用mysql进入 5.mysql> update mysql.user set authentication_string=password('123qwe') where user='root' and Host = 'localhost';    mysql> flush privileges;    mysql> quit; 6.将/etc/my.cnf文件还原，重新启动mysql:service mysql restart,这个时候可以使用mysql -u root -p'123qwe'进入了 7.mysql>SET PASSWORD = PASSWORD('newpasswd'); 设置新密码  总结一下：想尝鲜，就要付出代价！

版权声明：本文为博主原创文章，未经博主允许不得转载。

Oracle与Mysql数据的事务处理机制
相比oracle的事务处理，Mydql相对还是简单一点的，但是事务作为一项重要的安全机制在数据库里面是必不可少的，特别是里面的事务回滚机制非常的有用，不多说了，先说一下mysql的事务处理：
Mysql简单的回滚：
第一步：开始事务：start transaction;
第二步：执行dml等其他的变化操作。
第三步：混滚到开始事务之前状态： rollback;


这样的话在执行完毕之后就发现之前的一些操作没有执行成功。只有commit提交之后才会真正的把数据提交。但是，还要考虑下面的两种情况，一种是服务关闭的情况，这样我们的数据回滚在没有提交的情况下是有真正的去执行，还有就是回滚是能够用一次，再次使用虽然不报错，但是不起任何作用。
如果你会说了，我想在一次事务中国实现多次回滚怎么办，这里你可以使用设置保存点：
在上面的第二步中，你可以这样一下： savepoint 保存点1（名称而已）;
然后你继续执行dml，到了耨一个地方继续savepoint 保存点2;
你是不是想通过rollback to 保存点的方式会滚到之前的状态。但是，不得不告诉你，旺旺想想的和显示的情况并不是一模一样，你会发现只能够滚滚到第一个保存
点，其他的保存点会提示不存在！
Oracle的事务处理：
Oracle是不用说明开启事务的，针对dml语句，当你设置保存点，接下来的步骤如上返回到保准点同样的用commit提交事务。


但是，我这里就总结出了一个重要的原则：事务回滚只能反方向一直逆行，这个过程已然不可逆。除非提交结束事务。








版权声明：本文为博主原创文章，未经博主允许不得转载。

[置顶]
        C++栈学习——顺序栈和链栈的区别

C++中栈有顺序栈和链栈之分，在顺序栈中，定义了栈的栈底指针（存储空间首地址base）、栈顶指针top以及顺序存储空间的大小stacksize（个人感觉这个数据成员是可以不用定义的）

//顺序栈数据结构C++类声明（基类）
template <typename ElemType>
class SqStack 
{
public:
    void clear();                                                 //把顺序栈置空
    int getLength();                                              //求顺序栈中元素个数
    int getstackSize();                                     //返回当前已分配的存储空间的大小
    Status getTop(ElemType & e);                                  //读栈顶的元素
    bool isEmpty();                                               //判断顺序栈是否为空
    SqStack<ElemType> operator =(SqStack<ElemType> rightS);       //重载赋值运算符的定义
    Status pop(ElemType & e);                                     //弹出栈顶元素到e
    void push(ElemType & e );                                     //在栈顶压入元素e
//*****************************下面为系统自动调用构造函数及析构函数声明******************************//

    SqStack(); //顺序栈构造函数
    virtual ~SqStack();//顺序栈析构函数
    SqStack (const SqStack<ElemType>& otherS);//顺序栈拷贝初始换构造函数

protected:
    ElemType *base;
    ElemType *top;
    int stackSize;//顺序存储空间的大小
};

而对于链栈来说，它只定义栈顶指针。

template<typename ElemType>
class Linkstack
{

private:
    class LinkNode
    {
    public:
        ElemType data;
        LinkNode *next;
    };
    typedef LinkNode * NodePointer;

public:
    void clear();
    int getlength();
    void display();
    void randLinkStack();
    Linkstack <ElemType> operator = (Linkstack <ElemType> rightS);

protected:
    NodePointer top;

};
其实这二者的区别是由顺序表和链表的存储结构决定的，在空间上，顺序表是静态分配的，而链表则是动态分配的；就存储密度来说：顺序表等于1，而链式表小于1，但是链式表可以将很多零碎的空间利用起来；顺序表查找方便，链式表插入和删除时很方便。 
    顺序表的这种静态存储的方式，决定了必须至少得有首地址和末地址来决定一个空间，否则，不知道查找到哪了；链式表每个节点存储了下一个节点的指针信息，故，对于链栈来说，只需要一个top指针即可查找到整个栈。 
    另外，顺序栈和链栈的top指针有区别，顺序栈的top指针指向栈定的空元素处，top-1才指向栈定元素，而链栈的top指针相当于链表的head指针一样，指向实实在在的元素。 
    另外附自己写的顺序栈和链栈的随机产生函数：
//顺序栈：
template<typename ElemType>
void MyStack<ElemType>::RandStack()
{
    int *p;
    ElemType n;
    ElemType Elem[11];
    srand(time(NULL));
    n=rand()%10+1;
    cout<<"产生的随机栈的深度为："<<n<<endl;
    cout<<"产生的随机栈元素为："<<endl;
    for (int i = 0; i < n; i++)
    {
        Elem[i]=rand()%100+1;
        cout<<Elem[i]<<"  ";
    }
    cout<<endl;
    base=new ElemType[n];
    assert(base!=0);
    top=base;
    stackSize=n;
    for (int j = 0; j < n; j++)
        *(base+j)=Elem[j];
    top=base+n;
    cout<<"随机产生的栈为："<<endl;
    for (int i = 0; i < stackSize; i++)
        cout<<"  "<<*(base+i);
    cout<<endl;
    cout<<"  ♂";
    for (int i = 1; i <stackSize ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<"  ♂"<<endl;
    cout<<" base";
    for (int i = 1; i <stackSize ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<" top"<<endl;

}

template<typename ElemType>
void MyStack<ElemType>::display()
{
    int n=top-base;
    cout<<"当前栈为："<<endl;
    for (int i = 0; i < n; i++)
        cout<<"  "<<*(base+i);
    cout<<endl;
    cout<<"  ♂";
    for (int i = 1; i <n ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<"  ♂"<<endl;
    cout<<" base";
    for (int i = 1; i <n ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<" top"<<endl;
}

//链栈
template<typename ElemType>
void Linkstack<ElemType>::display()
{
    NodePointer r;
    int num=0;
    r=top;
    while (r)
    {
        cout<<r->data<<"  ";
        r=r->next;
        num++;
    }
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"↑"  ;
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"top"<<endl;

}

template <typename ElemType>
void Linkstack<ElemType>::randLinkStack()
{
    ElemType elem[11];
    srand(unsigned(time(NULL)));
    int n;
    n=rand()%10+1;
    cout<<"the number of the stack is:"<<n<<endl;
    cout<<"the elements here are:";
    for (int i = 0; i < n; i++)
    {
        elem[i]=rand()%100+1;
        cout<<elem[i]<<"  ";
    }
    cout<<endl;
    NodePointer p,s;
    p=NULL;
    for (int i = 0; i < n; i++)
    {
        s=new(LinkNode);
        assert(s!=NULL);
        s->data=elem[i];
        s->next=p;
        p=s;
    }
    top=p;
    cout<<"the stack produced is:"<<endl;
    NodePointer r;
    int num=0;
    r=top;
    while (r)
    {
        cout<<r->data<<"  ";
        r=r->next;
        num++;
    }
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"↑"  ;
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"top"<<endl;
}


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

[ERROR:] [插入失败]   code is 9998;desc is 得到唯一对象不唯一exception is null
1、错误描述[ERROR:]2015-08-25 17:04:38,861 [插入失败] 
 code is 9998;desc is 得到唯一对象不唯一exception is null
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.aop.framework.adapter.AfterReturningAdviceInterceptor.invoke(AfterReturningAdviceInterceptor.java:52)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy72.queryGoodsLogBySerial(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle

(ServletInvocableHandlerMethod.java:104)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod

(RequestMappingHandlerAdapter.java:749)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal

(RequestMappingHandlerAdapter.java:689)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at com.skycloud.oa.filter.TranscationFilter.doFilter(TranscationFilter.java:32)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at com.skycloud.oa.filter.ContentFilter.doFilter(ContentFilter.java:57)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:88)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:344)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:261)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)
2、错误原因SELECT 
  t.name 
FROM
  t_stu_info t 
WHERE t.id = 
  (SELECT 
    r.code 
  FROM
    t_teacher_info r 
  WHERE r.name = 'zhangsan') ;      内存查询的结果不唯一，导致id对应多个；本来id是需要是唯一的，但是这个查询出来的结果超过一个，导致报错3、解决办法（1）保证内存查询结果唯一，并且不出现重复（2）如果需要id为多个，可以将“=”改为“in”

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL 配置参数详解
MySQL安装成功后有几个默认的配置模板，列表如下：
my-huge.cnf ： 用于高端产品服务器，包括1到2GB RAM,主要运行mysql
my-innodb-heavy-4G.ini ： 用于只有innodb的安装，最多有4GB RAM，支持大的查询和低流量
my-large.cnf ： 用于中等规模的产品服务器，包括大约512M RAM
my-medium.cnf ： 用于低端产品服务器，包括很少内存（少于128M）
my-small.cnf ： 用于最低设备的服务器，只有一点内存（少于512M）
my.cnf具体的配置说明如下：




basedir = path
使用给定目录作为根目录(安装目录)。


character-sets-dir = path
给出存放着字符集的目录。


datadir = path
从给定目录读取数据库文件。


pid-file = filename
为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统); Init-V脚本需要使用这个文件里的进程ID结束mysqld进程。


socket = filename
为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(仅适用于UNIX/Linux系统; 默认设置一般是/var/lib/mysql/mysql.sock文件)。在Windows环境下，如果MySQL客户与服务器是通过命名管道进行通信 的，–sock选项给出的将是该命名管道的名字(默认设置是MySQL)。


lower_case_table_name = 1/0
新目录和数据表的名字是否只允许使用小写字母; 这个选项在Windows环境下的默认设置是1(只允许使用小写字母)。



mysqld程序：语言设置




character-sets-server = name
新数据库或数据表的默认字符集。为了与MySQL的早期版本保持兼容，这个字符集也可以用–default-character-set选项给出; 但这个选项已经显得有点过时了。


collation-server = name
新数据库或数据表的默认排序方式。


lanuage = name
用指定的语言显示出错信息。



mysqld程序：通信、网络、信息安全




enable-named-pipes
允许Windows 2000/XP环境下的客户和服务器使用命名管道(named pipe)进行通信。这个命名管道的默认名字是MySQL，但可以用–socket选项来改变。


local-infile [=0]
允许/禁止使用LOAD DATA LOCAL语句来处理本地文件。


myisam-recover [=opt1, opt2, ...]
在启动时自动修复所有受损的MyISAM数据表。这个选项的可取值有4种:DEFAULT、BACKUP、QUICK和FORCE; 它们与myisamchk程序的同名选项作用相同。


old-passwords
使用MySQL 3.23和4.0版本中的老算法来加密mysql数据库里的密码(默认使用MySQL 4.1版本开始引入的新加密算法)。


port = n
为MySQL程序指定一个TCP/IP通信端口(通常是3306端口)。


safe-user-create
只有在mysql.user数据库表上拥有INSERT权限的用户才能使用GRANT命令; 这是一种双保险机制(此用户还必须具备GRANT权限才能执行GRANT命令)。


shared-memory
允许使用内存(shared memory)进行通信(仅适用于Windows)。


shared-memory-base-name = name
给共享内存块起一个名字(默认的名字是MySQL)。


skip-grant-tables
不使用mysql数据库里的信息来进行访问控制(警告:这将允许用户任何用户去修改任何数据库)。


skip-host-cache
不使用高速缓存区来存放主机名和IP地址的对应关系。


skip-name-resovle
不把IP地址解析为主机名; 与访问控制(mysql.user数据表)有关的检查全部通过IP地址行进。


skip-networking
只允许通过一个套接字文件(Unix/Linux系统)或通过命名管道(Windows系统)进行本地连接，不允许ICP/IP连接; 这提高了安全性，但阻断了来自网络的外部连接和所有的Java客户程序(Java客户即使在本地连接里也使用TCP/IP)。


user = name
mysqld程序在启动后将在给定UNIX/Linux账户下执行; mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序。



mysqld程序：内存管理、优化、查询缓存区




bulk_insert_buffer_size = n
为一次插入多条新记录的INSERT命令分配的缓存区长度(默认设置是8M)。


key_buffer_size = n
用来存放索引区块的RMA值(默认设置是8M)。


join_buffer_size = n
在参加JOIN操作的数据列没有索引时为JOIN操作分配的缓存区长度(默认设置是128K)。


max_heap_table_size = n
HEAP数据表的最大长度(默认设置是16M); 超过这个长度的HEAP数据表将被存入一个临时文件而不是驻留在内存里。


max_connections = n
MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。


query_cache_limit = n
允许临时存放在查询缓存区里的查询结果的最大长度(默认设置是1M)。


query_cache_size = n
查询缓存区的最大长度(默认设置是0，不开辟查询缓存区)。


query_cache_type = 0/1/2
查询缓存区的工作模式:0, 禁用查询缓存区; 1，启用查询缓存区(默认设置); 2，”按需分配”模式，只响应SELECT SQL_CACHE命令。


read_buffer_size = n
为从数据表顺序读取数据的读操作保留的缓存区的长度(默认设置是128KB); 这个选项的设置值在必要时可以用SQL命令SET SESSION read_buffer_size = n命令加以改变。


read_rnd_buffer_size = n
类似于read_buffer_size选项，但针对的是按某种特定顺序(比如使用了ORDER BY子句的查询)输出的查询结果(默认设置是256K)。


sore_buffer = n
为排序操作分配的缓存区的长度(默认设置是2M); 如果这个缓存区太小，则必须创建一个临时文件来进行排序。


table_cache = n
同时打开的数据表的数量(默认设置是64)。


tmp_table_size = n
临时HEAP数据表的最大长度(默认设置是32M); 超过这个长度的临时数据表将被转换为MyISAM数据表并存入一个临时文件。



mysqld程序：日志




log [= file]
把所有的连接以及所有的SQL命令记入日志(通用查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname.log文件作为这种日志文件(hostname是服务器的主机名)。


log-slow-queries [= file]
把执行用时超过long_query_time变量值的查询命令记入日志(慢查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname-slow.log文件作为这种日志文件(hostname是服务器主机 名)。


long_query_time = n
慢查询的执行用时上限(默认设置是10s)。


long_queries_not_using_indexs
把慢查询以及执行时没有使用索引的查询命令全都记入日志(其余同–log-slow-queries选项)。


log-bin [= filename]
把对数据进行修改的所有SQL命令(也就是INSERT、UPDATE和DELETE命令)以二进制格式记入日志(二进制变更日志，binary update log)。这种日志的文件名是filename.n或默认的hostname.n，其中n是一个6位数字的整数(日志文件按顺序编号)。


log-bin-index = filename
二进制日志功能的索引文件名。在默认情况下，这个索引文件与二进制日志文件的名字相同，但后缀名是.index而不是.nnnnnn。


max_binlog_size = n
二进制日志文件的最大长度(默认设置是1GB)。在前一个二进制日志文件里的信息量超过这个最大长度之前，MySQL服务器会自动提供一个新的二进制日志文件接续上。


binlog-do-db = dbname
只把给定数 据库里的变化情况记入二进制日志文件，其他数据库里的变化情况不记载。如果需要记载多个数据库里的变化情况，就必须在配置文件使用多个本选项来设置，每个数据库一行。


binlog-ignore-db = dbname
不把给定数据库里的变化情况记入二进制日志文件。


sync_binlog = n
每经过n次日志写操作就把日志文件写入硬盘一次(对日志信息进行一次同步)。n=1是最安全的做法，但效率最低。默认设置是n=0，意思是由操作系统来负责二进制日志文件的同步工作。


log-update [= file]
记载出错情况的日志文件名(出错日志)。这种日志功能无法禁用。如果没有给出file参数，MySQL会使用hostname.err作为种日志文件的名字。



mysqld程序：镜像(主控镜像服务器)




server-id = n
给服务器分配一个独一无二的ID编号; n的取值范围是1~2的32次方启用二进制日志功能。


log-bin = name
启用二进制日志功能。这种日志的文件名是filename.n或默认的hostname.n，其中的n是一个6位数字的整数(日志文件顺序编号)。


binlog-do/ignore-db = dbname
只把给定数据库里的变化情况记入二进制日志文件/不把给定的数据库里的变化记入二进制日志文件。



mysqld程序：镜像(从属镜像服务器)




server-id = n
给服务器分配一个唯一的ID编号


log-slave-updates
启用从属服务器上的日志功能，使这台计算机可以用来构成一个镜像链(A->B->C)。


master-host = hostname
主控服务器的主机名或IP地址。如果从属服务器上存在mater.info文件(镜像关系定义文件)，它将忽略此选项。


master-user = replicusername
从属服务器用来连接主控服务器的用户名。如果从属服务器上存在mater.info文件，它将忽略此选项。


master-password = passwd
从属服务器用来连接主控服务器的密码。如果从属服务器上存在mater.info文件，它将忽略此选项。


master-port = n
从属服务器用来连接主控服务器的TCP/IP端口(默认设置是3306端口)。


master-connect-retry = n
如果与主控服务器的连接没有成功，则等待n秒(s)后再进行管理方式(默认设置是60s)。如果从属服务器存在mater.info文件，它将忽略此选项。


master-ssl-xxx = xxx
对主、从服务器之间的SSL通信进行配置。


read-only = 0/1
0: 允许从属服务器独立地执行SQL命令(默认设置); 1: 从属服务器只能执行来自主控服务器的SQL命令。


read-log-purge = 0/1
1: 把处理完的SQL命令立刻从中继日志文件里删除(默认设置); 0: 不把处理完的SQL命令立刻从中继日志文件里删除。


replicate-do-table = dbname.tablename
与–replicate-do-table选项的含义和用法相同，但数据库和数据库表名字里允许出现通配符”%” (例如: test%.%–对名字以”test”开头的所有数据库里的所以数据库表进行镜像处理)。


replicate-do-db = name
只对这个数据库进行镜像处理。


replicate-ignore-table = dbname.tablename
不对这个数据表进行镜像处理。


replicate-wild-ignore-table = dbn.tablen
不对这些数据表进行镜像处理。


replicate-ignore-db = dbname
不对这个数据库进行镜像处理。


replicate-rewrite-db = db1name > db2name
把主控数据库上的db1name数据库镜像处理为从属服务器上的db2name数据库。


report-host = hostname
从属服务器的主机名; 这项信息只与SHOW SLAVE HOSTS命令有关–主控服务器可以用这条命令生成一份从属服务器的名单。


slave-compressed-protocol = 1
主、从服务器使用压缩格式进行通信–如果它们都支持这么做的话。


slave-skip-errors = n1, n2, …或all
即使发生出错代码为n1、n2等的错误，镜像处理工作也继续进行(即不管发生什么错误，镜像处理工作也继续进行)。如果配置得当，从属服务器不应 该在执行 SQL命令时发生错误(在主控服务器上执行出错的SQL命令不会被发送到从属服务器上做镜像处理); 如果不使用slave-skip-errors选项，从属服务器上的镜像工作就可能因为发生错误而中断，中断后需要有人工参与才能继续进行。



mysqld–InnoDB：基本设置、表空间文件




skip-innodb
不加载InnoDB数据表驱动程序–如果用不着InnoDB数据表，可以用这个选项节省一些内存。


innodb-file-per-table
为每一个新数据表创建一个表空间文件而不是把数据表都集中保存在中央表空间里(后者是默认设置)。该选项始见于MySQL 4.1。


innodb-open-file = n
InnoDB数据表驱动程序最多可以同时打开的文件数(默认设置是300)。如果使用了innodb-file-per-table选项并且需要同时打开很多数据表的话，这个数字很可能需要加大。


innodb_data_home_dir = p
InnoDB主目录，所有与InnoDB数据表有关的目录或文件路径都相对于这个路径。在默认的情况下，这个主目录就是MySQL的数据目录。


innodb_data_file_path = ts
用来容纳InnoDB为数据表的表空间: 可能涉及一个以上的文件; 每一个表空间文件的最大长度都必须以字节(B)、兆字节(MB)或千兆字节(GB)为单位给出; 表空间文件的名字必须以分号隔开; 最后一个表空间文件还可以带一个autoextend属性和一个最大长度(max:n)。例如，ibdata1:1G; ibdata2:1G:autoextend:max:2G的意思是: 表空间文件ibdata1的最大长度是1GB，ibdata2的最大长度也是1G，但允许它扩充到2GB。除文件名外，还可以用硬盘分区的设置名来定义表
 空间，此时必须给表空间的最大初始长度值加上newraw关键字做后缀，给表空间的最大扩充长度值加上raw关键字做后缀(例如/dev/hdb1: 20Gnewraw或/dev/hdb1:20Graw); MySQL 4.0及更高版本的默认设置是ibdata1:10M:autoextend。


innodb_autoextend_increment = n
带有autoextend属性的表空间文件每次加大多少兆字节(默认设置是8MB)。这个属性不涉及具体的数据表文件，那些文件的增大速度相对是比较小的。


innodb_lock_wait_timeout = n
如果某个事务在等待n秒(s)后还没有获得所需要的资源，就使用ROLLBACK命令放弃这个事务。这项设置对于发现和处理未能被InnoDB数据表驱动 程序识别出来的死锁条件有着重要的意义。这个选项的默认设置是50s。


innodb_fast_shutdown 0/1
是否以最快的速度关闭InnoDB，默认设置是1，意思是不把缓存在INSERT缓存区的数据写入数据表，那些数据将在MySQL服务器下次启动 时再写入 (这么做没有什么风险，因为INSERT缓存区是表空间的一个组成部分，数据不会丢失)。把这个选项设置为0反面危险，因为在计算机关闭时，InnoDB 驱动程序很可能没有足够的时间完成它的数据同步工作，操作系统也许会在它完成数据同步工作之前强行结束InnoDB，而这会导致数据不完整。



mysqld程序：InnoDB–日志




innodb_log_group_home_dir = p
用来存放InnoDB日志文件的目录路径(如ib_logfile0、ib_logfile1等)。在默认的情况下，InnoDB驱动程序将使用 MySQL数据目录作为自己保存日志文件的位置。


innodb_log_files_in_group = n
使用多少个日志文件(默认设置是2)。InnoDB数据表驱动程序将以轮转方式依次填写这些文件; 当所有的日志文件都写满以后，之后的日志信息将写入第一个日志文件的最大长度(默认设置是5MB)。这个长度必须以MB(兆字节)或GB(千兆字节)为单 位进行设置。


innodb_flush_log_at_trx_commit = 0/1/2
这个选项决定着什么时候把日志信息写入日志文件以及什么时候把这些文件物理地写(术语称为”同步”)到硬盘上。设置值0的意思是每隔一秒写一次日 志并进行 同步，这可以减少硬盘写操作次数，但可能造成数据丢失; 设置值1(设置设置)的意思是在每执行完一条COMMIT命令就写一次日志并进行同步，这可以防止数据丢失，但硬盘写操作可能会很频繁; 设置值2是一般折衷的办法，即每执行完一条COMMIT命令写一次日志，每隔一秒进行一次同步。


innodb_flush_method = x
InnoDB日志文件的同步办法(仅适用于UNIX/Linux系统)。这个选项的可取值有两种: fdatasync，用fsync()函数进行同步; O_DSYNC，用O_SYNC()函数进行同步。


innodb_log_archive = 1
启用InnoDB驱动程序的archive(档案)日志功能，把日志信息写入ib_arch_log_n文件。启用这种日志功能在InnoDB与 MySQL一起使用时没有多大意义(启用MySQL服务器的二进制日志功能就足够用了)。



mysqld程序–InnoDB：缓存区的设置和优化




innodb_log_buffer_pool_size = n
为InnoDB数据表及其索引而保留的RAM内存量(默认设置是8MB)。这个参数对速度有着相当大的影响，如果计算机上只运行有 MySQL/InnoDB数据库服务器，就应该把全部内存的80%用于这个用途。


innodb_log_buffer_size = n
事务日志文件写操作缓存区的最大长度(默认设置是1MB)。


innodb_additional_men_pool_size = n
为用于内部管理的各种数据结构分配的缓存区最大长度(默认设置是1MB)。


innodb_file_io_threads = n
I/O操作(硬盘写操作)的最大线程个数(默认设置是4)。


innodb_thread_concurrency = n
InnoDB驱动程序能够同时使用的最大线程个数(默认设置是8)。



mysqld程序：其它选项




bind-address = ipaddr
MySQL服务器的IP地址。如果MySQL服务器所在的计算机有多个IP地址，这个选项将非常重要。


default-storage-engine = type
新数据表的默认数据表类型(默认设置是MyISAM)。这项设置还可以通过–default-table-type选项来设置。


default-timezone = name
为MySQL服务器设置一个地理时区(如果它与本地计算机的地理时区不一样)。


ft_min_word_len = n
全文索引的最小单词长度工。这个选项的默认设置是4，意思是在创建全文索引时不考虑那些由3个或更少的字符构建单词。


Max-allowed-packet = n
客户与服务器之间交换的数据包的最大长度，这个数字至少应该大于客户程序将要处理的最大BLOB块的长度。这个选项的默认设置是1MB。


Sql-mode = model1, mode2, …
MySQL将运行在哪一种SQL模式下。这个选项的作用是让MySQL与其他的数据库系统保持最大程度的兼容。这个选项的可取值包括ansi、db2、 oracle、no_zero_date、pipes_as_concat。



注意：如果在配置文件里给出的某个选项是mysqld无法识别的，MySQL服务器将不启动。

版权声明：本文为博主原创文章，未经博主允许不得转载。

每天进步一点点————MySQL锁
一、           锁
MySQL对MyISAM和MEMORY引擎实现行表级锁，对BDB存储引擎进行页级锁，对InnDB存储引擎表进行行行级锁。
按照粒度分：从大到小（MySQL仅支持表级锁，行锁需要存储引擎完成；所有引擎都有自己锁策略）
                   表锁：锁定整张表，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率最高，并发度最低。
                  页锁：锁定一个数据块（数据页面）。开销和加锁时间介于表锁行锁之间，会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。
                   行锁：锁定一个行。开销大，加锁慢；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。
一般来说,表锁适合以查询为主，只有少量按索引条件爱你更新数据的应用，如web应用。
行级锁适合大量按索引条件并发更新少量不同数据，同时又有并发查询能力的应用，如一些在线事务系统（OLTP）
1.   锁语句
手动加锁：lock tables 表名 [read|write]
 
      给t9表上只读锁
mysql>lock table t9 read;
Query OK, 0 rows affected (0.00 sec)
      查看锁
mysql>  show global status like"table_locks%";
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| Table_locks_immediate | 122   |                          
发生表锁定操作, 但表锁定后马上释放
| Table_locks_waited    | 0    |                         
发生表锁定,并因此具有锁等待
+-----------------------+-------+
2 rows in set (0.00 sec)
      给t9表上写锁
mysql>lock table t9 write;
Query OK, 0 rows affected (0.00 sec)
解锁表
mysql>unlock tables;
Query OK, 0 rows affected (0.00 sec)
2.   MyISAM表锁
         MyISAM存储引擎只支持表锁。
查看表级锁的征用情况
mysql> show status like 'table%';
+----------------------------+-------+
| Variable_name              | Value |
+----------------------------+-------+
| Table_locks_immediate      | 1258 |
|Table_locks_waited         | 0     |                    锁等待
| Table_open_cache_hits      | 99   |
| Table_open_cache_misses    | 1    |
| Table_open_cache_overflows | 0     |
+----------------------------+-------+
5 rows in set (0.00 sec)
如果说TABLE_LOCKS_WAITED的值比较高，则说明存在着较严重的争用情况。
mysql>lock table emp read local
Query OK, 0 rows affected (0.00 sec)
如果在locktables 时加了local，作用是在满足MyISAM表并发插入条件的情况下，允许其他用户在表尾插入记录。
 
mysql>select * from emp1;
ERROR 1100 (HY000): Table 'emp1' was notlocked with LOCK TABLES
 
在使用locktables 给表加锁时，必须同事取得所有涉及表的锁，并且MySQL不支持锁升级。也就是说，如果是读锁，只能执行查询操作，不能执行更新操作。
 
mysql>update emp set store_id=30 where id=25;
ERROR 1099 (HY000): Table 'emp' was lockedwith a READ lock and can't be updated
 
并且不能通过别名进行访问，所以需要对别名也要加锁。
 
mysql>select a.id from emp a;
ERROR 1100 (HY000): Table 'a' was notlocked with LOCK TABLES
 
并发插入
MyISAM表的读和写是串行的，但是在一定条件下，MyISAM表也会支持查询和插入操作的并发进行。
MyISAM存储引擎有一个系统变量concurrent_insert，是专门用来控制其并发插入的行为，值分别为0、1、2。
当为0时：不允许并发插入。
当为1时：如果MyISAM表中没有空洞，MyISAM允许在一个进程读取表，另一个进程从表尾插入记录。（默认）无论MyISAM表中有没有空洞都允许在表尾并发插入记录。
 
MyISAM调度锁
MyISAM存储引擎的读写锁是互斥的，如果一个进程请求某个MyISAM表的读锁，同时另一个进程请求同一个表的写锁，那么MySQL会让写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插入到读锁请求之前。这是因为MySQL认为写请求比一般的读请求重要。因此，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况非常糟糕！
幸好我们可以通过一些设置来调节MyISAM的调度行为。
         通过指定启动参数low-priority-updates,使MyISAM引擎默认给予读请求，以有线的权利。
         通过执行命令SETLOW_PRIORITY_UPDATES=1,使该链接发出的更新请求优先级降低。
         通过指定 insert、update、delete语句的LOW_PRIORITY属性，降低该语句的优先级。
         并且也可以通过给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁到达这个值后，MySQL就暂时将写请求的优先级降低，给读进程一个获得锁的机会。
这里强调一点：一个需要长时间的运行的查询操作，也会使写进程“饿死”！因此要尽量避免长时间查询操作。

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（四）——多表查询、子查询、分页查询、用查询结果创建新表和外连接
1、多表查询
1）笛卡尔集：


select  *  from  表名1，表名2select  *  from  表名1，表名2  where   表名1.字段名=表名2.字段名
注：



若有两张表有相同名字的字段，则使用时需带表名（别名）。order  by  置于where 条件之后。
2）自连接：同一张表的连接查询，将一张表视为两张表或多张表。

eg:显示公司每个员工名字和他的上级的名字。将emp表看做两张表worker和boss


select  worker.ename  雇员，boss.ename  老板   from   emp  worker,emp  boss  where   worker.mgr=boss.empno



2、子查询（嵌套查询）：嵌入在其它sql语句中的select语句。
1）单行子查询：只返回一行数据的子查询语句。
2）多行子查询：返回多行数据的子查询。
3）在from子句中使用子查询。
说明：
     当在from子句中使用子查询时，该子查询会被作为一个视图（临时表）来对待，因此也叫做内嵌视图。当在from子句中使用查询时，必须给子查询指定别名。


3、分页查询
1）top  n：前n条记录。


select  top  5  *  from  emp  order  by  hiredate显示第5个到第9个人的信息（按sal高低）

select  top  5   from  emp  where  empno  not  in  (select  top  4  empno  from  emp order  by  sal  desc)  order  by  sal  desc
identity(1,1)：表示字段自增长，从“1”开始增长，每次加“1”。

create  table  test  (testId  int  primary  key  identity(1,1))




4、用查询结果创建新表


select  *  into  另一个表名  from  表名删除表中的重复记录

select  distinct  *  into  #temo(新表)  from  表名1delete  from  表名1insert  into  表名1  select  *  from  #tempdrop  table  #temp




5、外连接：


左外连接：左边的表的记录全部显示，如果没有匹配的记录，用Null填补。右外连接：右边的表的记录全部显示，如果没有匹配的记录，用Null填补。



版权声明：本文为博主原创文章，未经博主允许不得转载。

在java代码中使用Oracle数据库的事务处理机制
//使用java代码操作oracle数据库的代码如下：
package Transaction;


import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

public class demon_1 {
	public static void main(String[] args) throws SQLException {
		//2：连接数据库
		Connection ct=null;
		//加载数据库驱动
		try {
			//1:加载数据库驱动
			Class.forName("oracle.jdbc.driver.OracleDriver");
			ct=DriverManager.getConnection("jdbc:oracle:thin:@127.0.0.1:1521:ORCL","SCOTT","toor");
			
			//3：预编译sql语句
//			PreparedStatement ps=ct.prepareStatement();
			//两种预编译都可以
			Statement ps=ct.createStatement();
			//4： 执行sql语句，执行结果集
			ResultSet res=ps.executeQuery("select * from emp_1");
			while(res.next()){
				System.out.println("员工名字是："+res.getString("ename"));
			}
			//执行事务
			ct.setAutoCommit(false);
			ps.executeUpdate("update emp_1 set ename='ccc' where empno=4");
			ps.executeUpdate("update emp_1 set ename='bbb' where emno=2");
			//提交事务
			ct.commit();
			ct.close();
		} catch (Exception e) {
			//取消事务
			ct.rollback();
			System.out.println("SQL语句执行错误。执行了事务回滚");
		}
	}
}





版权声明：本文为博主原创文章，未经博主允许不得转载。

一次数据库相关操作卡住的排查--enq: TX - row lock contention

问题描述：某日客户来电某HR系统排值班表的操作一直HANG住，一直无法完成。
这种问题，主要思路是围绕查看此操作因何HANG住。
常见的严重的HANG住有DB方面的AUDIT无空间、归档空间满以及主机方面CPU/内存使用率高或/根目录满等状况；

在此排查过程中，主机状态是第一步要查询的，如磁盘空间/CPU/内存使用情况等。
主机无异常后，查看DB状态，包括进程状态。如查看ALERT日志、确认能否登陆，查询一些V$视图、进行REDO切换等；
在这些基础的确认后再查询当前正在发生的等待事件及相关SESSION信息。

在本次问题中主要等待事件是enq: TX - row lock contention行锁问题。大致处理思路如下：
1.查询主机相关状态是否正常
2.查看DB状态
3.查看当前主要正在发生的等待事件及相关SESSION信息
4.确认相关等待事件后进一步排查何种原因引起
5.找到原因后进行处理




分析处理过程如下：
1.查询主机相关状态是正常的，此处不再多写输出。
使用命令有如下:
[oracle@ABCSRV02 ~]$ top
[oracle@ABCSRV02 ~]$ free -m
[oracle@ABCSRV02 ~]$ cat /proc/meminfo 
[oracle@ABCSRV02 ~]$ df -h

2.查看DB状态
查看通过监听连接的进程数
[oracle@ABCSRV02 ~]$ ps -ef|grep LOCAL=NO|wc -l
lsnrctl status
ps -ef|grep smon

sqlplus / as sysdba登陆后查询了如下信息：
--查询实例状态
select to_char(startup_time,'yyyy/mm/dd hh24:mi:ss'),instance_name,status from v$instance;
--查询DB读写状态
select name,open_mode from v$database;
--查询数据文件及表空间状态是否正常
set linesize 160
set pagesize 1000
col tablespace_name for a22
select tablespace_name,status from dba_tablespaces;
select file#,status from v$datafile;
　select * from v$recover_file;
--进行REDO LOG切换
SQL> alter system switch logfile;
System altered.
此处确认状态正常。

3.查看当前主要正在发生的等待事件及相关SESSION信息
set linesize 200
col event for a25
col username for a10
select g.Inst_id,g.sid,g.serial#,g.event,g.username, g.sql_id
from gv$session g,v$sql s
where g.Wait_class <> 'Idle' and g.sql_hash_value=s.HASH_VALUE;
  
本处查询到的是TX锁相关事件，使用如下SQL语句查询：  
select g.Inst_id,g.sid,g.serial#,g.event,g.username, g.sql_id
from gv$session g,v$sql s
where g.Wait_class <> 'Idle' and g.sql_hash_value=s.HASH_VALUE and g.event like '%T%';
--为了隐私实际查询的结果进行了一些修改。
   INST_ID        SID    SERIAL# EVENT                                    USERNAME   SQL_HASH_VALUE
---------- ---------- ---------- ---------------------------------------- ---------- --------------
         1       1056        397 enq: TX - row lock contention           ABCHR         1828393250
         1       1058       1981 enq: TX - row lock contention           ABCHR         3151192798
         1       1069       3802 enq: TX - row lock contention           ABCHR          734455977
         1       1075        397 enq: TX - row lock contention           ABCHR         3627991293
         1       1076       4025 enq: TX - row lock contention           ABCHR         3204010047
         1       1084       2990 enq: TX - row lock contention           ABCHR         3636242951
         1       1088       1185 enq: TX - row lock contention           ABCHR         1010386057
         1       1093        685 enq: TX - row lock contention           ABCHR         2969205112


4.确认相关等待事件后进一步排查

查询到大量enq: TX - row lock contention后，需要查出这些会话等待在哪个会话来释放enq: TX - row lock contention行锁资源。
也就是被哪个会话的什么操作阻塞。
--使用SQL如下：
select blocking_session,sid,serial#,wait_class,seconds_in_wait
from v$session
where blocking_session is not NULL
and sid in('sid');

如下查询到是SID=1050的会话阻塞了这些SQL；
SQL> select blocking_session,sid,serial#,wait_class,seconds_in_wait
  2  from v$session
  3  where blocking_session is not NULL
and sid in(1056,1058,1069,1075,1076,1084,1088,1093);
  4  
BLOCKING_SESSION        SID    SERIAL# WAIT_CLASS              SECONDS_IN_WAIT
---------------- ---------- ---------- ----------------------- ---------------
            1050       1056        397 Application                       10561
            1050       1058       1981 Application                        4129
            1050       1069       3802 Application                        8549
            1050       1075        397 Application                       10913
            1050       1076       4025 Application                         623
            1050       1084       2990 Application                        6234
            1050       1088       1185 Application                        2549
            1050       1093        685 Application                        8264

8 rows selected.

下一步进行查询SID=1050的会话在执行什么操作有何等待事件
SQL> set linesize 200
SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS from v$session where sid in('1050');

       SID    SERIAL# MACHINE              PROGRAM              EVENT                     SQL_ID        STATUS
---------- ---------- -------------------- -------------------- ------------------------- ------------- --------
      1050          8 ABCSRV01            JDBC Thin Client     latch: cache buffers chai 62rv794fswmcy ACTIVE
                                                                ns
可以看到此会话在执行的SQLID为62rv794fswmcy，相关等待事件是：latch: cache buffers chains；
进一步得出此SQL语句的文本及相应执行计划，以及事务的相关信息。
--如下SQL语句截取了部分，并修改了表、列名信息。语句较长，有多个case when判断及嵌套多个视图等复杂查询；
SQL> select sql_text from v$sqltext where sql_id='62rv794fswmcy' order by PIECE;

SQL_TEXT
----------------------------------------------------------------
update AAAA set ts='2015-08-24 13:14:10',AAAAA = ( cas
e when ( pAAAs = 'AA' or pAAAs = 'AAAAAAAA' ) then 0 else
 ( select AAAA from AAAA where pk_bclbid = AAAA.p
kAAAs ) end ), if_rest = ( case when ( pAAss = 'AAAX' or pk_c

查看此SQL的执行计划等信息--这里使用了awrsqrpt报告
--从游标缓存中查询此SQL语句执行计划信息：
col plan_table_output for a100
set long 900
set pagesize 100
 select * from table(dbms_xplan.display_cursor('62rv794fswmcy',0,'advanced'));
--生成awrsqrpt报告
SQL> @?/rdbms/admin/awrsqrpt.sql
---此处执行计划太长，文本型的基本不具有可读性，参考了awrsqrpt报告中的信息如下。
Stat Name    Statement Total    Per Execution    % Snap Total
Elapsed Time (ms)     2,686,331           4.69
CPU Time (ms)     2,669,779           24.23
Executions     0            
Buffer Gets     239,883,842           24.78
Disk Reads     0           0.00
Parse Calls     1           0.00
--可以看到此SQL语句目前已经执行了2686秒，Buffer Gets的数据块个数是239,883,842，2亿多个。



进一步查询此SQL对应的会话信息：
SQL> select sid,serial#,machine,program,EVENT,SADDR,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

       SID    SERIAL# MACHINE    PROGRAM              EVENT                                    SADDR            STATUS
---------- ---------- ---------- -------------------- ---------------------------------------- ---------------- ----------
      1050          8 ABCSRV01  JDBC Thin Client     SQL*Net more data from client            000000075B9EE930 ACTIVE


SQL> select START_TIME,STATUS,SES_ADDR from v$transaction where ses_addr='000000075B9EE930';

START_TIME           STATUS     SES_ADDR
-------------------- ---------- ----------------
08/24/15 10:37:21    ACTIVE     000000075B9EE930

可以看到此事务从上午10：37开始运行一直到进行处理时的14：30左右还在运行；
经观察此会话执行的SQL不只上面查出的一条且都运行时间较长，因此判断是同一事务中的多个大型SQL；
因SQL执行速度较慢且在同一事务中，在全部SQL执行完之前事务不提交也不回滚，导致TX行锁资源一直得不到释放。
进而导致其它会话的相关操作都HANG住在等待此会话释放TX锁资源。


5.找到问题原因--TX - row lock contention产生原因并进行处理
经与用户确认，决定KILL此会话；同时联系业务部门确认此会话的相关SQL执行的为何种操作并进行修正。
--KILL会话及后续查询如下：
SQL> col  EVENT for a20
SQL> select sid,serial#,machine,program,EVENT,SADDR,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

       SID    SERIAL# MACHINE    PROGRAM                   EVENT                SADDR            SQL_ID        STATUS
---------- ---------- ---------- ------------------------- -------------------- ---------------- ------------- ----------
      1050          8 ABCSRV01  JDBC Thin Client          latch free           000000075B9EE930 36mu7qg6cc5yu ACTIVE

SQL> alter system kill session '1050,8';

System altered.

SQL> select sid,serial#,machine,program,EVENT,SADDR,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

no rows selected

此时还有部分事务在等待TX锁，稍等后再查询，之前被阻塞的事务全部完成。

SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction);

       SID    SERIAL# MACHINE    PROGRAM                   EVENT                SQL_ID        STATUS
---------- ---------- ---------- ------------------------- -------------------- ------------- ----------
      1056        397 ABCSRV01  JDBC Thin Client          enq: TX - row lock c 2u1bpvx9s2ygp ACTIVE
                                                           ontention
SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS,saddr from v$session where SADDR in(select SES_ADDR from v$transaction);

no rows selected

SQL> select START_TIME,STATUS,SES_ADDR from v$transaction;

no rows selected


注意事项：
大事务回滚时可能会产生大量REDO信息；
同时并行回滚参数设置不当(如过高)也可能导致回滚事务时HANG住，建议使用默认值LOW。
SQL> show parameter fast_start_p

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
fast_start_parallel_rollback         string      LOW
查看一个参数是否默认值及允许的值
set linesize 200
col NAME for a30
 col value for a30
col isdefault for a10
select * from V$PARAMETER_VALID_VALUES where name='fast_start_parallel_rollback';

       NUM NAME                              ORDINAL VALUE                          ISDEFAULT
---------- ------------------------------ ---------- ------------------------------ ----------
       782 fast_start_parallel_rollback            1 FALSE                          FALSE
       782 fast_start_parallel_rollback            2 LOW                            TRUE
       782 fast_start_parallel_rollback            3 HIGH                           FALSE
SQL> select * from V$PARAMETER_VALID_VALUES where name like '%statistics_level%';

       NUM NAME                              ORDINAL VALUE                          ISDEFAULT
---------- ------------------------------ ---------- ------------------------------ ----------
      1182 statistics_level                        1 BASIC                          FALSE
      1182 statistics_level                        2 TYPICAL                        TRUE
      1182 statistics_level                        3 ALL                            FALSE
---------
Mon Aug 24 14:00:52 2015
Thread 1 advanced to log sequence 81771 (LGWR switch)
  Current log# 3 seq# 81771 mem# 0: /oralog/orcl/redo03.log
Mon Aug 24 14:47:49 2015
SMON: Restarting fast_start parallel rollback
Mon Aug 24 14:47:57 2015
ORA-00060: Deadlock detected. More info in file /u01/app/oracle/admin/orcl/udump/orcl_ora_27174.trc.
Mon Aug 24 14:47:57 2015
Thread 1 advanced to log sequence 81772 (LGWR switch)
Mon Aug 24 14:47:57 2015
Thread 1 advanced to log sequence 81772 (LGWR switch)
  Current log# 1 seq# 81772 mem# 0: /oralog/orcl/redo01.log
Mon Aug 24 14:48:09 2015
Thread 1 advanced to log sequence 81773 (LGWR switch)
  Current log# 2 seq# 81773 mem# 0: /oralog/orcl/redo02.log
Thread 1 cannot allocate new log, sequence 81774
Checkpoint not complete
  Current log# 2 seq# 81773 mem# 0: /oralog/orcl/redo02.log
Mon Aug 24 14:48:44 2015
Thread 1 advanced to log sequence 81774 (LGWR switch)
  Current log# 3 seq# 81774 mem# 0: /oralog/orcl/redo03.log
Mon Aug 24 14:48:55 2015
Thread 1 cannot allocate new log, sequence 81775
Checkpoint not complete
  Current log# 3 seq# 81774 mem# 0: /oralog/orcl/redo03.log

版权声明：本文为博主原创文章，未经博主允许不得转载。

【MongoDB】-MongoVUE增删改查使用说明
一、插入
1）右键点击集合名-左键点击InsertDocument 


2）在弹出的对话框里输入Json格式的数据，点击Insert完成插入。 


二、查询
1）选中要查询的集合，点击find； 
 
或点击工具栏中的find； 


2）查询界面包括四个区域 


{Find}区：输入查询条件的地方，查询语句区分大小写； 
格式为：{“sendId”:”000101”}，表示查询sendId=000101的记录； 
查询条件包含and时，格式为：{“sendId”:”000101”,”operParam5”:”vfre”} 
查询条件包含or时，格式为：{$or:[{“sendId”:”000101”},{“sendId”:”1234567890”}]} 
查询条件中包含like时，格式为： 
operParam like ‘%set%’，{“operParam”:new RegExp(“.set.“)}
{Fields}区：设置需要查询的字段的地方，不填默认显示所有字段；Fields区的格式如：{“id”:”“,”sendId”:”“,”appId”:”“}。
{Sort}区：设置排序方式的区域； 
Sort区的格式如：{“id”:1}或{“id”:-1}，1表示按照id升序排序，-1表示按照id降序排序。
Limit用来设置返回多少条记录，Skip用来设置跳过多少条记录。

三、删除
选中要操作的集合，点击Remove进入删除面板，输入要删除数据的查询条件，点击Remove，在弹出的提示框中确认删除即可。

四、修改
选中要操作的集合，点击Update进入修改面板； 


左侧输入查询条件，右侧输入要更新的字段名称和值； 
格式如：｛$set:{“sendId”:”000102”}｝

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

leveldb学习：Memtable和Varint
Memtable：
Memtable是leveldb数据在内存中的存储形式，写操作的数据都会先写到memtable中，memtable的size有限制最大值（write_buffer_size）。memtable的底层数据结构是skiplist，class memtable完成的只是key-value的打包和调用底层跳表的接口。  
首先看看Memtable的成员变量： 

//实例化跳表模板
  typedef SkipList<const char*, KeyComparator> Table;
  //字符大小比较器
  KeyComparator comparator_;
  //memtable的引用计数
  int refs_;
  //内存池
  Arena arena_;
  //底层跳表
  Table table_;
由于memtable只是一个配接器，entry的插入操作交由底层的skiplist接口完成，关于leveldb的skiplist实现请看我写的另一篇博客。memtable真正完成的任务是打包key_value数据。memtable中存储的数据格式如下图：

首先把key、SequenceNumber、valueType打包成InternalKey。注：SequenceNumber是leveldb的不同版本，每次更新（put/delete）操作都会产生新的版本；valueType是区分entry是真实的KV数据还是删除操作，valueType是delete型的数据表示是要删除的数据，leveldb先记录，在后台的compact线程中完成真实的删除工作。存储时，SequenceNumber占56bits，valueType占8bits，两者共同占64bits（uint_64t）。
打包除了有key(InternalKey)和value，还加入了key、value的长度信息，而两者均是自定义的Varint（变长整型）数据，需要完成把int型转化为Varint型。

参见MemTable::Add函数：
void MemTable::Add(SequenceNumber s, ValueType type,
                   const Slice& key,
                   const Slice& value) {
  // Format of an entry is concatenation of:
  //  key_size     : varint32 of internal_key.size()
  //  key bytes    : char[internal_key.size()]
  //  value_size   : varint32 of value.size()
  //  value bytes  : char[value.size()]
  size_t key_size = key.size();
  size_t val_size = value.size();
  //sequencenumber和valuetype另占8字节
  size_t internal_key_size = key_size + 8;
  //插入的entry打包后的长度
  //VarintLength(int)计算int转化为varint所要字节
  const size_t encoded_len =
      VarintLength(internal_key_size) + internal_key_size +
      VarintLength(val_size) + val_size;
  //分配内存
  char* buf = arena_.Allocate(encoded_len);
  //EncodeVarint32()完成varint型转化工作
  char* p = EncodeVarint32(buf, internal_key_size);
  //写入内存
  memcpy(p, key.data(), key_size);
  p += key_size;
  //把sequencenumber和valuetype写入内存
  EncodeFixed64(p, (s << 8) | type);
  p += 8;
  //把valuesize和value写入内存
  p = EncodeVarint32(p, val_size);
  memcpy(p, value.data(), val_size);
  assert((p + val_size) - buf == encoded_len);
  //把char指针插入跳表
  table_.Insert(buf);
}
在bool MemTable::Get( )就是查找操作
bool MemTable::Get(const LookupKey& key, std::string* value, Status* s) {
  //LookupKey是leveldb为了在memtable/sstable查找方便，为key包装的类型
  //调用成员函数memtable_key可以返回在memtable中的key格式
  Slice memkey = key.memtable_key();
  //利用skiplist的专属迭代器查找key
  Table::Iterator iter(&table_);
  iter.Seek(memkey.data());
  if (iter.Valid()) {
    // entry format is:
    //    klength  varint32
    //    userkey  char[klength]
    //    tag      uint64
    //    vlength  varint32
    //    value    char[vlength]
    // Check that it belongs to same user key.  We do not check the
    // sequence number since the Seek() call above should have skipped
    // all entries with overly large sequence numbers.
    const char* entry = iter.key();
    uint32_t key_length;
    //找到了再把memtable中记录的userkey和查找对象做一次检查，要求相等
    const char* key_ptr = GetVarint32Ptr(entry, entry+5, &key_length);
    if (comparator_.comparator.user_comparator()->Compare(
            Slice(key_ptr, key_length - 8),
            key.user_key()) == 0) {
      // Correct user key
      //从memtable中的entry提取valuetype 
      //valuetype = kTypeValue，是真实的数据，查找成功
      //valuetype = kTypeDeletion，是要删除的数据，并非不代表数据真实存在
      const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8);
      switch (static_cast<ValueType>(tag & 0xff)) {
        case kTypeValue: {
          Slice v = GetLengthPrefixedSlice(key_ptr + key_length);
          value->assign(v.data(), v.size());
          return true;
        }
        case kTypeDeletion:
          *s = Status::NotFound(Slice());
          return true;
      }
    }
  }
  return false;
}
GetVarint32Ptr函数就是varint型编码的逆过程，解码。
关于Varint（变长整型）
Varint是一种紧凑的表示数字的方法。Varint中的每个byte的最高位bit有特殊的含义，如果该位为 1，表示后续的byte也是该数字的一部分，如果该位为0，则结束。其他的7个bit都用来表示数字。因此小于128的int都可以用一个byte表示。大于 128的数字，比如300，会用两个字节来表示：1010 1100 0000 0010，而4字节的int型最多需要5字节用varint表示。  
有关varint的编码和解码部分实现都在coding.cc中。
int VarintLength(uint64_t v) {
  int len = 1;
  while (v >= 128) {
    v >>= 7;
    len++;
  }
  return len;
}
编码：
char* EncodeVarint32(char* dst, uint32_t v) {
  // Operate on characters as unsigneds
  unsigned char* ptr = reinterpret_cast<unsigned char*>(dst);
  static const int B = 128;
  if (v < (1<<7)) {
    *(ptr++) = v;
  } else if (v < (1<<14)) {
    *(ptr++) = v | B;
    *(ptr++) = v>>7;
  } else if (v < (1<<21)) {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = v>>14;
  } else if (v < (1<<28)) {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = (v>>14) | B;
    *(ptr++) = v>>21;
  } else {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = (v>>14) | B;
    *(ptr++) = (v>>21) | B;
    *(ptr++) = v>>28;
  }
  return reinterpret_cast<char*>(ptr);
}
解码：
inline const char* GetVarint32Ptr(const char* p,
                                  const char* limit,
                                  uint32_t* value) {
  if (p < limit) {
    uint32_t result = *(reinterpret_cast<const unsigned char*>(p));
    if ((result & 128) == 0) {
      *value = result;
      return p + 1;
    }
  }
  return GetVarint32PtrFallback(p, limit, value);
}


const char* GetVarint32PtrFallback(const char* p,
                                   const char* limit,
                                   uint32_t* value) {
  uint32_t result = 0;
  for (uint32_t shift = 0; shift <= 28 && p < limit; shift += 7) {
    uint32_t byte = *(reinterpret_cast<const unsigned char*>(p));
    p++;
    if (byte & 128) {
      // More bytes are present
      result |= ((byte & 127) << shift);
    } else {
      result |= (byte << shift);
      *value = result;
      return reinterpret_cast<const char*>(p);
    }
  }
  return NULL;
}
没什么可说的，用心体会里面的移位操作。
看完32位int转varint，来看看进阶版64位int的编码和解码，里面采用了循环结构
char* EncodeVarint64(char* dst, uint64_t v) {
  static const int B = 128;
  unsigned char* ptr = reinterpret_cast<unsigned char*>(dst);
  while (v >= B) {
    *(ptr++) = (v & (B-1)) | B;
    v >>= 7;
  }
  *(ptr++) = static_cast<unsigned char>(v);
  return reinterpret_cast<char*>(ptr);
}
const char* GetVarint64Ptr(const char* p, const char* limit, uint64_t* value) {
  uint64_t result = 0;
  for (uint32_t shift = 0; shift <= 63 && p < limit; shift += 7) {
    uint64_t byte = *(reinterpret_cast<const unsigned char*>(p));
    p++;
    if (byte & 128) {
      // More bytes are present
      result |= ((byte & 127) << shift);
    } else {
      result |= (byte << shift);
      *value = result;
      return reinterpret_cast<const char*>(p);
    }
  }
  return NULL;
}

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【linux】ubuntu 14.04下安装mysql 5.5
1.ubuntu下安装mysql需要一些命令
(1)sudo apt-get install mysql-server
安装过程中会出现如下窗口，需要用户输入相关的数据库密码：

（2）sudo apt-get isntall mysql-client
（3）sudo apt-get install libmysqlclient-dev
安装完成之后，可以用下面的命令检查是否安装成功
sudo netstat -tap | grep mysql

通过上述命令检查之后，如果看到有mysql 的socket处于 listen 状态则表示安装成功。
　tcp        0      0 localhost:mysql         *:*                     LISTEN      28385/mysqld  


2.登陆mysql可以输入如下命令：
mysql -u root -p

show databases;
show tables;


3.补充一些安装过程中的小知识
1. 删除mysql


  sudo apt-get autoremove --purge mysql-server-5.0

  sudo apt-get remove mysql-server

  sudo apt-get autoremove mysql-server

  sudo apt-get remove mysql-common (非常重要)

上面的其实有一些是多余的，建议还是按照顺序执行一遍

2. 清理残留数据

dpkg -l |grep ^rc|awk'{print $2}' |sudoxargs
 dpkg -P 

3.ubuntu下启动/停止/重启 mysql
   3.1使用service：

   sudo service mysql stop
   sudo service mysql start
   sudo service mysql restart


   3.2使用 mysqld 脚本启动

    sudo /etc/init.d/mysql stop
    sudo /etc/init.d/mysql start
    sudo /etc/init.d/mysql restart

重启提示错误如下：
Rather than invoking init scripts through /etc/init.d, use the service(8) utility, e.g. service mysql start...
其实已经告诉怎么做了：用sudo service mysql start启动即可












by：Alice















  


版权声明：本文为博主原创文章，未经博主允许不得转载。

SQLite和MySQL数据库的区别与应用
简单来说，SQLITE功能简约，小型化，追求最大磁盘效率；MYSQL功能全面，综合化，追求最大并发效率。如果只是单机上用的，数据量不是很大，需要方便移植或者需要频繁读/写磁盘文件的话，就用SQLite比较合适；如果是要满足多用户同时访问，或者是网站访问量比较大是使用MYSQL比较合适。


下面详细介绍两者的区别和应用：



SQLite

SQLite是非凡的数据库，他可以进程在使用它的应用中。作为一个自包含、基于文件的数据库，SQLite提供了出色的工具集，可以处理所有类型的数据，没有什么限制，而且比起服务器运行的进程型服务器使用起来轻松许多。

一个应用使用SQLite时，它的功能直接被集成在其中，应用会直接访问包含数据的文件(即SQLite数据库),而不是通过一些端口(port, socket)来交互。感谢这种底层技术，这使SQLite变得非常快速和高效，并且十分强大。

SQLite支持的数据类型

NULL:

NULL值。

INTEGER:

有符号整数，按照设置用1、2、3、4、6或8字节存储。

REAL:

浮点数，使用8字节IEEE浮点数方式存储。

TEXT:

文本字符串，使用数据库编码存储(UTF-8, UTF-16BE 或 UTF-16LE)。

BLOB:

二进制大对象，怎么输入就怎么存储。

注: 想了解更多有关SQLite数据类型的信息，可以查看这一主题的 官方文档 。

SQLite 的优点

基于文件:

整个数据库都包含在磁盘上的一个文件中，因此它有很好的迁移性。

标准化:

尽管它看起来像个“简化版”的数据库，SQLite 确实支持 SQL。它略去了一些功能(RIGHT OUTER JOIN 和 FOR EACH STATEMENT)，但是，又同时增加了一些其他功能。

对开发乃至测试都很棒:

在绝大多数应用的开发阶段中，大部分人都非常需要解决方案能有并发的灵活性。SQLite 含有丰富功能基础，所能提供的超乎开发所需，并且简洁到只需一个文件和一个 C 链接库。

SQLite的缺点

没有用户管理:

高级数据库都能支持用户系统，例如，能管理数据库连接对数据库和表的访问权限。但由于 SQLite 产生的目的和本身性质(没有多用户并发的高层设计)，它没有这个功能。

缺乏额外优化性能的灵活性：

仍然是从设计之初，SQLite 就不支持使用各种技巧来进行额外的性能优化。这个库容易配置，容易使用。既然它并不复杂，理论上就无法让它比现在更快，其实现在它已经很快了。

何时使用 SQLite ?

嵌入式应用:

所有需要迁移性，不需要扩展的应用，例如，单用户的本地应用，移动应用和游戏。

代替磁盘访问:

在很多情况下，需要频繁直接读/写磁盘文件的应用，都很适合转为使用 SQLite ，可以得益于 SQLite 使用 SQL 带来的功能性和简洁性。

测试:

它能秒杀大部分专门针对应用业务逻辑(也就是应用的主要目的：能完成功能)的测试。

何时不用 SQLite ?

多用户应用:

如果你在开发的应用需要被多用户访问，而且这些用户都用同一个数据库，那么相比 SQLite 最好还是选择一个功能完整的关系型数据库(例如 MySQL)。

需要大面积写入数据的应用:

SQLite 的缺陷之一是它的写入操作。这个数据库同一时间只允许一个写操作，因此吞吐量有限。




MySQL

MySQL 在所有大型数据库服务器中最流行的一个. 它的特性丰富，产品的开源性质使得其驱动了线上大量的网站和应用程序. 要入手 MySQL 相对简单，开发人员可以在互联网上面访问到大量有关这个数据库的信息.

注意: 由于这个产品的普及性，大量的第三方应用、工具和集成库对于操作这个RDBCMS的方方面面大有帮助.

Mysql没有尝试去实现SQL标准的全部，而是为用户提供了很多有用的功能. 作为一个独立的数据库服务器，应用程序同Mysql守护进程的交互，告诉它去访问数据库自身 -- 这一点不像 SQLite.

MySQL支持的数据类型

TINYINT:

一个非常小的整数.

SMALLINT:

一个小整数.

MEDIUMINT:

一个中间大小的整数.

INT or INTEGER:

一个正常大小的整数.

BIGINT:

一个大的整数.

FLOAT:

一个小的 (单精度) 浮点数，不能是无符号的那种.

DOUBLE, DOUBLE PRECISION, REAL:

一个正常大小 (双精度) 的浮点数，不能使无符号的那种.

DECIMAL, NUMERIC:

没有被包装的浮点数。不能使无符号的那种.

DATE:

一个日期.

DATETIME:

一个日期和时间的组合.

TIMESTAMP:

一个时间戳.

TIME:

一个时间.

YEAR:

一个用两位或者4位数字格式表示的年份(默认是4位).

CHAR:

一个固定长度的字符串，存储时总是在其固定长度的空间里右对齐.

VARCHAR:

一个可变长度的字符串.

TINYBLOB, TINYTEXT:

一个BLOB或者TEXT列，最大长度255 (2^8 - 1)个字符.

BLOB, TEXT:

一个BLOB或者TEXT列，最大长度 65535 (2^16 - 1)个字符.

MEDIUMBLOB, MEDIUMTEXT:

一个BLOB或者TEXT列，最大长度 16777215 (2^24 - 1)个字符.

LONGBLOB, LONGTEXT:

一个BLOB或者TEXT列，最大长度4294967295 (2^32 - 1) 个字符.

ENUM:

一个枚举类型.

SET:

一个集合.

MySQL的优点

容易使用:

安装MySQL非常容易。第三方库，包括可视化(也就是有GUI)的库让上手使用数据库非常简单。

功能丰富:

MySQL 支持大部分关系型数据库应该有的 SQL 功能——有些直接支持，有些间接支持。

安全:

MYSQL 有很多安全特性，其中有些相当高级。

灵活而强大:

MySQL 能处理很多数据，此外如有需要，它还能“适应”各种规模的数据。

快速:

放弃支持某些标准，让 MySQL 效率更高并能使用捷径，因此带来速度的提升。

MySQL的缺点

已知的局限:

从设计之初，MySQL 就没打算做到全知全能，因此它有一些功能局限，无法满足某些顶尖水平应用的需求。

可靠性问题:

MySQL 对于某些功能的实现方式(例如，引用，事务，数据审核等) 使得它比其他一些关系型数据库略少了一些可靠性。

开发停滞:

尽管 MySQL 理论上仍是开源产品，也有人抱怨它诞生之后更新缓慢。然而，应该注意到有一些基于 MySQL 并完整集成的数据库(如 MariaDB)，在标准的 MySQL 基础上带来了额外价值。

何时使用 MySQL?

分布式操作:

当SQLite所提供的不能满足你的需要时，可以把MySQL包括进你的部署栈，就像任何一个独立的数据库服务器，它会带来大量的操作自由性和一些先进的功能。

高安全性:

MySQL的安全功能，用一种简单的方式为数据访问(和使用)提供了可靠的保护。

Web网站 和 Web应用:

绝大多数的网站(和Web应用程序)可以忽视约束性地简单工作在MySQL上。这种灵活的和可扩展的工具是易于使用和易于管理的——这被证明非常有助于长期运行。

定制解决方案:

如果你工作在一个高度量身定制的解决方案上，MySQL能够很容易地尾随和执行你的规则，这要感谢其丰富的配置设置和操作模式。

何时不用 MySQL?

SQL 服从性:

因为 MySQL 没有[想要]实现 SQL 的全部标准，所以这个工具不完全符合SQL。如果你需要对这样的关系数据库管理系统进行整合，从MySQL进行切换是不容易的。

并发:

即使MySQL和一些存储引擎能够真地很好执行读取操作，但并发读写还是有问题的。

缺乏特色:

再次提及，根据数据库引擎的选择标准，MySQL会缺乏一定的特性，如全文搜索。



版权声明：本文为博主原创文章，未经博主允许不得转载。

Hadoop之Reduce侧的联结
理解其就像关系型数据库中的链接查询一样,数据很多的时候,几个数据文件的数据能够彼此有联系,可以使用Reduce联结。举个很简单的例子来说,一个只存放了顾客信息Customer.txt文件,和一个顾客相关联的Order.txt文件,要进行两个文件的信息组合,原理图如下: 
 
 
这里涉及的几个专业术语:Group key ，datasourde,Tag.前者的话通俗点来说的话就相当于关系型数据库中的主键和外键,通过其id进行的联结依据。datasource，顾名思义,就是数据的来源,那么这里指的就是Custonmers和Orders,Tag的话也比较好理解,就是里面的字段到底是属于哪个文件的。 
操作Reduce的侧联结,要用到hadoop-datajoin-2.6.0.jar包,默认路径: 
E:\hadoop-2.6.0\share\hadoop\tools\lib(hadoop的工作目录)。 
用到的3个类： 
1、DataJoinMapperBase 
2、DataJoinReducerBase 
3、TaggedMapOutput 
比较正式的工作原理: 
 1、mapper端输入后，将数据封装成TaggedMapOutput类型，此类型封装数据源(tag)和值(value)； 
2、map阶段输出的结果不在是简单的一条数据，而是一条记录。记录=数据源(tag)+数据值(value). 
3、combine接收的是一个组合：不同数据源却有相同组键的值； 
4、不同数据源的每一条记录只能在一个combine中出现； 
好,了解了这些我们就进行编码阶段: 
这里的话将几个类写在一起测试，感觉另有一番感觉： 
联结之前的Custmoner.txt文件: 

联结之前的Order.txt文件: 
 
测试代码:
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;
import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;
import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.chain.ChainMapper;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class DataJoin extends Configuration{
    //DataJoinMapperBase默认没导入,路径E:\hadoop-2.6.0\share\hadoop\tools\lib
    public static class MapClass extends DataJoinMapperBase{
        // 设置组键
        @Override
        protected Text generateGroupKey(TaggedMapOutput aRecord) {
            String line=((Text)aRecord.getData()).toString();
            String [] tokens=line.split(",");
            String groupkey=tokens[0];
            return new Text(groupkey);
        }
        /* 
         * 这个在任务开始时调用，用于产生标签
               此处就直接以文件名作为标签
         */
        @Override
        protected Text generateInputTag(String inputFile) {
            return new Text(inputFile);
        }
        // 返回一个任何带任何我们想要的Text标签的TaggedWritable
        @Override
        protected TaggedMapOutput generateTaggedMapOutput(Object value) {
            TaggedWritable retv=new TaggedWritable((Text) value);
            retv.setTag(this.inputTag);
            return retv;
        }

    }

    public static class Reduce extends DataJoinReducerBase{
        // 两个参数数组大小一定相同，并且最多等于数据源个数
        @Override
        protected TaggedMapOutput combine(Object[] tags, Object[] values) {
            if(tags.length<2){
                return null;// 这一步，实现内联结
            }
            String joinedStr="";
            for(int i=0;i<values.length;i++){
                if(i>0){
                    joinedStr+=",";// 以逗号作为原两个数据源记录链接的分割符
                    TaggedWritable tw=(TaggedWritable)values[i];
                    String line=((Text)tw.getData()).toString();
                    String[] tokens=line.split(",",2);// 将一条记录划分两组，去掉第一组的组键名。
                    joinedStr+=tokens[1];
                }
            }
            TaggedWritable retv=new TaggedWritable(new Text(joinedStr));
            retv.setTag((Text)tags[0]);
            return retv;// 这只retv的组键，作为最终输出键。
        }
    }

    /*TaggedMapOutput是一个抽象数据类型，封装了标签与记录内容
     此处作为DataJoinMapperBase的输出值类型，需要实现Writable接口，所以要实现两个序列化方法
     自定义输入类型*/
    public static class TaggedWritable extends TaggedMapOutput{
        private Writable data;
        //如果不给其一个默认的构造方法,Hadoop的使用反射来创建这个对象，需要一个默认的构造函数（无参数）
        public TaggedWritable(){
        }
        public TaggedWritable(Writable data){
            //TODO 这里可以通过setTag()方法进行设置
            this.tag=new Text("");
            this.data=data;
        }

        @Override
        public void readFields(DataInput in) throws IOException {
            this.tag.readFields(in);
            //加入以下的代码.避免出现空指针异常,当时一定要在其写的时候加入out.writeUTF(this.data.getClass().getName());
            //不然会出现readFully错误
            String temp=in.readUTF();
            if(this.data==null||!this.data.getClass().getName().equals(temp)){
                try {
                    this.data=(Writable)ReflectionUtils.newInstance(Class.forName(temp), null);
                } catch (ClassNotFoundException e) {
                    e.printStackTrace();
                }
            }
            this.data.readFields(in);
        }

        @Override
        public void write(DataOutput out) throws IOException {
            this.tag.write(out);
            out.writeUTF(this.data.getClass().getName());
            this.data.write(out);
        }

        @Override
        public Writable getData() {
            return data;
        }
    }


    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration(); //组件配置是由Hadoop的Configuration的一个实例实现
        JobConf job = new JobConf(conf, DataJoin.class); 
        Path in=new Path("hdfs://master:9000/user/input/yfl/*.txt");
        Path out=new Path("hdfs://master:9000/user/output/testfeng1");
        FileSystem fs=FileSystem.get(conf);
        //通过其命令来删除输出目录
        if(fs.exists(out)){
            fs.delete(out,true);
        }
        //TODO这里注意别导错包了
        FileInputFormat.setInputPaths(job, in);
        FileOutputFormat.setOutputPath(job, out);
        job.setJobName("DataJoin");
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);
        job.setInputFormat(TextInputFormat.class);
        job.setOutputFormat(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(TaggedWritable.class);
        job.set("mapred.textoutputformat.separator", ",");
        JobClient.runJob(job);

    }
}

运行的结果：
为了让调试更加的方便,在程序中直接使用delete命令已达到删除输出目录的功能,省去每次都要手动删除的麻烦,这里需要在我们的工程目录下面的bin目录下面添加主机的core-site.xml和hdfs-site.xml文件,然后给对于的目录赋上权限chmod -R 777 xxx，即可。 
 
hadoop很有意思,我希望自己能走的更远！！！坚持,加油！！！

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Spring Data MongoDB实战(上)
Spring Data MongoDB实战(上)
作者：chszs，版权所有，未经同意，不得转载。博主主页：http://blog.csdn.net/chszs
本文会详细展示Spring Data MongoDB是如何访问MongoDB数据库的。MongoDB是一个开源的文档型NoSQL数据库，而Spring Data MongoDB是Spring Data的模块之一，专用于访问MongoDB数据库。Spring Data MongoDB模块既提供了基于方法名的查询方式，也提供了基于注释的查询方式。
1、用Spring Data配置并管理MongoDB
要安装MongoDB数据库，可以从这里下载：https://www.mongodb.org/downloads
安装过程省略。完成MongoDB的安装和运行后，可以开始应用开发了。
首先在Eclipse创建一个简单的Maven项目，并配置pom.xml管理Spring Data MongoDB项目的依赖。内容如下：

pom.xml
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>SpringDataMongoDBDemo</groupId>
    <artifactId>SpringDataMongoDBDemo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>org.springframework.data</groupId>
            <artifactId>spring-data-mongodb</artifactId>
            <version>1.7.2.RELEASE</version>
        </dependency>
    </dependencies>
</project>

Eclipse会下载所需的JAR包并把依赖关系配置到项目的类路径下。现在项目的依赖关系已经完成导入，可以开始编写实际的代码了。
首先创建需要持久化到MongoDB数据库的实体类。

Person.java
package com.ch.jpa.entity;

import java.util.ArrayList;
import java.util.List;
import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.PersistenceConstructor;
import org.springframework.data.mongodb.core.mapping.DBRef;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "person")
public class Person {

    @Id
    private Long personId;

    private String name;

    private int age;

    @DBRef(db = "address")
    private List<Address> addresses = new ArrayList<>();

    public Person() {
    }

    @PersistenceConstructor
    public Person(Long personId, String name, int age) {
        super();
        this.personId = personId;
        this.name = name;
        this.age = age;
    }

    public Long getPersonId() {
        return personId;
    }

    public void setPersonId(Long personId) {
        this.personId = personId;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public List<Address> getAddresses() {
        return addresses;
    }

    public void setAddresses(List<Address> addresses) {
        this.addresses = addresses;
    }

    @Override
    public String toString() {
        return "Person [personId=" + personId + ", name=" + name + ", age=" + age + ", addresses=" + addresses + "]";
    }
}

注释@Document表示待持久化的数据是一个集合。如果集合没有指定名字，那么默认会使用实体类的类名作为集合名。
注释@Id表示被注解的域被映射到集合中的_id列。如果实体类中未使用此注释，那么默认名为id的域会被映射到集合中的_id列。而且此域的值由MongoDB的驱动包自动产生，它的值在在POJO中是不可用的。
注释@DBRef用于在当前的实体类中引用已有的实体类。然而，与关系数据库的情况不同，如果我们保存当前实体，它不会保存引用的相关实体。引用的相关实体的持久化是分开的。
注释@PersistenceConstructor用于标记从MongoDB数据库服务器取回数据时创建实体的构造方法。
下面是关联的Address实体类：
Address.java
package com.ch.jpa.entity;

import org.springframework.data.annotation.Id;
import org.springframework.data.annotation.PersistenceConstructor;
import org.springframework.data.mongodb.core.mapping.Document;

@Document(collection = "address")
public class Address {

    @Id
    private long addressId;

    private String address;

    private String city;

    private String state;

    private long zipcode;

    public Address() {

        System.out.println("CAlling default cons");
    }

    @PersistenceConstructor
    public Address(long addressId, String address, String city, String state, long zipcode) {
        super();
        this.addressId = addressId;
        this.address = address;
        this.city = city;
        this.state = state;
        this.zipcode = zipcode;
    }

    public String getAddress() {
        return address;
    }

    public void setAddress(String address) {
        this.address = address;
    }

    public String getCity() {
        return city;
    }

    public void setCity(String city) {
        this.city = city;
    }

    public String getState() {
        return state;
    }

    public void setState(String state) {
        this.state = state;
    }

    public long getZipcode() {
        return zipcode;
    }

    public void setZipcode(long zipcode) {
        this.zipcode = zipcode;
    }

    @Override
    public String toString() {
        return "Address [address=" + address + ", city=" + city + ", state=" + state + ", zipcode=" + zipcode + "]";
    }
}


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主chszs的原创文章，未经博主允许不得转载。

项目组数据库脚本的维护方案

背景

版本发布密集，为满足客户和领导的要求，项目经理决定每周必须发布并上线一个版本。功能修改频繁，相应的表结构、表数据的变动也比较频繁。产品满足两种部署方案，一是总部部署，由项目组直接来维护，供内部客户使用；二是客户自行安装、升级和维护，项目组提供版本和技术支持。项目选型使用MySQL作为数据存储软件。

方案

针对现状，设计出数据库脚本的维护方案，脚本分为全量脚本和升级脚本两套。
全量脚本，包括

表结构定义，包括表结构定义、列的索引定义。初始化数据，包括系统正常运行时需要的初始化数据。存储过程定义，包括系统运行时使用到的存储过程的集合。函数定义，包括系统运行时使用到的、自定义的MySQL

升级脚本，相比于全量脚本，升级脚本的构成相对简单一些，以天为单位来维护脚本，比如2015年8月10日脚本有变化，那么就创建一个脚本文件，名为2015-08-10.sql，这个文件里保存对原有数据库表对象进行增量修改的语句。

方案的问题

由于团队成员中以刚毕业、工作不满一年的新员工为主，前述方案在实际使用时遇到几个问题：

全量脚本和升级脚本中，经常会出现不一致。比如升级脚本中增加了字段，但全量脚本中没有增加；全量脚本中增加字段时出现了重复增加，导致建失败，等等。数据库脚本中存在语法错误，比如语句末尾的“;”经常忘记增加，导致同一文件中后面的脚本执行失败。数据库脚本中存在乱码，比如书写脚本时经常忘记切换输入法，导致分号、逗号等是中文符号，执行时失败。数据库脚本缺少注释，尤其是升级脚本中缺少注释，更别提场景描述，测试人员验证升级脚本时需要大量时间来确认脚本使用的场景。升级脚本的场景存在设计遗漏，测试经常在版本临上线前突然发现脚本未能覆盖全部场景，此时往往需要安排脚本的开发人员及骨干开发放下手上的工作，临时救场。。。。

针对遇到的问题，项目组在例会上时安排骨干开发人员来讲解脚本的作用及开发说明，但收效不明显。脚本中依然不断出现各式的问题，导致日常特性测试、数据库对象升级测试、性能测试过程中，测试人员和骨干开发人员花费大量时间来排查此类错误，极大的降低了团队的效率，相应的团队自身也很疲惫。

解决方法

经过分析，前述问题主要分为几类：

注释不足，比如注释不全，缺少场景说明等；脚本维护中的低级错误，比如遗漏、语法错误、非法字符等；场景设计遗漏，场景考虑不全面，导致脚本实现不合理，不满足业务需求。

解决方案如下：

注释不足。项目组周例会上宣讲脚本的注释要求，并指定专人负责检查脚本的规范符合度，不合要求的脚本直接要求提交人修改，同时作为关键事件通报批评；脚本维护中的低级错误。由于脚本比较多，代码量比较大，靠人去对比不太现实，因此利用MySQL备份工具mysqldump的能力，开发脚本对比工具来完成数据库表定义差异的对比，简化了人的操作，降低了脚本检查的工作量，人只需要查看报告即可以找到脚本中存在的问题。项目组指定专人负责阅读工具输出的报告，当发现脚本存在低级错误时，则要求提交人修改，同时通报批评；场景设计遗漏。修改当前的Story开发流程，增加专门章节，要求开发人员务必分析当前待开发特性在生产环境上线时的数据库对象升级策略；Story评审时，本章节作为必须评审的主题，如果开发人员未准备或者准备不足，测试人员有权要求Story重新评审，项目经理将此事件作为关键事件记录，迭代总结或者项目总结时将有专门议题要求开发人员作出解释。

脚本对比工具的工作流程比较简单，如下：

导出生产环境的数据库表结构定义，R.sql。本地的数据库。创建本地数据库DB_upgrade。打开DB_upgrade，导入R.sql，同时导入本周内，从周一到周六的升级脚本，如无则直接跳过。导出DB_upgrade的表结构定义，upgrade.sql。创建本地数据库DB_install。打开DB_install，导入全量数据库脚本，包括表定义、索引定义。导出DB_install的表结构定义，install.sql。使用工具对比upgrade.sql和install.sql。

工具开发过程中应用到了mysqldump、mysql、msys、jrunscript。

mysqldump，MySQL数据库自带的备份工具，通过指定选项可以只导出表定义。mysql，MySQL数据库自带的客户端软件，用于执行脚本。msys，有了它就可以在windows环境直接运行shell脚本。jrunscript，JDK1.6版本起自带的js执行器，1.8版本还可以执行其它类型的脚本。shell虽然强大，但仍然有些工作不太方便完成，此时即可编写js代码来访问JDK中的API完成那部分操作，同时不需要引入更多的jar，使用时相当方便。


欢迎访问Jackie的家，http://jackieathome.sinaapp.com/，如需转载文章，请注明出处。


版权声明：本文为博主原创文章，未经博主允许不得转载。

MongoDB(1)--简介以及安装
    前段时间接触了NoSql类型的数据库redis，当时是作为缓存服务器使用的。那么从这篇博客开始学习另一个很出名的NoSql数据库：MongoDb。不过目前还没有在开发当中使用，一步一步来吧。

简介
    MongoDB是一个开源的，基于分布式的，面向文档存储的非关系型数据库。是非关系型数据库当中功能最丰富、最像关系数据库的。
    MongoDB由C++编写，其名字来源于"humongous"这个单词，其宗旨在于处理大量数据。
    MongoDB可以运行在Windows、unix、OSX、Solaris系统上，支持32位和64位应用，提供多种编程语言的驱动程序。
    MongoDB支持的数据结构非常松散，是类似json的BSON格式，通过键值对的形式存储数据，可以存储复杂的数据类型。
    MongoDB支持的数据类型有：null、boolean、String、objectId、32位整数、64位整数、64位浮点数、日期、正则表达式、js代码、二进制数据、数组、内嵌文档、最大值、最小值、未定义类型。
    其中，内嵌文档我理解的并不是.doc.txt等文件，这里所指的文档是mongoDB的一个存储单元(相当于关系型数据当中的记录)，在mongoDB中的表现形式为{key1:value1，key2：value2}，而内嵌文档则是这样的形式{key1:value1,key2:{key2.1:value2.1,key2.2:value2.2}}。
    MongoDB最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。

windows下的安装
    安装
    下载路径：https://www.mongodb.org/downloads
    解压之后安装这没什么好说的，因为连安装路径都不用你选的。我还注意，它就安装完了。我连路径都没找着，还是上网查的。默认安装在了C:\Program Files\MongoDB下。

    启动
     创建数据库目录E:\mongodb，接下来打开命令行窗口：
       



    使用配置文件启动
    官方下载的安装包里面没有默认的配置文件，若想使用配置文件自己动手来吧，使用配置文件怎么着也比敲命令来的高级些吧？使用命令的都是大师！使用配置文件配置数据库文件、日志文件以及其它的一些配置如下：




建立数据库目录 E:\mongodb\data建立日志目录 E:\mongodb\log建立配置文件 E:\mongodb\conf建立.conf配置文件，配置文件内容如下：

dbpath=E:\mongodb\data #数据库路径
logpath=E:\mongodb\log\mongodb.log #日志输出文件路径
logappend=true #错误日志采用追加模式，配置这个选项后mongodb的日志会追加到现有的日志文件，而不是从新创建一个新文件
journal=true #启用日志文件，默认启用
quiet=true #这个选项可以过滤掉一些无用的日志信息，若需要调试使用请设置为false
port=27017 #端口号 默认为27017

    普通启动



    访问：http://localhost:27017/可以看到显示信息如下，就表明启动成功了。
    It looks like you are trying to access MongoDB over HTTP on the native driver port.
    MongoDB安装为Windows服务
    将mongodb安装为windows服务非常简单只需要在上面执行的命令行后添加 --install即可



小结：安装和简介就到此结束了，基本上安装这个部分和redis没有差别。感觉nosql的都相对轻便灵活一些。




版权声明：本文为博主原创文章，未经博主允许不得转载。

ORACLE-016：ora-01720 授权选项对于'xxxx'不存在
报错的情形如下，A用户：视图V_AB用户：视图V_B，并且用到了V_AC用户：需要用V_B，授权过程，A用户下：grant select on V_A to B
B用户下：grant select on V_B to C此时报错：ora-01720 授权选项对于'V_A'不存在。那么是什么原因呢，因为B还需要授权视图给C用户，但是B用到的视图是A下的，所以除了将V_A授权select权限给B外，还要授权操作权限。比如这里就需要在A用户下，授权grant操作权限给B，那么B才能继续授权给C。如下：A用户下：grant select on V_A to B with grant option
B用户下：grant select on V_B to C此时正确。C中能正常使用V_B了。同样如果C还要继续授权则B用户下也要依此进行授权。 

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL Server 6.0完全卸载以及卸载中遇到的问题
第一次卸载Mysql的时候一直没卸载干净，当时也没当回事，直接重装系统解决的。直到今天又遇到了这个问题，悔不当初啊，上网查了各种资料，用了半天时间才解决，到最后发现其实很简单，所以现在就写一篇文章留待下次卸载的时候使用。
1.在服务里把MySQL服务停掉

2.控制面板卸载MySQL

3.到Mysql的安装目录里删掉整个文件夹

4.在C盘删掉MySQL的数据文件，我的是在ProgramData里，这个文件夹是隐藏的，可以在文件夹选项里设置显示隐藏文件。如果删不掉，可以用文件粉碎等软件删掉。

5.到注册表里删掉MySQL的注册文件，当时就是这里没删干净，建议大家仔细找一找，注册表进入方法：开始->运行regedit

打开HKEY_LOCAL_MACHINE->SYSTEM->ControlSet001->services查看目录下是否有与Mysql有关的文件夹，如果有删掉。再进入services->eventlog->Application把与MySQL有关的文件夹删掉。 
同理，在ControlSet002，CurrentControlSet文件夹里重复上述操作。

6.关机重启，查看服务里是否还有Mysql服务，如果有，就先停止服务，然后去cmd执行sc delete mysql ，最好以管理员身份运行cmd，成功后显示[SC] DeleteService SUCCESS 。Mysql最后配置的时候在第三步出现的错误cannot create windows service for mysql.error:0也是同样的解决方法。 
第四步出现错误可能就是上面的步骤没删干净。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-基本概念和术语
数据：对客观事物的符号表示，在计算机学科中指所有能输入到计算机中并被计算机程序处理的符号的总称。 
数据元素：数据的基本单位，在计算机程序中通常作为一个整体进行考虑和处理。 
数据项：数据的不可分割的最小单位。一个数据元素可由若干个数据项组成。 
数据对象：性质相同的数据元素的集合，是数据的子集。 
数据结构：相互之间存在一种或多种特定关系的数据元素的集合。

数据元素都不是孤立存在的，在他们之间存在着某种关系，这种数据元素相互之间的关系称为结构。 
根据数据元素之间关系的不同特点，通常分以下四种基本结构： 
集合：数据元素之间除“同属于一个集合”外，没有其他关系； 
线性结构：数据元素之间存在一对一的关系； 
树形结构：数据元素之间存在一对多的关系； 
网状结构（图状结构）：数据元素之间存在多对多的关系。

上述四种结构描述的是数据元素之间的逻辑关系，因此又称为数据的逻辑结构 
数据结构在计算机中的表示（映像）称为数据的物理结构（存储结构） 
数据元素之间的关系在计算机中有两种不同的表示方式：顺序表示和非顺序表示，由此得到两种不同的存储结构：顺序存储结构和链式存储结构。

算法：对特定问题求解步骤的一种描述，是指令的有限序列。 
算法具有5个特性： 
有穷性：一个算法必须在执行有穷步之后结束，且每一步都可以在有穷时间内完成。 
确定性：算法中每一条指令必须有确切的含义，读者理解是不会产生二义性。 
可行性：算法中描述的操作都可以通过已经实现的基本操作执行有限次来实现。 
输入：一个算法有零个或多个输入。 
输出：一个算法有一个或多个输出。

算法的时间量度（时间复杂度）记作T（n）=O(f(n)) 
它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同。 
算法所需存储空间的量度（空间复杂度）记作S(n)=O(f(n))，其中n表示问题的规模。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

JDBC ORACLE 事物处理

数据库事物：
在数据库中,所谓事务是指一组逻辑操作单元,使数据从一种状态变换到另一种状态。
为确保数据库中数据的一致性,数据的操纵应当是离散的成组的逻辑单元:当它全部
完成时,数据的一致性可以保持,而当这个单元中的一部分操作失败,整个事务应全部视
为错误,所有从起始点以后的操作应全部回退到开始状态。 
事务的操作:先定义开始一个事务,然后对数据作修改操作,这时如果提交(COMMIT),这些修改
就永久地保存下来,如果回退(ROLLBACK),数据库管理系统将放弃所作的所有修
改而回到开始事务时的状态。


事务的ACID(acid)属性
1. 原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，
要么都不发生。
2. 一致性（Consistency）事务必须使数据库从一个一致性状态变换到另外一个一致性状态。
3. 隔离性（Isolation）事务的隔离性是指一个事务的执行不能被其他事务干扰，即一个事务
内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个
事务之间不能互相干扰。
4. 持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，
  接下来的其他操作和数据库故障不应该对其有任何影响
  
事务：指构成单个逻辑工作单元的操作集合
事务处理：保证所有事务都作为一个工作单元来执行，即使出现了故障，都不能改变这种执行方式。
当在一个事务中执行多个操作时，要么所有的事务都被提交(commit)，要么整个事务回滚(rollback)到最初状态
        当一个连接对象被创建时，默认情况下是自动提交事务：每次执行一个 SQL 语句时，如果执行成功，就会向数据
库自动提交，而不能回滚
为了让多个 SQL 语句作为一个事务执行：
调用 Connection 对象的 setAutoCommit(false); 以取消自动提交事务
在所有的 SQL 语句都成功执行后，调用 commit(); 方法提交事务
在出现异常时，调用 rollback(); 方法回滚事务
若此时 Connection 没有被关闭, 则需要恢复其自动提交状态
常用的代码结构
    public void test(String sql, Object... args)
    {
        
        Connection conn = null;
        PreparedStatement prepareStatement = null;
        
        try
        {
            conn = getConn();
            // 事物处理前，取消Connection的默认提交行为
            conn.setAutoCommit(false);
            prepareStatement = conn.prepareStatement(sql);
            
            for (int i = 0; i < args.length; i++)
            {
                prepareStatement.setObject(i + 1, args[i]);
            }
            prepareStatement.executeUpdate();
            
            // 事物处理：如果事物处理成功则提交事物
            conn.commit();
        }
        catch (Exception e)
        {
            e.printStackTrace();
            try
            {
                // 事物处理：如果出现异常 则在catch块中回滚事物
                conn.rollback();
            }
            catch (SQLException e1)
            {
                e1.printStackTrace();
            }
        }
        finally
        { // 释放数据库资源
            releaseSource(prepareStatement, conn, null);
        }
        
    }

版权声明：本文为博主原创文章，未经博主允许不得转载。

为什么你应该永远不要再使用MongoDB


Sven Slootweg (joepie91)是一名黑客，同时也是CrytoCC的创建者，现在提供Node.js代码评审服务。近日，他在个人博客上发表了一篇博文《为什么你应该永远、永远、永远不要再使用MongoDB》。在文中，他列举了如下理由：

丢失数据（见1、2）；默认忽略错误，假设每次写入都是成功的，在32位系统上，这可能会导致数据无声无息地丢失；即使是在MongoDB宣传的适用场景下，其性能依然不高（见3、4）；几乎在所有的应用场景下，开发者都会被迫养成使用隐式模式的坏习惯（见4）；存在锁问题（见4）；对安全问题响应很慢（见5）；不符合ACID（见6）；扩展和维护困难；JSON存储也不是MongoDB独有的功能，PostgreSQL、CouchDB也支持（见7、8)。
joepie91认为，MongoDB不仅存在诸多问题，而且并无突出之处。如果项目涉及用户账户或者两条记录之间存在某种关系，那么就应该使用关系型数据库，而不是文档存储；如果项目在使用Mongoose，那么也应该使用关系型数据库，因为Mongoose只是使用文档存储模拟了有模式的关系型数据库。因此，大多数情况实际上需要的都是一个关系型数据库。在这些情况下，PostgreSQL是个不错的可选方案。开发者可以使用查询构建器或ORM来简化使用过程，比如，在Node.js中，可以选用Knex、Bookshelf、Sequelize或Waterline。即使真得需要一个文档存储，那么也有比MongoDB更好的选项。另外，他也不认为MongoDB适合于创建原型，因为如果生产环境使用不同的数据库，则还需要重写所有的代码。总之，MongoDB并没有什么适用场景。它在技术上比不上其它可选方案，并没有提供真正有用的独有的特性，而且开发人员也无法确保数据一致性和安全。最后，joepie91指出，流行度并不等同于质量，只能说明产品有一个不错的市场团队：

永远不要因为“其他人那样做”就使用一个数据库，对于一个特定的数据库，要自己研究它的优点和不足。

joepie91的观点在Hack News上得到了广泛的赞同。网友karmakaze也认为，有了PostgreSQL 9.4，就没有任何理由要使用MongoDB了（JSONB比BSON更合用），另外还可以使用CouchDB。对于MongoDB的具体限制，网友giaour建议阅读aphyr的系列文章Call Me Maybe，并指出，虽然存在已知的变通方案，但那大大降低了MongoDB的开发体验。网友Animats认为，如果站点的流量比维基小，那么使用某种关系型数据库就可以了。网友PebblesHD有类似的观点：

作为一个规模较小的部署……，只安装一个基本的MySQL有什么问题吗？在我们的内部维基上，我们每天的访问量已经超过了2万次……

但是也有一些不同的声音。例如，网友threeseed就表示，MongoDB仍然是最容易安装和使用的数据库之一。对此，joepie91回复如下：

以错误的方式做事，想不容易都难——MongoDB恰好就是那么做的。它不需要设置身份验证或表模式，因此才看上去“易于安装”。但实际上，为了节省10分钟，你正在浪费几个小时的时间。因为稍后，你将会遇到入侵（没有身份验证）或数据破坏……

Shodan的报道也佐证了joepie91的这一说法，互联网上有将近3万个MongoDB实例没有启用任何的身份验证。这个问题随处可见，而且已经存在多年。
网友toyg则评论说：

我最近首次使用了MongoDB，是在一个内部项目里。我认为，没有模式确实显著了提升了开发速度……现在项目已经成熟，回过头来，我可以看到为什么关系型数据库会更合适，但如果我从开始就使用RDBMS，那么我可能无法这么快地完成迁移。虽然切换到真正的RDBMS意味着要修改三两个类，但变化不大。所以，我不同意MongoDB不适合原型开发的说法。

joepie91对“修改三两个类，但变化不大”的说法提出了质疑，因为根据自己从事代码审查的经验，迁移到不同的数据库通常需要大量的工作。至于切换速度，joepie91指出，在一个有回滚机制的系统中，可能会更快。
然而，在有些情况下，开发者并没有其它选择。例如，有网友就提出，Meteor就使用而且只能使用MongoDB。而由于同Hadoop的合作伙伴关系，MongoDB同Hadoop有很好的集成，因此，它在大数据分析领域非常流行。
另外，来自SourceGear的软件开发人员Eric Sink在读过的joepie91文章之后表示：

（他所列举的内容）部分（也许全部）确有其事。事实上，现在，就假设他所写的都是正确的。我这里不是要说作者是错的。更确切地说，我这里想指出的是，这种博文只能让我了解很少有关MongoDB的知识，但却让我感受到了写这篇博文的人的许多情感。

他觉得，不能因为那些问题就彻底地否定MongoDB，毕竟：

MongoDB是顶级的NoSQL供应商。每天，成千上万的企业用它为数以百万计的用户提供服务。像所有有大量用户的新生软件一样，它有漏洞和缺陷。但它正稳步改善。任何有关技术缺陷的讨论，如果无助于解决问题，那么很大程度上只能是一种情绪的宣泄。



本文最初发表在infoq，文章内容属作者个人观点，不代表本站立场。


版权声明：本文为博主原创文章，未经博主允许不得转载。

ORACLE-017：SQL优化-is not null和nvl
今天在优化一段sql，原脚本大致如下：select  a.字段n from tab_a a

where

a.字段2 is not null;a.字段2增加了索引的，但是查询速度非常慢，于是做了如下修改：select  a.字段n from tab_a a

where

nvl(a.字段2,'0' ) != '0';速度提升很明显。原因是什么呢？其实很简单，因为is null和is not null使字段的索引失效了。虽然都知道哪些情形下会使索引失效，但是有时难免受业务需求的影响而考虑的不够全面，所以sql优化要时刻进行，随时进行。努力提高sql的执行效率。

版权声明：本文为博主原创文章，未经博主允许不得转载。

Linux 系统中重启数据库
Linux 系统中重启数据库：
1、进入数据库用户：  su - oracle
2、先关闭监听： lsnrctl stop
3、再关闭数据库服务 :  sqlplus shutdown immediate
4、开启数据库服务：sqlplus startup
5、开启监听：lsnrctl start


版权声明：本文为博主原创文章，未经博主允许不得转载。

NoSql-MongoDB+Dos窗口下的增删改
   上篇文章主要介绍了MongoDB的下载http://blog.csdn.net/huo065000/article/details/42806533，本篇主要是来简单了解一下通过Dos窗口如何来完成对MOngoDB的增删改查。
   在进入增删改的环境之前，我们还是要打开运行环境的，由于使用频繁，所以我们可以自己创建一个.bat文件来执行，
以我的mongod.exe的存放路径以及数据的存放路径举例：

D:\MongoDB\Server\3.0\bin\mongod.exe -dbpath=D:\Mongodb\data
pause运行的效果如下：



在运行的情况下，我们直接双击启动mongo.exe即可直接操作增删改查等等。
❶对数据库的操作


创建数据库

    >use DB3  


显示已有的数据库  

    >show dbs
运行效果：


❷对表的增删改操作


增加一条数据

   > db.student.insert({"name":"huo","age":"10"})


循环增加数据
   > for(var i=1;i<5;i++) db.student.insert({"name":"huohuo"+1,"age":i});


查看增加的数据

   >db.student.find()


json字符串查看增加的数据

   > var cursor=db.student.find();
       >while(cursor.hasNext()) printjson(cursor.next())
运行效果：







更新字段的操作

     > db.student.update({"age":"10"},{"name":"huo","age":"10"})


删除数据的操作

     >db.student.remove({"name":"huo"})
删除运行效果：


❸MongoDB的一些常用命令
      >show dbs    -- 查看数据库列表


> use DB3   --创建DB3数据库，如果存在DB3数据库则使用DB3数据库

> db   ---显示当前使用的数据库名称

> db.getName()  ---显示当前使用的数据库名称

> db.dropDatabase()  --删当前使用的数据库

> db.repairDatabase()  --修复当前数据库

> db.version()   --当前数据库版本

> db.getMongo()  --查看当前数据库的链接机器地址 

> db.stats() 显示当前数据库状态，包含数据库名称，集合个数，当前数据库大小 ...

> db.getCollectionNames()   --查看数据库中有那些个集合（表）

> show collections    --查看数据库中有那些个集合（表）

> db.person.drop()  --删除当前集合（表）person

部分操作的图示显示：







   Dos窗口下，对有MongoDb的一些基本操作就介绍这些，其实MongoDB一般都是用于各个项目中，比如Java，C#等常用的语言……

……待续

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL设置数据库表为只读
1、设置命令mysql> use test;
Database changed
mysql> lock table t_depart_info read;
Query OK, 0 rows affected (0.00 sec)2、插入数据3、指令分析     由于设置了t_depart_info为只读，不能向其插入数据，故截图中一直在加载请求中...

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-线性表的顺序表示和实现
线性表：最常用且最简单的一种数据结构，是n个数据元素的优先序列。线性表是一个相当灵活的数据结构，它的长度可以根据需要增长或缩短，即对线性表的数据元素不仅可以访问，还以进行插入和删除等。

线性表的顺序表示：用一组地址连续的存储单元依次存储线性表的数据元素。 
线性表的这种机内存储结构称作线性表的顺序存储结构。 
线性表的顺序存储结构是一种随机存取结构，表中的任一数据元素都可以随机存取。

读取操作：对于采用顺序存储结构的线性表，读取操作是非常容易实现的，知道第一个数据元素的存储地址LOC(a1)和每个数据元素所占用的存储单元个数L求第i个数据元素的存储位置： 
LOC(ai)=LOC(a1)+(i-1)*L
插入操作：在线性表的第i-1个数据元素和第i个数据元素之间插入一个新的数据元素b，使长度为n的线性表：（a1，…ai-1，ai，…an） 
变为长度为n+1的线性表：（a1，…ai-1，b,ai，…an）。 
由于逻辑上相邻的元素在物理位置上也是相邻的，所以第i到n个数据元素必须向后移动才能反映这个逻辑关系的变化。
删除操作：线性表的删除操作使长度为n的线性表（a1，…ai-1，ai，ai+1…an） 
变为长度为n-1的线性表（a1，…ai-1，ai+1，…an）. 
数据元素ai-1,ai,ai+1之间的逻辑关系发生了变化，为了在存储结构上反映这一变化，同样需要移动数据元素。将第i+1到第n个数据元素向前移动。
时间复杂度：在顺序存储的线性表中插入或者删除一个数据元素，其时间主要耗费在移动元素上，插入与删除操作的时间复杂度为O（n）.

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（六）——修改表、数据的备份和恢复
1、修改表


添加一个字段：alter  table  distributors


2、数据的备份和恢复

1）使用企业管理器完成备份和恢复（2种方式）


分离/附加

分离完成后，到sql  server 安装的目录下找两个文件（数据库名.mdf）和（数据库名.ldf）。这两个文件即分离后的文件，数据库分离后，该数据库不可再用。附加：当用户需要重新使用某个分离的数据库时进行的操作，即让sql  server数据库重新关联该数据库。
备份/恢复

备份数据库：把某个数据库文件从sql  server中备份出来，这样用于可以根据需要再使用（用于恢复、复用……）。不会影响源数据库的使用。恢复数据库：当源数据库因为某种原因需要恢复时进行的操作。

2）查询管理器



备份

backup  database  数据库名  to  disk='d:\\xxx.bak'
恢复

restore  database  数据库名  from  disk='d:\\xxx.bak'



版权声明：本文为博主原创文章，未经博主允许不得转载。

[Err] 2006 - MySQL server has gone away
1、错误描述[Err] 2006 - MySQL server has gone away2、错误原因     在将数据库脚本利用MySQL客户端导入时，出现这个错误；结果查明，由于脚本中的insert语句过多，插入数据量过大，导致MySQL客户端和服务器连接断开3、解决办法（1）修改MySQL配置文件my.ini       设置max_allowed_packed参数（2）查看MySQL连接是否超时（3）查看服务是否中断

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL查看运行时间
1、查看MySQL运行多长时间mysql> SHOW GLOBAL STATUS LIKE 'UPTIME';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| Uptime        | 12823 |
+---------------+-------+
1 row in set (0.00 sec)2、查看MySQL连接超时mysql> SHOW GLOBAL VARIABLES LIKE '%TIMEOUT';
+-----------------------------+----------+
| Variable_name               | Value    |
+-----------------------------+----------+
| connect_timeout             | 10       |
| delayed_insert_timeout      | 300      |
| innodb_flush_log_at_timeout | 1        |
| innodb_lock_wait_timeout    | 50       |
| innodb_rollback_on_timeout  | OFF      |
| interactive_timeout         | 28800    |
| lock_wait_timeout           | 31536000 |
| net_read_timeout            | 30       |
| net_write_timeout           | 60       |
| rpl_stop_slave_timeout      | 31536000 |
| slave_net_timeout           | 3600     |
| wait_timeout                | 28800    |
+-----------------------------+----------+
12 rows in set (0.00 sec)3、查看mysql请求链接进程被主动杀死 mysql> SHOW GLOBAL STATUS LIKE 'COM_KILL';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| Com_kill      | 0     |
+---------------+-------+
1 row in set (0.00 sec)4、查看MySQL通信信息包最大值mysql> SHOW GLOBAL VARIABLES LIKE 'MAX_ALLOWED_PACKET';
+--------------------+---------+
| Variable_name      | Value   |
+--------------------+---------+
| max_allowed_packet | 4194304 |
+--------------------+---------+
1 row in set (0.00 sec)

版权声明：本文为博主原创文章，未经博主允许不得转载。

Mysql5.7全新的root密码规则
原创文章，转载请注明出处：服务器非业余研究http://blog.csdn.net/erlib 作者Sunface联系邮箱：cto@188.com 今天在安装mysql5.7.8的时候遇到一些问题，首当其冲便的是初始root密码的变更，特分享解决方法如下： 1.mysql5.7会生成一个初始化密码，而在之前的版本首次登陆不需要登录。 shell> cat /root/.mysql_secret  # Password set for user 'root@localhost' at 2015-04-22 22:13:23  ?G5W&tz1z.cN 2.若第一步成功，则使用该密码继续第7步（笔者由于找不到该文件，只能从第3步开始) 3.修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables 4.service mysqld restart后，即可直接用mysql进入 5.mysql> update mysql.user set authentication_string=password('123qwe') where user='root' and Host = 'localhost';    mysql> flush privileges;    mysql> quit; 6.将/etc/my.cnf文件还原，重新启动mysql:service mysql restart,这个时候可以使用mysql -u root -p'123qwe'进入了 7.mysql>SET PASSWORD = PASSWORD('newpasswd'); 设置新密码  总结一下：想尝鲜，就要付出代价！

版权声明：本文为博主原创文章，未经博主允许不得转载。

Oracle与Mysql数据的事务处理机制
相比oracle的事务处理，Mydql相对还是简单一点的，但是事务作为一项重要的安全机制在数据库里面是必不可少的，特别是里面的事务回滚机制非常的有用，不多说了，先说一下mysql的事务处理：
Mysql简单的回滚：
第一步：开始事务：start transaction;
第二步：执行dml等其他的变化操作。
第三步：混滚到开始事务之前状态： rollback;


这样的话在执行完毕之后就发现之前的一些操作没有执行成功。只有commit提交之后才会真正的把数据提交。但是，还要考虑下面的两种情况，一种是服务关闭的情况，这样我们的数据回滚在没有提交的情况下是有真正的去执行，还有就是回滚是能够用一次，再次使用虽然不报错，但是不起任何作用。
如果你会说了，我想在一次事务中国实现多次回滚怎么办，这里你可以使用设置保存点：
在上面的第二步中，你可以这样一下： savepoint 保存点1（名称而已）;
然后你继续执行dml，到了耨一个地方继续savepoint 保存点2;
你是不是想通过rollback to 保存点的方式会滚到之前的状态。但是，不得不告诉你，旺旺想想的和显示的情况并不是一模一样，你会发现只能够滚滚到第一个保存
点，其他的保存点会提示不存在！
Oracle的事务处理：
Oracle是不用说明开启事务的，针对dml语句，当你设置保存点，接下来的步骤如上返回到保准点同样的用commit提交事务。


但是，我这里就总结出了一个重要的原则：事务回滚只能反方向一直逆行，这个过程已然不可逆。除非提交结束事务。








版权声明：本文为博主原创文章，未经博主允许不得转载。

[置顶]
        C++栈学习——顺序栈和链栈的区别

C++中栈有顺序栈和链栈之分，在顺序栈中，定义了栈的栈底指针（存储空间首地址base）、栈顶指针top以及顺序存储空间的大小stacksize（个人感觉这个数据成员是可以不用定义的）

//顺序栈数据结构C++类声明（基类）
template <typename ElemType>
class SqStack 
{
public:
    void clear();                                                 //把顺序栈置空
    int getLength();                                              //求顺序栈中元素个数
    int getstackSize();                                     //返回当前已分配的存储空间的大小
    Status getTop(ElemType & e);                                  //读栈顶的元素
    bool isEmpty();                                               //判断顺序栈是否为空
    SqStack<ElemType> operator =(SqStack<ElemType> rightS);       //重载赋值运算符的定义
    Status pop(ElemType & e);                                     //弹出栈顶元素到e
    void push(ElemType & e );                                     //在栈顶压入元素e
//*****************************下面为系统自动调用构造函数及析构函数声明******************************//

    SqStack(); //顺序栈构造函数
    virtual ~SqStack();//顺序栈析构函数
    SqStack (const SqStack<ElemType>& otherS);//顺序栈拷贝初始换构造函数

protected:
    ElemType *base;
    ElemType *top;
    int stackSize;//顺序存储空间的大小
};

而对于链栈来说，它只定义栈顶指针。

template<typename ElemType>
class Linkstack
{

private:
    class LinkNode
    {
    public:
        ElemType data;
        LinkNode *next;
    };
    typedef LinkNode * NodePointer;

public:
    void clear();
    int getlength();
    void display();
    void randLinkStack();
    Linkstack <ElemType> operator = (Linkstack <ElemType> rightS);

protected:
    NodePointer top;

};
其实这二者的区别是由顺序表和链表的存储结构决定的，在空间上，顺序表是静态分配的，而链表则是动态分配的；就存储密度来说：顺序表等于1，而链式表小于1，但是链式表可以将很多零碎的空间利用起来；顺序表查找方便，链式表插入和删除时很方便。 
    顺序表的这种静态存储的方式，决定了必须至少得有首地址和末地址来决定一个空间，否则，不知道查找到哪了；链式表每个节点存储了下一个节点的指针信息，故，对于链栈来说，只需要一个top指针即可查找到整个栈。 
    另外，顺序栈和链栈的top指针有区别，顺序栈的top指针指向栈定的空元素处，top-1才指向栈定元素，而链栈的top指针相当于链表的head指针一样，指向实实在在的元素。 
    另外附自己写的顺序栈和链栈的随机产生函数：
//顺序栈：
template<typename ElemType>
void MyStack<ElemType>::RandStack()
{
    int *p;
    ElemType n;
    ElemType Elem[11];
    srand(time(NULL));
    n=rand()%10+1;
    cout<<"产生的随机栈的深度为："<<n<<endl;
    cout<<"产生的随机栈元素为："<<endl;
    for (int i = 0; i < n; i++)
    {
        Elem[i]=rand()%100+1;
        cout<<Elem[i]<<"  ";
    }
    cout<<endl;
    base=new ElemType[n];
    assert(base!=0);
    top=base;
    stackSize=n;
    for (int j = 0; j < n; j++)
        *(base+j)=Elem[j];
    top=base+n;
    cout<<"随机产生的栈为："<<endl;
    for (int i = 0; i < stackSize; i++)
        cout<<"  "<<*(base+i);
    cout<<endl;
    cout<<"  ♂";
    for (int i = 1; i <stackSize ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<"  ♂"<<endl;
    cout<<" base";
    for (int i = 1; i <stackSize ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<" top"<<endl;

}

template<typename ElemType>
void MyStack<ElemType>::display()
{
    int n=top-base;
    cout<<"当前栈为："<<endl;
    for (int i = 0; i < n; i++)
        cout<<"  "<<*(base+i);
    cout<<endl;
    cout<<"  ♂";
    for (int i = 1; i <n ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<"  ♂"<<endl;
    cout<<" base";
    for (int i = 1; i <n ; i++)
    {
        setw(4);
        cout<<"    ";
    }
    cout<<" top"<<endl;
}

//链栈
template<typename ElemType>
void Linkstack<ElemType>::display()
{
    NodePointer r;
    int num=0;
    r=top;
    while (r)
    {
        cout<<r->data<<"  ";
        r=r->next;
        num++;
    }
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"↑"  ;
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"top"<<endl;

}

template <typename ElemType>
void Linkstack<ElemType>::randLinkStack()
{
    ElemType elem[11];
    srand(unsigned(time(NULL)));
    int n;
    n=rand()%10+1;
    cout<<"the number of the stack is:"<<n<<endl;
    cout<<"the elements here are:";
    for (int i = 0; i < n; i++)
    {
        elem[i]=rand()%100+1;
        cout<<elem[i]<<"  ";
    }
    cout<<endl;
    NodePointer p,s;
    p=NULL;
    for (int i = 0; i < n; i++)
    {
        s=new(LinkNode);
        assert(s!=NULL);
        s->data=elem[i];
        s->next=p;
        p=s;
    }
    top=p;
    cout<<"the stack produced is:"<<endl;
    NodePointer r;
    int num=0;
    r=top;
    while (r)
    {
        cout<<r->data<<"  ";
        r=r->next;
        num++;
    }
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"↑"  ;
    cout<<endl;
    for(int i=0;i<num-1;i++)
        cout<<setw(4)<<"  ";
    cout<<"top"<<endl;
}


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

[ERROR:] [插入失败]   code is 9998;desc is 得到唯一对象不唯一exception is null
1、错误描述[ERROR:]2015-08-25 17:04:38,861 [插入失败] 
 code is 9998;desc is 得到唯一对象不唯一exception is null
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.aop.framework.adapter.AfterReturningAdviceInterceptor.invoke(AfterReturningAdviceInterceptor.java:52)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy72.queryGoodsLogBySerial(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle

(ServletInvocableHandlerMethod.java:104)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod

(RequestMappingHandlerAdapter.java:749)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal

(RequestMappingHandlerAdapter.java:689)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at com.skycloud.oa.filter.TranscationFilter.doFilter(TranscationFilter.java:32)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at com.skycloud.oa.filter.ContentFilter.doFilter(ContentFilter.java:57)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:88)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:344)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:261)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)
2、错误原因SELECT 
  t.name 
FROM
  t_stu_info t 
WHERE t.id = 
  (SELECT 
    r.code 
  FROM
    t_teacher_info r 
  WHERE r.name = 'zhangsan') ;      内存查询的结果不唯一，导致id对应多个；本来id是需要是唯一的，但是这个查询出来的结果超过一个，导致报错3、解决办法（1）保证内存查询结果唯一，并且不出现重复（2）如果需要id为多个，可以将“=”改为“in”

版权声明：本文为博主原创文章，未经博主允许不得转载。

MySQL 配置参数详解
MySQL安装成功后有几个默认的配置模板，列表如下：
my-huge.cnf ： 用于高端产品服务器，包括1到2GB RAM,主要运行mysql
my-innodb-heavy-4G.ini ： 用于只有innodb的安装，最多有4GB RAM，支持大的查询和低流量
my-large.cnf ： 用于中等规模的产品服务器，包括大约512M RAM
my-medium.cnf ： 用于低端产品服务器，包括很少内存（少于128M）
my-small.cnf ： 用于最低设备的服务器，只有一点内存（少于512M）
my.cnf具体的配置说明如下：




basedir = path
使用给定目录作为根目录(安装目录)。


character-sets-dir = path
给出存放着字符集的目录。


datadir = path
从给定目录读取数据库文件。


pid-file = filename
为mysqld程序指定一个存放进程ID的文件(仅适用于UNIX/Linux系统); Init-V脚本需要使用这个文件里的进程ID结束mysqld进程。


socket = filename
为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(仅适用于UNIX/Linux系统; 默认设置一般是/var/lib/mysql/mysql.sock文件)。在Windows环境下，如果MySQL客户与服务器是通过命名管道进行通信 的，–sock选项给出的将是该命名管道的名字(默认设置是MySQL)。


lower_case_table_name = 1/0
新目录和数据表的名字是否只允许使用小写字母; 这个选项在Windows环境下的默认设置是1(只允许使用小写字母)。



mysqld程序：语言设置




character-sets-server = name
新数据库或数据表的默认字符集。为了与MySQL的早期版本保持兼容，这个字符集也可以用–default-character-set选项给出; 但这个选项已经显得有点过时了。


collation-server = name
新数据库或数据表的默认排序方式。


lanuage = name
用指定的语言显示出错信息。



mysqld程序：通信、网络、信息安全




enable-named-pipes
允许Windows 2000/XP环境下的客户和服务器使用命名管道(named pipe)进行通信。这个命名管道的默认名字是MySQL，但可以用–socket选项来改变。


local-infile [=0]
允许/禁止使用LOAD DATA LOCAL语句来处理本地文件。


myisam-recover [=opt1, opt2, ...]
在启动时自动修复所有受损的MyISAM数据表。这个选项的可取值有4种:DEFAULT、BACKUP、QUICK和FORCE; 它们与myisamchk程序的同名选项作用相同。


old-passwords
使用MySQL 3.23和4.0版本中的老算法来加密mysql数据库里的密码(默认使用MySQL 4.1版本开始引入的新加密算法)。


port = n
为MySQL程序指定一个TCP/IP通信端口(通常是3306端口)。


safe-user-create
只有在mysql.user数据库表上拥有INSERT权限的用户才能使用GRANT命令; 这是一种双保险机制(此用户还必须具备GRANT权限才能执行GRANT命令)。


shared-memory
允许使用内存(shared memory)进行通信(仅适用于Windows)。


shared-memory-base-name = name
给共享内存块起一个名字(默认的名字是MySQL)。


skip-grant-tables
不使用mysql数据库里的信息来进行访问控制(警告:这将允许用户任何用户去修改任何数据库)。


skip-host-cache
不使用高速缓存区来存放主机名和IP地址的对应关系。


skip-name-resovle
不把IP地址解析为主机名; 与访问控制(mysql.user数据表)有关的检查全部通过IP地址行进。


skip-networking
只允许通过一个套接字文件(Unix/Linux系统)或通过命名管道(Windows系统)进行本地连接，不允许ICP/IP连接; 这提高了安全性，但阻断了来自网络的外部连接和所有的Java客户程序(Java客户即使在本地连接里也使用TCP/IP)。


user = name
mysqld程序在启动后将在给定UNIX/Linux账户下执行; mysqld必须从root账户启动才能在启动后切换到另一个账户下执行; mysqld_safe脚本将默认使用–user=mysql选项来启动mysqld程序。



mysqld程序：内存管理、优化、查询缓存区




bulk_insert_buffer_size = n
为一次插入多条新记录的INSERT命令分配的缓存区长度(默认设置是8M)。


key_buffer_size = n
用来存放索引区块的RMA值(默认设置是8M)。


join_buffer_size = n
在参加JOIN操作的数据列没有索引时为JOIN操作分配的缓存区长度(默认设置是128K)。


max_heap_table_size = n
HEAP数据表的最大长度(默认设置是16M); 超过这个长度的HEAP数据表将被存入一个临时文件而不是驻留在内存里。


max_connections = n
MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。


query_cache_limit = n
允许临时存放在查询缓存区里的查询结果的最大长度(默认设置是1M)。


query_cache_size = n
查询缓存区的最大长度(默认设置是0，不开辟查询缓存区)。


query_cache_type = 0/1/2
查询缓存区的工作模式:0, 禁用查询缓存区; 1，启用查询缓存区(默认设置); 2，”按需分配”模式，只响应SELECT SQL_CACHE命令。


read_buffer_size = n
为从数据表顺序读取数据的读操作保留的缓存区的长度(默认设置是128KB); 这个选项的设置值在必要时可以用SQL命令SET SESSION read_buffer_size = n命令加以改变。


read_rnd_buffer_size = n
类似于read_buffer_size选项，但针对的是按某种特定顺序(比如使用了ORDER BY子句的查询)输出的查询结果(默认设置是256K)。


sore_buffer = n
为排序操作分配的缓存区的长度(默认设置是2M); 如果这个缓存区太小，则必须创建一个临时文件来进行排序。


table_cache = n
同时打开的数据表的数量(默认设置是64)。


tmp_table_size = n
临时HEAP数据表的最大长度(默认设置是32M); 超过这个长度的临时数据表将被转换为MyISAM数据表并存入一个临时文件。



mysqld程序：日志




log [= file]
把所有的连接以及所有的SQL命令记入日志(通用查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname.log文件作为这种日志文件(hostname是服务器的主机名)。


log-slow-queries [= file]
把执行用时超过long_query_time变量值的查询命令记入日志(慢查询日志); 如果没有给出file参数，MySQL将在数据库目录里创建一个hostname-slow.log文件作为这种日志文件(hostname是服务器主机 名)。


long_query_time = n
慢查询的执行用时上限(默认设置是10s)。


long_queries_not_using_indexs
把慢查询以及执行时没有使用索引的查询命令全都记入日志(其余同–log-slow-queries选项)。


log-bin [= filename]
把对数据进行修改的所有SQL命令(也就是INSERT、UPDATE和DELETE命令)以二进制格式记入日志(二进制变更日志，binary update log)。这种日志的文件名是filename.n或默认的hostname.n，其中n是一个6位数字的整数(日志文件按顺序编号)。


log-bin-index = filename
二进制日志功能的索引文件名。在默认情况下，这个索引文件与二进制日志文件的名字相同，但后缀名是.index而不是.nnnnnn。


max_binlog_size = n
二进制日志文件的最大长度(默认设置是1GB)。在前一个二进制日志文件里的信息量超过这个最大长度之前，MySQL服务器会自动提供一个新的二进制日志文件接续上。


binlog-do-db = dbname
只把给定数 据库里的变化情况记入二进制日志文件，其他数据库里的变化情况不记载。如果需要记载多个数据库里的变化情况，就必须在配置文件使用多个本选项来设置，每个数据库一行。


binlog-ignore-db = dbname
不把给定数据库里的变化情况记入二进制日志文件。


sync_binlog = n
每经过n次日志写操作就把日志文件写入硬盘一次(对日志信息进行一次同步)。n=1是最安全的做法，但效率最低。默认设置是n=0，意思是由操作系统来负责二进制日志文件的同步工作。


log-update [= file]
记载出错情况的日志文件名(出错日志)。这种日志功能无法禁用。如果没有给出file参数，MySQL会使用hostname.err作为种日志文件的名字。



mysqld程序：镜像(主控镜像服务器)




server-id = n
给服务器分配一个独一无二的ID编号; n的取值范围是1~2的32次方启用二进制日志功能。


log-bin = name
启用二进制日志功能。这种日志的文件名是filename.n或默认的hostname.n，其中的n是一个6位数字的整数(日志文件顺序编号)。


binlog-do/ignore-db = dbname
只把给定数据库里的变化情况记入二进制日志文件/不把给定的数据库里的变化记入二进制日志文件。



mysqld程序：镜像(从属镜像服务器)




server-id = n
给服务器分配一个唯一的ID编号


log-slave-updates
启用从属服务器上的日志功能，使这台计算机可以用来构成一个镜像链(A->B->C)。


master-host = hostname
主控服务器的主机名或IP地址。如果从属服务器上存在mater.info文件(镜像关系定义文件)，它将忽略此选项。


master-user = replicusername
从属服务器用来连接主控服务器的用户名。如果从属服务器上存在mater.info文件，它将忽略此选项。


master-password = passwd
从属服务器用来连接主控服务器的密码。如果从属服务器上存在mater.info文件，它将忽略此选项。


master-port = n
从属服务器用来连接主控服务器的TCP/IP端口(默认设置是3306端口)。


master-connect-retry = n
如果与主控服务器的连接没有成功，则等待n秒(s)后再进行管理方式(默认设置是60s)。如果从属服务器存在mater.info文件，它将忽略此选项。


master-ssl-xxx = xxx
对主、从服务器之间的SSL通信进行配置。


read-only = 0/1
0: 允许从属服务器独立地执行SQL命令(默认设置); 1: 从属服务器只能执行来自主控服务器的SQL命令。


read-log-purge = 0/1
1: 把处理完的SQL命令立刻从中继日志文件里删除(默认设置); 0: 不把处理完的SQL命令立刻从中继日志文件里删除。


replicate-do-table = dbname.tablename
与–replicate-do-table选项的含义和用法相同，但数据库和数据库表名字里允许出现通配符”%” (例如: test%.%–对名字以”test”开头的所有数据库里的所以数据库表进行镜像处理)。


replicate-do-db = name
只对这个数据库进行镜像处理。


replicate-ignore-table = dbname.tablename
不对这个数据表进行镜像处理。


replicate-wild-ignore-table = dbn.tablen
不对这些数据表进行镜像处理。


replicate-ignore-db = dbname
不对这个数据库进行镜像处理。


replicate-rewrite-db = db1name > db2name
把主控数据库上的db1name数据库镜像处理为从属服务器上的db2name数据库。


report-host = hostname
从属服务器的主机名; 这项信息只与SHOW SLAVE HOSTS命令有关–主控服务器可以用这条命令生成一份从属服务器的名单。


slave-compressed-protocol = 1
主、从服务器使用压缩格式进行通信–如果它们都支持这么做的话。


slave-skip-errors = n1, n2, …或all
即使发生出错代码为n1、n2等的错误，镜像处理工作也继续进行(即不管发生什么错误，镜像处理工作也继续进行)。如果配置得当，从属服务器不应 该在执行 SQL命令时发生错误(在主控服务器上执行出错的SQL命令不会被发送到从属服务器上做镜像处理); 如果不使用slave-skip-errors选项，从属服务器上的镜像工作就可能因为发生错误而中断，中断后需要有人工参与才能继续进行。



mysqld–InnoDB：基本设置、表空间文件




skip-innodb
不加载InnoDB数据表驱动程序–如果用不着InnoDB数据表，可以用这个选项节省一些内存。


innodb-file-per-table
为每一个新数据表创建一个表空间文件而不是把数据表都集中保存在中央表空间里(后者是默认设置)。该选项始见于MySQL 4.1。


innodb-open-file = n
InnoDB数据表驱动程序最多可以同时打开的文件数(默认设置是300)。如果使用了innodb-file-per-table选项并且需要同时打开很多数据表的话，这个数字很可能需要加大。


innodb_data_home_dir = p
InnoDB主目录，所有与InnoDB数据表有关的目录或文件路径都相对于这个路径。在默认的情况下，这个主目录就是MySQL的数据目录。


innodb_data_file_path = ts
用来容纳InnoDB为数据表的表空间: 可能涉及一个以上的文件; 每一个表空间文件的最大长度都必须以字节(B)、兆字节(MB)或千兆字节(GB)为单位给出; 表空间文件的名字必须以分号隔开; 最后一个表空间文件还可以带一个autoextend属性和一个最大长度(max:n)。例如，ibdata1:1G; ibdata2:1G:autoextend:max:2G的意思是: 表空间文件ibdata1的最大长度是1GB，ibdata2的最大长度也是1G，但允许它扩充到2GB。除文件名外，还可以用硬盘分区的设置名来定义表
 空间，此时必须给表空间的最大初始长度值加上newraw关键字做后缀，给表空间的最大扩充长度值加上raw关键字做后缀(例如/dev/hdb1: 20Gnewraw或/dev/hdb1:20Graw); MySQL 4.0及更高版本的默认设置是ibdata1:10M:autoextend。


innodb_autoextend_increment = n
带有autoextend属性的表空间文件每次加大多少兆字节(默认设置是8MB)。这个属性不涉及具体的数据表文件，那些文件的增大速度相对是比较小的。


innodb_lock_wait_timeout = n
如果某个事务在等待n秒(s)后还没有获得所需要的资源，就使用ROLLBACK命令放弃这个事务。这项设置对于发现和处理未能被InnoDB数据表驱动 程序识别出来的死锁条件有着重要的意义。这个选项的默认设置是50s。


innodb_fast_shutdown 0/1
是否以最快的速度关闭InnoDB，默认设置是1，意思是不把缓存在INSERT缓存区的数据写入数据表，那些数据将在MySQL服务器下次启动 时再写入 (这么做没有什么风险，因为INSERT缓存区是表空间的一个组成部分，数据不会丢失)。把这个选项设置为0反面危险，因为在计算机关闭时，InnoDB 驱动程序很可能没有足够的时间完成它的数据同步工作，操作系统也许会在它完成数据同步工作之前强行结束InnoDB，而这会导致数据不完整。



mysqld程序：InnoDB–日志




innodb_log_group_home_dir = p
用来存放InnoDB日志文件的目录路径(如ib_logfile0、ib_logfile1等)。在默认的情况下，InnoDB驱动程序将使用 MySQL数据目录作为自己保存日志文件的位置。


innodb_log_files_in_group = n
使用多少个日志文件(默认设置是2)。InnoDB数据表驱动程序将以轮转方式依次填写这些文件; 当所有的日志文件都写满以后，之后的日志信息将写入第一个日志文件的最大长度(默认设置是5MB)。这个长度必须以MB(兆字节)或GB(千兆字节)为单 位进行设置。


innodb_flush_log_at_trx_commit = 0/1/2
这个选项决定着什么时候把日志信息写入日志文件以及什么时候把这些文件物理地写(术语称为”同步”)到硬盘上。设置值0的意思是每隔一秒写一次日 志并进行 同步，这可以减少硬盘写操作次数，但可能造成数据丢失; 设置值1(设置设置)的意思是在每执行完一条COMMIT命令就写一次日志并进行同步，这可以防止数据丢失，但硬盘写操作可能会很频繁; 设置值2是一般折衷的办法，即每执行完一条COMMIT命令写一次日志，每隔一秒进行一次同步。


innodb_flush_method = x
InnoDB日志文件的同步办法(仅适用于UNIX/Linux系统)。这个选项的可取值有两种: fdatasync，用fsync()函数进行同步; O_DSYNC，用O_SYNC()函数进行同步。


innodb_log_archive = 1
启用InnoDB驱动程序的archive(档案)日志功能，把日志信息写入ib_arch_log_n文件。启用这种日志功能在InnoDB与 MySQL一起使用时没有多大意义(启用MySQL服务器的二进制日志功能就足够用了)。



mysqld程序–InnoDB：缓存区的设置和优化




innodb_log_buffer_pool_size = n
为InnoDB数据表及其索引而保留的RAM内存量(默认设置是8MB)。这个参数对速度有着相当大的影响，如果计算机上只运行有 MySQL/InnoDB数据库服务器，就应该把全部内存的80%用于这个用途。


innodb_log_buffer_size = n
事务日志文件写操作缓存区的最大长度(默认设置是1MB)。


innodb_additional_men_pool_size = n
为用于内部管理的各种数据结构分配的缓存区最大长度(默认设置是1MB)。


innodb_file_io_threads = n
I/O操作(硬盘写操作)的最大线程个数(默认设置是4)。


innodb_thread_concurrency = n
InnoDB驱动程序能够同时使用的最大线程个数(默认设置是8)。



mysqld程序：其它选项




bind-address = ipaddr
MySQL服务器的IP地址。如果MySQL服务器所在的计算机有多个IP地址，这个选项将非常重要。


default-storage-engine = type
新数据表的默认数据表类型(默认设置是MyISAM)。这项设置还可以通过–default-table-type选项来设置。


default-timezone = name
为MySQL服务器设置一个地理时区(如果它与本地计算机的地理时区不一样)。


ft_min_word_len = n
全文索引的最小单词长度工。这个选项的默认设置是4，意思是在创建全文索引时不考虑那些由3个或更少的字符构建单词。


Max-allowed-packet = n
客户与服务器之间交换的数据包的最大长度，这个数字至少应该大于客户程序将要处理的最大BLOB块的长度。这个选项的默认设置是1MB。


Sql-mode = model1, mode2, …
MySQL将运行在哪一种SQL模式下。这个选项的作用是让MySQL与其他的数据库系统保持最大程度的兼容。这个选项的可取值包括ansi、db2、 oracle、no_zero_date、pipes_as_concat。



注意：如果在配置文件里给出的某个选项是mysqld无法识别的，MySQL服务器将不启动。

版权声明：本文为博主原创文章，未经博主允许不得转载。

每天进步一点点————MySQL锁
一、           锁
MySQL对MyISAM和MEMORY引擎实现行表级锁，对BDB存储引擎进行页级锁，对InnDB存储引擎表进行行行级锁。
按照粒度分：从大到小（MySQL仅支持表级锁，行锁需要存储引擎完成；所有引擎都有自己锁策略）
                   表锁：锁定整张表，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率最高，并发度最低。
                  页锁：锁定一个数据块（数据页面）。开销和加锁时间介于表锁行锁之间，会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。
                   行锁：锁定一个行。开销大，加锁慢；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。
一般来说,表锁适合以查询为主，只有少量按索引条件爱你更新数据的应用，如web应用。
行级锁适合大量按索引条件并发更新少量不同数据，同时又有并发查询能力的应用，如一些在线事务系统（OLTP）
1.   锁语句
手动加锁：lock tables 表名 [read|write]
 
      给t9表上只读锁
mysql>lock table t9 read;
Query OK, 0 rows affected (0.00 sec)
      查看锁
mysql>  show global status like"table_locks%";
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| Table_locks_immediate | 122   |                          
发生表锁定操作, 但表锁定后马上释放
| Table_locks_waited    | 0    |                         
发生表锁定,并因此具有锁等待
+-----------------------+-------+
2 rows in set (0.00 sec)
      给t9表上写锁
mysql>lock table t9 write;
Query OK, 0 rows affected (0.00 sec)
解锁表
mysql>unlock tables;
Query OK, 0 rows affected (0.00 sec)
2.   MyISAM表锁
         MyISAM存储引擎只支持表锁。
查看表级锁的征用情况
mysql> show status like 'table%';
+----------------------------+-------+
| Variable_name              | Value |
+----------------------------+-------+
| Table_locks_immediate      | 1258 |
|Table_locks_waited         | 0     |                    锁等待
| Table_open_cache_hits      | 99   |
| Table_open_cache_misses    | 1    |
| Table_open_cache_overflows | 0     |
+----------------------------+-------+
5 rows in set (0.00 sec)
如果说TABLE_LOCKS_WAITED的值比较高，则说明存在着较严重的争用情况。
mysql>lock table emp read local
Query OK, 0 rows affected (0.00 sec)
如果在locktables 时加了local，作用是在满足MyISAM表并发插入条件的情况下，允许其他用户在表尾插入记录。
 
mysql>select * from emp1;
ERROR 1100 (HY000): Table 'emp1' was notlocked with LOCK TABLES
 
在使用locktables 给表加锁时，必须同事取得所有涉及表的锁，并且MySQL不支持锁升级。也就是说，如果是读锁，只能执行查询操作，不能执行更新操作。
 
mysql>update emp set store_id=30 where id=25;
ERROR 1099 (HY000): Table 'emp' was lockedwith a READ lock and can't be updated
 
并且不能通过别名进行访问，所以需要对别名也要加锁。
 
mysql>select a.id from emp a;
ERROR 1100 (HY000): Table 'a' was notlocked with LOCK TABLES
 
并发插入
MyISAM表的读和写是串行的，但是在一定条件下，MyISAM表也会支持查询和插入操作的并发进行。
MyISAM存储引擎有一个系统变量concurrent_insert，是专门用来控制其并发插入的行为，值分别为0、1、2。
当为0时：不允许并发插入。
当为1时：如果MyISAM表中没有空洞，MyISAM允许在一个进程读取表，另一个进程从表尾插入记录。（默认）无论MyISAM表中有没有空洞都允许在表尾并发插入记录。
 
MyISAM调度锁
MyISAM存储引擎的读写锁是互斥的，如果一个进程请求某个MyISAM表的读锁，同时另一个进程请求同一个表的写锁，那么MySQL会让写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插入到读锁请求之前。这是因为MySQL认为写请求比一般的读请求重要。因此，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况非常糟糕！
幸好我们可以通过一些设置来调节MyISAM的调度行为。
         通过指定启动参数low-priority-updates,使MyISAM引擎默认给予读请求，以有线的权利。
         通过执行命令SETLOW_PRIORITY_UPDATES=1,使该链接发出的更新请求优先级降低。
         通过指定 insert、update、delete语句的LOW_PRIORITY属性，降低该语句的优先级。
         并且也可以通过给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁到达这个值后，MySQL就暂时将写请求的优先级降低，给读进程一个获得锁的机会。
这里强调一点：一个需要长时间的运行的查询操作，也会使写进程“饿死”！因此要尽量避免长时间查询操作。

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（四）——多表查询、子查询、分页查询、用查询结果创建新表和外连接
1、多表查询
1）笛卡尔集：


select  *  from  表名1，表名2select  *  from  表名1，表名2  where   表名1.字段名=表名2.字段名
注：



若有两张表有相同名字的字段，则使用时需带表名（别名）。order  by  置于where 条件之后。
2）自连接：同一张表的连接查询，将一张表视为两张表或多张表。

eg:显示公司每个员工名字和他的上级的名字。将emp表看做两张表worker和boss


select  worker.ename  雇员，boss.ename  老板   from   emp  worker,emp  boss  where   worker.mgr=boss.empno



2、子查询（嵌套查询）：嵌入在其它sql语句中的select语句。
1）单行子查询：只返回一行数据的子查询语句。
2）多行子查询：返回多行数据的子查询。
3）在from子句中使用子查询。
说明：
     当在from子句中使用子查询时，该子查询会被作为一个视图（临时表）来对待，因此也叫做内嵌视图。当在from子句中使用查询时，必须给子查询指定别名。


3、分页查询
1）top  n：前n条记录。


select  top  5  *  from  emp  order  by  hiredate显示第5个到第9个人的信息（按sal高低）

select  top  5   from  emp  where  empno  not  in  (select  top  4  empno  from  emp order  by  sal  desc)  order  by  sal  desc
identity(1,1)：表示字段自增长，从“1”开始增长，每次加“1”。

create  table  test  (testId  int  primary  key  identity(1,1))




4、用查询结果创建新表


select  *  into  另一个表名  from  表名删除表中的重复记录

select  distinct  *  into  #temo(新表)  from  表名1delete  from  表名1insert  into  表名1  select  *  from  #tempdrop  table  #temp




5、外连接：


左外连接：左边的表的记录全部显示，如果没有匹配的记录，用Null填补。右外连接：右边的表的记录全部显示，如果没有匹配的记录，用Null填补。



版权声明：本文为博主原创文章，未经博主允许不得转载。

在java代码中使用Oracle数据库的事务处理机制
//使用java代码操作oracle数据库的代码如下：
package Transaction;


import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

public class demon_1 {
	public static void main(String[] args) throws SQLException {
		//2：连接数据库
		Connection ct=null;
		//加载数据库驱动
		try {
			//1:加载数据库驱动
			Class.forName("oracle.jdbc.driver.OracleDriver");
			ct=DriverManager.getConnection("jdbc:oracle:thin:@127.0.0.1:1521:ORCL","SCOTT","toor");
			
			//3：预编译sql语句
//			PreparedStatement ps=ct.prepareStatement();
			//两种预编译都可以
			Statement ps=ct.createStatement();
			//4： 执行sql语句，执行结果集
			ResultSet res=ps.executeQuery("select * from emp_1");
			while(res.next()){
				System.out.println("员工名字是："+res.getString("ename"));
			}
			//执行事务
			ct.setAutoCommit(false);
			ps.executeUpdate("update emp_1 set ename='ccc' where empno=4");
			ps.executeUpdate("update emp_1 set ename='bbb' where emno=2");
			//提交事务
			ct.commit();
			ct.close();
		} catch (Exception e) {
			//取消事务
			ct.rollback();
			System.out.println("SQL语句执行错误。执行了事务回滚");
		}
	}
}





版权声明：本文为博主原创文章，未经博主允许不得转载。

一次数据库相关操作卡住的排查--enq: TX - row lock contention

问题描述：某日客户来电某HR系统排值班表的操作一直HANG住，一直无法完成。
这种问题，主要思路是围绕查看此操作因何HANG住。
常见的严重的HANG住有DB方面的AUDIT无空间、归档空间满以及主机方面CPU/内存使用率高或/根目录满等状况；

在此排查过程中，主机状态是第一步要查询的，如磁盘空间/CPU/内存使用情况等。
主机无异常后，查看DB状态，包括进程状态。如查看ALERT日志、确认能否登陆，查询一些V$视图、进行REDO切换等；
在这些基础的确认后再查询当前正在发生的等待事件及相关SESSION信息。

在本次问题中主要等待事件是enq: TX - row lock contention行锁问题。大致处理思路如下：
1.查询主机相关状态是否正常
2.查看DB状态
3.查看当前主要正在发生的等待事件及相关SESSION信息
4.确认相关等待事件后进一步排查何种原因引起
5.找到原因后进行处理




分析处理过程如下：
1.查询主机相关状态是正常的，此处不再多写输出。
使用命令有如下:
[oracle@ABCSRV02 ~]$ top
[oracle@ABCSRV02 ~]$ free -m
[oracle@ABCSRV02 ~]$ cat /proc/meminfo 
[oracle@ABCSRV02 ~]$ df -h

2.查看DB状态
查看通过监听连接的进程数
[oracle@ABCSRV02 ~]$ ps -ef|grep LOCAL=NO|wc -l
lsnrctl status
ps -ef|grep smon

sqlplus / as sysdba登陆后查询了如下信息：
--查询实例状态
select to_char(startup_time,'yyyy/mm/dd hh24:mi:ss'),instance_name,status from v$instance;
--查询DB读写状态
select name,open_mode from v$database;
--查询数据文件及表空间状态是否正常
set linesize 160
set pagesize 1000
col tablespace_name for a22
select tablespace_name,status from dba_tablespaces;
select file#,status from v$datafile;
　select * from v$recover_file;
--进行REDO LOG切换
SQL> alter system switch logfile;
System altered.
此处确认状态正常。

3.查看当前主要正在发生的等待事件及相关SESSION信息
set linesize 200
col event for a25
col username for a10
select g.Inst_id,g.sid,g.serial#,g.event,g.username, g.sql_id
from gv$session g,v$sql s
where g.Wait_class <> 'Idle' and g.sql_hash_value=s.HASH_VALUE;
  
本处查询到的是TX锁相关事件，使用如下SQL语句查询：  
select g.Inst_id,g.sid,g.serial#,g.event,g.username, g.sql_id
from gv$session g,v$sql s
where g.Wait_class <> 'Idle' and g.sql_hash_value=s.HASH_VALUE and g.event like '%T%';
--为了隐私实际查询的结果进行了一些修改。
   INST_ID        SID    SERIAL# EVENT                                    USERNAME   SQL_HASH_VALUE
---------- ---------- ---------- ---------------------------------------- ---------- --------------
         1       1056        397 enq: TX - row lock contention           ABCHR         1828393250
         1       1058       1981 enq: TX - row lock contention           ABCHR         3151192798
         1       1069       3802 enq: TX - row lock contention           ABCHR          734455977
         1       1075        397 enq: TX - row lock contention           ABCHR         3627991293
         1       1076       4025 enq: TX - row lock contention           ABCHR         3204010047
         1       1084       2990 enq: TX - row lock contention           ABCHR         3636242951
         1       1088       1185 enq: TX - row lock contention           ABCHR         1010386057
         1       1093        685 enq: TX - row lock contention           ABCHR         2969205112


4.确认相关等待事件后进一步排查

查询到大量enq: TX - row lock contention后，需要查出这些会话等待在哪个会话来释放enq: TX - row lock contention行锁资源。
也就是被哪个会话的什么操作阻塞。
--使用SQL如下：
select blocking_session,sid,serial#,wait_class,seconds_in_wait
from v$session
where blocking_session is not NULL
and sid in('sid');

如下查询到是SID=1050的会话阻塞了这些SQL；
SQL> select blocking_session,sid,serial#,wait_class,seconds_in_wait
  2  from v$session
  3  where blocking_session is not NULL
and sid in(1056,1058,1069,1075,1076,1084,1088,1093);
  4  
BLOCKING_SESSION        SID    SERIAL# WAIT_CLASS              SECONDS_IN_WAIT
---------------- ---------- ---------- ----------------------- ---------------
            1050       1056        397 Application                       10561
            1050       1058       1981 Application                        4129
            1050       1069       3802 Application                        8549
            1050       1075        397 Application                       10913
            1050       1076       4025 Application                         623
            1050       1084       2990 Application                        6234
            1050       1088       1185 Application                        2549
            1050       1093        685 Application                        8264

8 rows selected.

下一步进行查询SID=1050的会话在执行什么操作有何等待事件
SQL> set linesize 200
SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS from v$session where sid in('1050');

       SID    SERIAL# MACHINE              PROGRAM              EVENT                     SQL_ID        STATUS
---------- ---------- -------------------- -------------------- ------------------------- ------------- --------
      1050          8 ABCSRV01            JDBC Thin Client     latch: cache buffers chai 62rv794fswmcy ACTIVE
                                                                ns
可以看到此会话在执行的SQLID为62rv794fswmcy，相关等待事件是：latch: cache buffers chains；
进一步得出此SQL语句的文本及相应执行计划，以及事务的相关信息。
--如下SQL语句截取了部分，并修改了表、列名信息。语句较长，有多个case when判断及嵌套多个视图等复杂查询；
SQL> select sql_text from v$sqltext where sql_id='62rv794fswmcy' order by PIECE;

SQL_TEXT
----------------------------------------------------------------
update AAAA set ts='2015-08-24 13:14:10',AAAAA = ( cas
e when ( pAAAs = 'AA' or pAAAs = 'AAAAAAAA' ) then 0 else
 ( select AAAA from AAAA where pk_bclbid = AAAA.p
kAAAs ) end ), if_rest = ( case when ( pAAss = 'AAAX' or pk_c

查看此SQL的执行计划等信息--这里使用了awrsqrpt报告
--从游标缓存中查询此SQL语句执行计划信息：
col plan_table_output for a100
set long 900
set pagesize 100
 select * from table(dbms_xplan.display_cursor('62rv794fswmcy',0,'advanced'));
--生成awrsqrpt报告
SQL> @?/rdbms/admin/awrsqrpt.sql
---此处执行计划太长，文本型的基本不具有可读性，参考了awrsqrpt报告中的信息如下。
Stat Name    Statement Total    Per Execution    % Snap Total
Elapsed Time (ms)     2,686,331           4.69
CPU Time (ms)     2,669,779           24.23
Executions     0            
Buffer Gets     239,883,842           24.78
Disk Reads     0           0.00
Parse Calls     1           0.00
--可以看到此SQL语句目前已经执行了2686秒，Buffer Gets的数据块个数是239,883,842，2亿多个。



进一步查询此SQL对应的会话信息：
SQL> select sid,serial#,machine,program,EVENT,SADDR,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

       SID    SERIAL# MACHINE    PROGRAM              EVENT                                    SADDR            STATUS
---------- ---------- ---------- -------------------- ---------------------------------------- ---------------- ----------
      1050          8 ABCSRV01  JDBC Thin Client     SQL*Net more data from client            000000075B9EE930 ACTIVE


SQL> select START_TIME,STATUS,SES_ADDR from v$transaction where ses_addr='000000075B9EE930';

START_TIME           STATUS     SES_ADDR
-------------------- ---------- ----------------
08/24/15 10:37:21    ACTIVE     000000075B9EE930

可以看到此事务从上午10：37开始运行一直到进行处理时的14：30左右还在运行；
经观察此会话执行的SQL不只上面查出的一条且都运行时间较长，因此判断是同一事务中的多个大型SQL；
因SQL执行速度较慢且在同一事务中，在全部SQL执行完之前事务不提交也不回滚，导致TX行锁资源一直得不到释放。
进而导致其它会话的相关操作都HANG住在等待此会话释放TX锁资源。


5.找到问题原因--TX - row lock contention产生原因并进行处理
经与用户确认，决定KILL此会话；同时联系业务部门确认此会话的相关SQL执行的为何种操作并进行修正。
--KILL会话及后续查询如下：
SQL> col  EVENT for a20
SQL> select sid,serial#,machine,program,EVENT,SADDR,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

       SID    SERIAL# MACHINE    PROGRAM                   EVENT                SADDR            SQL_ID        STATUS
---------- ---------- ---------- ------------------------- -------------------- ---------------- ------------- ----------
      1050          8 ABCSRV01  JDBC Thin Client          latch free           000000075B9EE930 36mu7qg6cc5yu ACTIVE

SQL> alter system kill session '1050,8';

System altered.

SQL> select sid,serial#,machine,program,EVENT,SADDR,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction) and sid=1050;

no rows selected

此时还有部分事务在等待TX锁，稍等后再查询，之前被阻塞的事务全部完成。

SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS from v$session where SADDR in(select SES_ADDR from v$transaction);

       SID    SERIAL# MACHINE    PROGRAM                   EVENT                SQL_ID        STATUS
---------- ---------- ---------- ------------------------- -------------------- ------------- ----------
      1056        397 ABCSRV01  JDBC Thin Client          enq: TX - row lock c 2u1bpvx9s2ygp ACTIVE
                                                           ontention
SQL> select sid,serial#,machine,program,EVENT,SQL_ID,STATUS,saddr from v$session where SADDR in(select SES_ADDR from v$transaction);

no rows selected

SQL> select START_TIME,STATUS,SES_ADDR from v$transaction;

no rows selected


注意事项：
大事务回滚时可能会产生大量REDO信息；
同时并行回滚参数设置不当(如过高)也可能导致回滚事务时HANG住，建议使用默认值LOW。
SQL> show parameter fast_start_p

NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
fast_start_parallel_rollback         string      LOW
查看一个参数是否默认值及允许的值
set linesize 200
col NAME for a30
 col value for a30
col isdefault for a10
select * from V$PARAMETER_VALID_VALUES where name='fast_start_parallel_rollback';

       NUM NAME                              ORDINAL VALUE                          ISDEFAULT
---------- ------------------------------ ---------- ------------------------------ ----------
       782 fast_start_parallel_rollback            1 FALSE                          FALSE
       782 fast_start_parallel_rollback            2 LOW                            TRUE
       782 fast_start_parallel_rollback            3 HIGH                           FALSE
SQL> select * from V$PARAMETER_VALID_VALUES where name like '%statistics_level%';

       NUM NAME                              ORDINAL VALUE                          ISDEFAULT
---------- ------------------------------ ---------- ------------------------------ ----------
      1182 statistics_level                        1 BASIC                          FALSE
      1182 statistics_level                        2 TYPICAL                        TRUE
      1182 statistics_level                        3 ALL                            FALSE
---------
Mon Aug 24 14:00:52 2015
Thread 1 advanced to log sequence 81771 (LGWR switch)
  Current log# 3 seq# 81771 mem# 0: /oralog/orcl/redo03.log
Mon Aug 24 14:47:49 2015
SMON: Restarting fast_start parallel rollback
Mon Aug 24 14:47:57 2015
ORA-00060: Deadlock detected. More info in file /u01/app/oracle/admin/orcl/udump/orcl_ora_27174.trc.
Mon Aug 24 14:47:57 2015
Thread 1 advanced to log sequence 81772 (LGWR switch)
Mon Aug 24 14:47:57 2015
Thread 1 advanced to log sequence 81772 (LGWR switch)
  Current log# 1 seq# 81772 mem# 0: /oralog/orcl/redo01.log
Mon Aug 24 14:48:09 2015
Thread 1 advanced to log sequence 81773 (LGWR switch)
  Current log# 2 seq# 81773 mem# 0: /oralog/orcl/redo02.log
Thread 1 cannot allocate new log, sequence 81774
Checkpoint not complete
  Current log# 2 seq# 81773 mem# 0: /oralog/orcl/redo02.log
Mon Aug 24 14:48:44 2015
Thread 1 advanced to log sequence 81774 (LGWR switch)
  Current log# 3 seq# 81774 mem# 0: /oralog/orcl/redo03.log
Mon Aug 24 14:48:55 2015
Thread 1 cannot allocate new log, sequence 81775
Checkpoint not complete
  Current log# 3 seq# 81774 mem# 0: /oralog/orcl/redo03.log

版权声明：本文为博主原创文章，未经博主允许不得转载。

【MongoDB】-MongoVUE增删改查使用说明
一、插入
1）右键点击集合名-左键点击InsertDocument 


2）在弹出的对话框里输入Json格式的数据，点击Insert完成插入。 


二、查询
1）选中要查询的集合，点击find； 
 
或点击工具栏中的find； 


2）查询界面包括四个区域 


{Find}区：输入查询条件的地方，查询语句区分大小写； 
格式为：{“sendId”:”000101”}，表示查询sendId=000101的记录； 
查询条件包含and时，格式为：{“sendId”:”000101”,”operParam5”:”vfre”} 
查询条件包含or时，格式为：{$or:[{“sendId”:”000101”},{“sendId”:”1234567890”}]} 
查询条件中包含like时，格式为： 
operParam like ‘%set%’，{“operParam”:new RegExp(“.set.“)}
{Fields}区：设置需要查询的字段的地方，不填默认显示所有字段；Fields区的格式如：{“id”:”“,”sendId”:”“,”appId”:”“}。
{Sort}区：设置排序方式的区域； 
Sort区的格式如：{“id”:1}或{“id”:-1}，1表示按照id升序排序，-1表示按照id降序排序。
Limit用来设置返回多少条记录，Skip用来设置跳过多少条记录。

三、删除
选中要操作的集合，点击Remove进入删除面板，输入要删除数据的查询条件，点击Remove，在弹出的提示框中确认删除即可。

四、修改
选中要操作的集合，点击Update进入修改面板； 


左侧输入查询条件，右侧输入要更新的字段名称和值； 
格式如：｛$set:{“sendId”:”000102”}｝

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

leveldb学习：Memtable和Varint
Memtable：
Memtable是leveldb数据在内存中的存储形式，写操作的数据都会先写到memtable中，memtable的size有限制最大值（write_buffer_size）。memtable的底层数据结构是skiplist，class memtable完成的只是key-value的打包和调用底层跳表的接口。  
首先看看Memtable的成员变量： 

//实例化跳表模板
  typedef SkipList<const char*, KeyComparator> Table;
  //字符大小比较器
  KeyComparator comparator_;
  //memtable的引用计数
  int refs_;
  //内存池
  Arena arena_;
  //底层跳表
  Table table_;
由于memtable只是一个配接器，entry的插入操作交由底层的skiplist接口完成，关于leveldb的skiplist实现请看我写的另一篇博客。memtable真正完成的任务是打包key_value数据。memtable中存储的数据格式如下图：

首先把key、SequenceNumber、valueType打包成InternalKey。注：SequenceNumber是leveldb的不同版本，每次更新（put/delete）操作都会产生新的版本；valueType是区分entry是真实的KV数据还是删除操作，valueType是delete型的数据表示是要删除的数据，leveldb先记录，在后台的compact线程中完成真实的删除工作。存储时，SequenceNumber占56bits，valueType占8bits，两者共同占64bits（uint_64t）。
打包除了有key(InternalKey)和value，还加入了key、value的长度信息，而两者均是自定义的Varint（变长整型）数据，需要完成把int型转化为Varint型。

参见MemTable::Add函数：
void MemTable::Add(SequenceNumber s, ValueType type,
                   const Slice& key,
                   const Slice& value) {
  // Format of an entry is concatenation of:
  //  key_size     : varint32 of internal_key.size()
  //  key bytes    : char[internal_key.size()]
  //  value_size   : varint32 of value.size()
  //  value bytes  : char[value.size()]
  size_t key_size = key.size();
  size_t val_size = value.size();
  //sequencenumber和valuetype另占8字节
  size_t internal_key_size = key_size + 8;
  //插入的entry打包后的长度
  //VarintLength(int)计算int转化为varint所要字节
  const size_t encoded_len =
      VarintLength(internal_key_size) + internal_key_size +
      VarintLength(val_size) + val_size;
  //分配内存
  char* buf = arena_.Allocate(encoded_len);
  //EncodeVarint32()完成varint型转化工作
  char* p = EncodeVarint32(buf, internal_key_size);
  //写入内存
  memcpy(p, key.data(), key_size);
  p += key_size;
  //把sequencenumber和valuetype写入内存
  EncodeFixed64(p, (s << 8) | type);
  p += 8;
  //把valuesize和value写入内存
  p = EncodeVarint32(p, val_size);
  memcpy(p, value.data(), val_size);
  assert((p + val_size) - buf == encoded_len);
  //把char指针插入跳表
  table_.Insert(buf);
}
在bool MemTable::Get( )就是查找操作
bool MemTable::Get(const LookupKey& key, std::string* value, Status* s) {
  //LookupKey是leveldb为了在memtable/sstable查找方便，为key包装的类型
  //调用成员函数memtable_key可以返回在memtable中的key格式
  Slice memkey = key.memtable_key();
  //利用skiplist的专属迭代器查找key
  Table::Iterator iter(&table_);
  iter.Seek(memkey.data());
  if (iter.Valid()) {
    // entry format is:
    //    klength  varint32
    //    userkey  char[klength]
    //    tag      uint64
    //    vlength  varint32
    //    value    char[vlength]
    // Check that it belongs to same user key.  We do not check the
    // sequence number since the Seek() call above should have skipped
    // all entries with overly large sequence numbers.
    const char* entry = iter.key();
    uint32_t key_length;
    //找到了再把memtable中记录的userkey和查找对象做一次检查，要求相等
    const char* key_ptr = GetVarint32Ptr(entry, entry+5, &key_length);
    if (comparator_.comparator.user_comparator()->Compare(
            Slice(key_ptr, key_length - 8),
            key.user_key()) == 0) {
      // Correct user key
      //从memtable中的entry提取valuetype 
      //valuetype = kTypeValue，是真实的数据，查找成功
      //valuetype = kTypeDeletion，是要删除的数据，并非不代表数据真实存在
      const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8);
      switch (static_cast<ValueType>(tag & 0xff)) {
        case kTypeValue: {
          Slice v = GetLengthPrefixedSlice(key_ptr + key_length);
          value->assign(v.data(), v.size());
          return true;
        }
        case kTypeDeletion:
          *s = Status::NotFound(Slice());
          return true;
      }
    }
  }
  return false;
}
GetVarint32Ptr函数就是varint型编码的逆过程，解码。
关于Varint（变长整型）
Varint是一种紧凑的表示数字的方法。Varint中的每个byte的最高位bit有特殊的含义，如果该位为 1，表示后续的byte也是该数字的一部分，如果该位为0，则结束。其他的7个bit都用来表示数字。因此小于128的int都可以用一个byte表示。大于 128的数字，比如300，会用两个字节来表示：1010 1100 0000 0010，而4字节的int型最多需要5字节用varint表示。  
有关varint的编码和解码部分实现都在coding.cc中。
int VarintLength(uint64_t v) {
  int len = 1;
  while (v >= 128) {
    v >>= 7;
    len++;
  }
  return len;
}
编码：
char* EncodeVarint32(char* dst, uint32_t v) {
  // Operate on characters as unsigneds
  unsigned char* ptr = reinterpret_cast<unsigned char*>(dst);
  static const int B = 128;
  if (v < (1<<7)) {
    *(ptr++) = v;
  } else if (v < (1<<14)) {
    *(ptr++) = v | B;
    *(ptr++) = v>>7;
  } else if (v < (1<<21)) {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = v>>14;
  } else if (v < (1<<28)) {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = (v>>14) | B;
    *(ptr++) = v>>21;
  } else {
    *(ptr++) = v | B;
    *(ptr++) = (v>>7) | B;
    *(ptr++) = (v>>14) | B;
    *(ptr++) = (v>>21) | B;
    *(ptr++) = v>>28;
  }
  return reinterpret_cast<char*>(ptr);
}
解码：
inline const char* GetVarint32Ptr(const char* p,
                                  const char* limit,
                                  uint32_t* value) {
  if (p < limit) {
    uint32_t result = *(reinterpret_cast<const unsigned char*>(p));
    if ((result & 128) == 0) {
      *value = result;
      return p + 1;
    }
  }
  return GetVarint32PtrFallback(p, limit, value);
}


const char* GetVarint32PtrFallback(const char* p,
                                   const char* limit,
                                   uint32_t* value) {
  uint32_t result = 0;
  for (uint32_t shift = 0; shift <= 28 && p < limit; shift += 7) {
    uint32_t byte = *(reinterpret_cast<const unsigned char*>(p));
    p++;
    if (byte & 128) {
      // More bytes are present
      result |= ((byte & 127) << shift);
    } else {
      result |= (byte << shift);
      *value = result;
      return reinterpret_cast<const char*>(p);
    }
  }
  return NULL;
}
没什么可说的，用心体会里面的移位操作。
看完32位int转varint，来看看进阶版64位int的编码和解码，里面采用了循环结构
char* EncodeVarint64(char* dst, uint64_t v) {
  static const int B = 128;
  unsigned char* ptr = reinterpret_cast<unsigned char*>(dst);
  while (v >= B) {
    *(ptr++) = (v & (B-1)) | B;
    v >>= 7;
  }
  *(ptr++) = static_cast<unsigned char>(v);
  return reinterpret_cast<char*>(ptr);
}
const char* GetVarint64Ptr(const char* p, const char* limit, uint64_t* value) {
  uint64_t result = 0;
  for (uint32_t shift = 0; shift <= 63 && p < limit; shift += 7) {
    uint64_t byte = *(reinterpret_cast<const unsigned char*>(p));
    p++;
    if (byte & 128) {
      // More bytes are present
      result |= ((byte & 127) << shift);
    } else {
      result |= (byte << shift);
      *value = result;
      return reinterpret_cast<const char*>(p);
    }
  }
  return NULL;
}

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【linux】ubuntu 14.04下安装mysql 5.5
1.ubuntu下安装mysql需要一些命令
(1)sudo apt-get install mysql-server
安装过程中会出现如下窗口，需要用户输入相关的数据库密码：

（2）sudo apt-get isntall mysql-client
（3）sudo apt-get install libmysqlclient-dev
安装完成之后，可以用下面的命令检查是否安装成功
sudo netstat -tap | grep mysql

通过上述命令检查之后，如果看到有mysql 的socket处于 listen 状态则表示安装成功。
　tcp        0      0 localhost:mysql         *:*                     LISTEN      28385/mysqld  


2.登陆mysql可以输入如下命令：
mysql -u root -p

show databases;
show tables;


3.补充一些安装过程中的小知识
1. 删除mysql


  sudo apt-get autoremove --purge mysql-server-5.0

  sudo apt-get remove mysql-server

  sudo apt-get autoremove mysql-server

  sudo apt-get remove mysql-common (非常重要)

上面的其实有一些是多余的，建议还是按照顺序执行一遍

2. 清理残留数据

dpkg -l |grep ^rc|awk'{print $2}' |sudoxargs
 dpkg -P 

3.ubuntu下启动/停止/重启 mysql
   3.1使用service：

   sudo service mysql stop
   sudo service mysql start
   sudo service mysql restart


   3.2使用 mysqld 脚本启动

    sudo /etc/init.d/mysql stop
    sudo /etc/init.d/mysql start
    sudo /etc/init.d/mysql restart

重启提示错误如下：
Rather than invoking init scripts through /etc/init.d, use the service(8) utility, e.g. service mysql start...
其实已经告诉怎么做了：用sudo service mysql start启动即可












by：Alice















  


版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（三）——表的复杂查询
1、数据分组——max/min/avg/sum/count


select  avg(字段名)，sum(字段名)  from  表名select  count(*)  from  表名select  字段1，字段2  from  表名  where  字段=（select  avg(字段名)  from  表名）
注：


SQL语句执行，从右向左。select语句优化原则：尽量把可以减少结果集的条件放在右边。



2、group  by和having子句（having与group  by结合使用，对分组后的结果进行筛选）



group  by 用与对查询的结果分组统计having子句用来限制分组显示结果select  字段  from  表名  group  by  字段名


3、对数据分组的总结



分组函数只能出现在选择列表、having、order  by 子句中如果在select语句中同时出现包含有group  by，having，order  by，那么顺序为group  by，having，order  by在选择列中，如果有列、表达式和分组函数，那么这些列和表达式必须有一个出现在group  by 子句中，否则就会出错


版权声明：本文为博主原创文章，未经博主允许不得转载。

JDBC 批量处理
必要的文字描述：
当需要成批插入或者更新记录时。可以采用Java的批量更新机制，
这一机制允许多条语句一次性提交给数据库批量处理。通常情况下比单独提交处理更有效率；
JDBC的批量处理语句包括下面两个方法：
addBatch(String)：添加需要批量处理的SQL语句或是参数；
executeBatch（）；执行批量处理语句；
通常我们会遇到两种批量执行SQL语句的情况：
多条SQL语句的批量处理；
一个SQL语句的批量传参；
简洁的代码：
    /** 
     * <一句话功能简述>批量处理插入10000条数据
     * <功能详细描述>
     * @see [类、类#方法、类#成员]
     */
    public void update()
    {
        String sql = "insert into t_emp13 (first_name,salary) values (?,?)";
        Connection conn = null;
        PreparedStatement preparedStatement = null;
        ResultSet result = null;
        
        long begin = System.currentTimeMillis();
        
        try
        {
            conn = getConn();
            conn.setAutoCommit(false);
            preparedStatement = conn.prepareStatement(sql);
            
            // 将参数放到preparedStatement的未知参数中
            
            for (int i = 0; i < 10000; i++)
            {
                preparedStatement.setString(1, "name_" + i);
                preparedStatement.setObject(2, i);
                // 积攒SQl
                preparedStatement.addBatch();
                // 当积攒到一定程度 就统一执行一次，并且清空积攒的sql
                if (0 == (i + 1) % 300)
                {
                    preparedStatement.executeBatch();
                    preparedStatement.clearBatch();
                }
            }
            
            if (0 != 10000 % 300)
            {
                preparedStatement.executeBatch();
                preparedStatement.clearBatch();
            }
            
            conn.commit();
            
            long end = System.currentTimeMillis();
            System.out.println(begin - end);
        }
        catch (Exception e)
        {
            e.printStackTrace();
        }
        finally
        {
            releaseSource(preparedStatement, conn, result);
        }
        
    }
    

版权声明：本文为博主原创文章，未经博主允许不得转载。

[置顶]
        Eclipse 下的 Hibernate jbosstools 的创建、配置、测试详解
PS：文中用的都是Eclipse配置中对应的英文提示，防止配置时找不到入口。
一、前置条件 
1.Eclipse 4.4.2 
下载地址：Eclipse官网 http://download.eclipse.org/eclipse/downloads/ 


2.hibernate-release-4.3.11.Final 
下载地址：Hibernate官网 http://hibernate.org/orm/ 

3.jbosstools-4.2.3.Final_2015-03-26_23-05-30-B264-updatesite-hibernatetools 
下载地址：jbosstools官网 http://tools.jboss.org/downloads/jbosstools/luna/4.2.3.Final.html 

4.mysql 5.6 
下载地址：mysql官网 http://www.mysql.com/ 

5.common-logging.jar Hibernate内部记录日志jar包 
下载地址：common-logging.jar http://download.csdn.net/detail/mimica/9035949
6.mysql-connector-java-5.1.7 
下载地址：mysql-connector-java-5.1.7.zip 
7.Navicat for mysql 10.1.7  企业版 
下载地址：暂不提供，网上搜索即可，最好是破解版的，方便操作，其他的mysql数据库操作客户端亦可，根据个人喜好。
二、安装 
1.Eclipse 4.4.2直接解压即可。 
2.可先将hibernate-release-4.3.11.Final解压至Eclipse 4.4.2的plugins目录，为后面导入库做准备。 
3.打开Eclipse的软件安装界面，如下所示(按照提示进行安装)： 
 

三、使用Hibernate操作mysql数据库表结构 
1.创建测试数据库test(略)。 
2.创建测试表结构user/news，暂不需要插入任何数据(略)。 
3.新建Java project工程，如下所示： 
 

4.新建User Libraries 
 

5.给Java project添加User Libraries，如下所示： 
 
 
 
 

6.创建Hibernate配置文件,使用默认hibernate.cfg.xml 
 
最终我将hibernate.cfg.xml文件配置为如下格式。参数的作用各不相同，依据自己的喜好进行配置。如下所示：
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE hibernate-configuration PUBLIC "-//Hibernate/Hibernate Configuration DTD 3.0//EN"
                                         "http://hibernate.sourceforge.net/hibernate-configuration-3.0.dtd">
<hibernate-configuration>
 <session-factory>
  <!-- <property name="hibernate.connection.datasource">java:comp/env/jdbc/dstest</property> -->
  <property name="hibernate.connection.driver_class">com.mysql.jdbc.Driver</property>
  <property name="hibernate.connection.url">jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8</property>
  <property name="hibernate.connection.username">[你登录mysql数据库的用户名]</property>
  <property name="hibernate.connection.password">[你登录mysql数据库的密码]</property>
  <property name="hibernate.dialect">org.hibernate.dialect.MySQL5InnoDBDialect</property>
  <property name="hibernate.show_sql">true</property>
  <property name="hibernate.hbm2ddl.auto">update</property>
  <property name="hibernate.c3p0.max_size">200</property>
  <property name="hibernate.c3p0.min_size">2</property>
  <property name="hibernate.c3p0.timeout">1800</property>
  <property name="hibernate.c3p0.max_statements">50</property>

  <mapping resource="db/dao/News.hbm.xml"></mapping>
  <mapping resource="db/dao/User.hbm.xml"></mapping>
 </session-factory>
</hibernate-configuration>
7.配置Hibernate configuration 
 
 

8.新建Hibernate 逆向POJO实体类的反向配置文件 

9.新建Hibernate Configuration properties添加当前工程的Console Configuration文件 
1) 找到显示Hibernate Configuration视图的选项 

2) 输入关键字选择Hibernate Configuration视图 

3) 显示Hibernate Configuration视图 

4) Hibernate Configuration properties添加当前工程的Console Configuration文件 

5) 新建成功的反向工程配置文件 

10.新增需要反向的工程 
1) 选择刚才的反向配置文件的configuration properties文件 

2) 添加需要反向的表结构 
 

3) 填写反向表结构的信息 
 

11.生成反向类文件（如果类文件提示错误，请查看包名是否有错） 
注：如果工具栏里没有显示这个按钮，可以 window -> Customize Perspective，切换到Command Groups Availablity标签页，把Hibernate Code Generation给勾选上 
1) 设置Hibernate code Generation Configurations配置信息->Main 

2) 设置Hibernate code Generation Configurations配置信息->Exporters 

3) 设置Hibernate code Generation Configurations配置信息->Refresh 

4) 生成的类文件 

12.生成映射文件 
1) 生成映射文件 

2) 点击下一步 

3) 下一步 

4) 已经生成映射配置文件User.hbm.xml 

package db.dao;

// default package
// Generated 2015-8-24 18:39:26 by Hibernate Tools 4.3.1

/**
 * User generated by hbm2java
 */
public class User implements java.io.Serializable {

    private Integer id;
    private String name;

    public User() {
    }

    public User(String name) {
        this.name = name;
    }

    public Integer getId() {
        return this.id;
    }

    public void setId(Integer id) {
        this.id = id;
    }

    public String getName() {
        return this.name;
    }

    public void setName(String name) {
        this.name = name;
    }

}
添加如下代码，完成POJO的对应关系
@Entity
@Table(name="user")
public class User implements java.io.Serializable 
{
    @Id @Column(name="id")
    @GeneratedValue(strategy=GenerationType.IDENTITY)
    ......(以下代码略去)
}
5) 生成的User.hbm.xml配置文件源码
<?xml version="1.0"?>
<!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN"
"http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd">
<!-- Generated 2015-8-24 18:49:15 by Hibernate Tools 3.4.0.CR1 -->
<hibernate-mapping>
    <class name="db.dao.User" table="USER">
        <id name="id" type="java.lang.Integer">
            <column name="ID" />
            <generator class="assigned" />
        </id>
        <property name="name" type="java.lang.String">
            <column name="NAME" />
        </property>
    </class>
</hibernate-mapping>
13.添加类映射配置文件User.hbm.xml 
 

五、测试POJO类 
1.新建User.java(POJO)的测试文件 

2.编写User.java(POJO)的测试文件
package db.dao;

import org.hibernate.Session;
import org.hibernate.SessionFactory;
import org.hibernate.Transaction;
import org.hibernate.cfg.Configuration;
import org.hibernate.service.ServiceRegistry;
import org.hibernate.service.ServiceRegistryBuilder;

public class UserMgr {

    public static void main(String[] args) 
    {
        // Configuation实例代表了应用程序到SQL数据库的配置信息
        // 加载默认的hibernate.cfg.xml文件,若设置config()中的参数，则表示加载其他的配置文件
        Configuration conf = new Configuration().configure();

        @SuppressWarnings("deprecation")
        ServiceRegistry serviceReg = new ServiceRegistryBuilder()
                .applySettings(conf.getProperties()).buildServiceRegistry();

        // 以Configuration实例创建不可变SessionFactory实例
        SessionFactory sessionFac = conf.buildSessionFactory(serviceReg);

        // 创建Session
        Session session = sessionFac.openSession();

        // 开始事务
        Transaction trans = session.beginTransaction();

        // 创建消息对象
        User user = new User();

        user.setName("测试用户名1");

        // 保存信息
        session.save(user);

        // 提交事务
        trans.commit();

        // 关闭session
        session.close();
        sessionFac.close();
    }
}
3.依据Eclipse开启调试即可。 

4.查看数据库是否已经插入对应的数据。 
 

六、常见问题 
1.Error parsing JNDI name [] 
请配置JNDI配置信息（如果没有配置则删除hibernate.cfg.xml中的name=”“属性）
2.映射文件/资源找不到 
在hibernate.cfg.xml中添加如下对应的属性，这里是指配置文件的相对路径。 

3.ids for this class must be manually assigned before calling save(): db.dao.User 
如果id字段在UserMgr.hbm.xml文件中被设置为assigned同时在数据库又是自增字段时，可以尝试手动通过setter设置字段值。 

4.数据写入成功后为？？？？？？的解决方案 
 
在hibernate.cfg.xml文件中添加如下配置： 
?useUnicode=true&characterEncoding=UTF-8 

5.hibernate.cfg.xml中c3p0的常规配置
<property name="hibernate.c3p0.timeout">1800</property>
<property name="hibernate.c3p0.max_size">30</property>
<property name="hibernate.c3p0.min_size">1</property>
<property name="hibernate.c3p0.max_statements">100</property>
<property name="hibernate.c3p0.idle_test_period">3000</property>
<property name="hibernate.c3p0.acquire_increment">2</property>
<property name="hibernate.c3p0.validate">true</property>

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-线性表的链式存储结构
引言：由于线性表的顺序存储结构在插入和删除时需要大量移动数据元素，从而引入线性表的链式存储结构。

线性表的链式存储结构：用一组任意的存储单元（可以连续也可以不连续）存储线性表的数据元素。
为了表示数据元素ai和其直接后继ai+1之间的逻辑关系，对ai来说，除了存储其本身的数据信息外，还需要存储其直接后继的存储位置。这两部分信息组成数据元素ai的存储映像（结点）。它包含两个域：其中存储数据元素信息的域称为数据域；存储直接后继存储位置的域称为指针域。
n个结点链接成一个链表，称为线性链表，由于此链表中的每个结点只包含单个指针域，故又称作单链表。

头结点：在单链表的第一个结点前附设一个结点，头结点的指针域指向第一个结点，数据域可以不存任何信息，也可以存储线性表的长度等附加信息。
读取操作：由于任何两个元素的存储位置之间没有固定的联系，元素的存储位置只能从其直接前驱结点的指针域中得到。 
假设p是指向第i个数据元素的指针，则p->next是指向第i+1个数据元素的指针。换句话说：p->data=ai,p->next->data=ai+1，由此，在单链表中取得第i个数据元素必须从头指针出发寻找，因此，单链表是非随机存取的存储结构。
插入操作：假设在线性表的两个数据元素a，b中插入数据元素x，已知p是指向结点a的指针。 
为了插入数据元素x，首先要生成数据域为x的结点，s为指向结点x的指针。将x插入单链表的过程可以描述为： 
s->next=p->next,p->next=s;
删除操作：假设在线性表的三个数据元素a，b，c中删除数据元素b，已知p是指向结点a的指针。 
从单链表中删除结点b的过程可以描述为： 
p->next=p->next-next;
单链表中读取、插入和删除操作的时间复杂度都是O（n），这是因为读取第i个结点，删除第i个结点，在第i个结点前插入新结点都必须先找到第i-1个结点。

循环链表：链表中最后一个结点的指针域指向头结点，整个链表形成一个环。由此，从表中任一结点出发均可找到表中其他结点。
在单链表中nextElem的执行时间为O（1），priorElem的执行时间是O（n）,为了克服单链表单向性的 缺点，引入双向链表。 
双向链表：在双向链表的结点中有两个指针域，一个指向直接后继，另一个指向直接前驱。 
d->next->prior=d=d->prior-next;

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（八）——Statement与PreparedStatement的区别，JDBC方式操作数据库
1、Statement与PreparedStatement的区别
1）都可用于  把sql语句从java程序中发送到制定数据库，并执行sql语句。
2）区别


直接使用Statement，驱动程序一般不会对sql语句做处理，而直接交给数据库。使用PreparedStatement，形成预编译的过程，并且会对语句做字符集的转换（至少在sql  server中如此）。好处：对于多次重复执行的语句，PreparedStatement效率更高，适合批量（batch），且解决系统的本地化问题。PreparedStatement可有效防止危险字符的注入，即sql注入问题。但要求用“？”赋值方式才可以。"?"可解决注入漏洞问题

ps=ct.PreparedStatement("select  *  from  dept  where  deptno=?  and  loc=?");ps.setInt(1,20);ps.setString(2,"dallas");




2、JDBC方式操作数据
1）定义对象


PreparedStatement  ps=null;Connection  ct=null;ResultSet  rs=null;
2)初始化


加载驱动

Class.forName("com.microsoft.sqlserver.jdbc.SQLServerDriver")
得到连接

ct=DriverManager.getConnection("jdbc:sqlserver://127.0.0.1:1433;database=数据库名","用户名","密码")127.0.0.1表示要连接的数据库的IP，此处表示本地1433表示sqlserver的默认端口（一共65535个端口）oracle：1521，mysql:3306
创建火箭车

ps=ct.PreparedStatement("Select  *  from  emp");
执行

rs=ps.executeQuery();rs=ps.executeUpdate();
循环读取

while(rs.next()){  }rs指向结果集的第一条的前一条，一定要rs.next()一下。如果取值按编号取，则需要一一对应。如果取值按猎命去，则顺序灵活。





3、作业相关问题
1）job<>'manager'：不等于
2）oracle中有一个可以获得每个月的最后一天的方法
3）sql  server：DATEDIFF(datapart,startpart,endpart)
4）upper len substring
     以首字母大写的方式显示员工姓名
     select  upper(substring(ename,1,1))+lower(substring(ename,2,len(ename)))  from  emp
5）len(字段名)：字段长度
6）显示所有员工姓名的前三个字符
     select  substring(ename,1,3)  from  emp
7）replace(字段，‘被替代’，‘替换’)
8）datepart(年/月/日...，字段名)

版权声明：本文为博主原创文章，未经博主允许不得转载。

JDBC ORACLE BLOB处理

LOB，即Large Objects（大对象），是用来存储大量的二进制和文本数据的一种数据类型（一个LOB字段可存储可多达4GB的数据）。
LOB 分为两种类型：内部LOB和外部LOB。
    内部LOB将数据以字节流的形式存储在数据库的内部。因而，内部LOB的许多操作都可以参与事务，
也可以像处理普通数据一样对其进行备份和恢复操作。Oracle支持三种类型的内部LOB：
BLOB（二进制数据）  
CLOB（单字节字符数据） 
NCLOB（多字节字符数据）。
CLOB和NCLOB类型适用于存储超长的文本数据，BLOB字段适用于存储大量的二进制数据，如图像、视频、音频，文件等。


目前只支持一种外部LOB类型，即BFILE类型。在数据库内，该类型仅存储数据在操作系统中的位置信息，
而数据的实体以外部文件的形式存在于操作系统的文件系统中。因而，该类型所表示的数据是只读的，不参与事务。
该类型可帮助用户管理大量的由外部程序访问的文件.


使用JDBC来写入Blob型数据到Oracle中 :
Oracle的Blob字段比long字段的性能要好，可以用来保存如图片之类的二进制数据。 
Oracle的BLOB字段由两部分组成：数据（值）和指向数据的指针（定位器）。尽管值与
 表自身一起存储，但是一个BLOB列并不包含值，仅有它的定位指针。为了使用大对象，程序必须声明定位器类型的本地变量。
当Oracle内部LOB被创建时，定位器被存放在列中，值被存放在LOB段中，LOB段是在数据库内部表的一部分。
因为Blob自身有一个cursor，当写入Blob字段必须使用指针（定位器）对Blob进行操作，因而在写入Blob之前，
 必须获得指针（定位器）才能进行写入
如何获得Blob的指针（定位器） ：需要先插入一个empty的blob，这将创建一个blob的指针，然后再把这个empty
 的blob的指针查询出来，这样通过两步操作，就获得了blob的指针，可以真正的写入blob数据了。 
具体步骤：
1、插入空blob insert into javatest(name,content) values(?,empty_blob()); 


2、获得blob的cursor select content from javatest where name= ? for update; 
  注意:  须加for update，锁定该行，直至该行被修改完毕，保证不产生并发冲突。

3、利用 io，和获取到的cursor往数据库写数据流

案例：以下案例实现插入一张图片，和获取有一张图片的查询结果； /*
 * 文件名：BlobTest.java
 * 版权：Copyright by www.huawei.com
 * 描述：
 * 修改人：Cuigaochong
 * 修改时间：2015-8-25
 * 跟踪单号：
 * 修改单号：
 * 修改内容：
 */

package com.jdbc.cgc.blob;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

import oracle.sql.BLOB;

import org.junit.Test;

/**
 * <一句话功能简述> <功能详细描述>
 * 
 * @author 姓名 工号
 * @version [版本号, 2015-8-25]
 * @see [相关类/方法]
 * @since [产品/模块版本]
 */
public class BlobTest
{
    
    /**
     * <一句话功能简述>测试方法，插入一条数据，数据中有一个列是Blob <功能详细描述>
     * 
     * @throws FileNotFoundException
     * @see [类、类#方法、类#成员]
     */
    @Test
    public void test00()
        throws FileNotFoundException
    {
        // 注意: 对于empty_blob()应放在SQL语句中直接赋值, 使用预置语句的方式赋值无法实现.
        String sql = "insert into t_emp13(first_name,salary,picture) values (?,?,Empty_BLOB())";
        // 查询Blob 注意进行 行锁定 后边的for update
        String sqlQuery = "select picture from t_emp13 where first_name = ? and salary = ? for update";
        updateBlob(sql, sqlQuery, "QQA122", 1233);
    }
    
    /**
     * <一句话功能简述>更新有blob的数据库表 <功能详细描述>
     * 
     * @param sqlInsert
     * @param sqlQuery
     * @param args
     * @see [类、类#方法、类#成员]
     */
    public void updateBlob(String sqlInsert, String sqlQuery, Object... args)
    {
        Connection conn = null;
        PreparedStatement prepareStatement = null;
        ResultSet rs = null;
        
        OutputStream os = null;
        FileInputStream fis = null;
        try
        {
            // 现在表中插入空的Blob
            conn = getConn();
            // 事物处理前，取消Connection的默认提交行为
            conn.setAutoCommit(false);
            prepareStatement = conn.prepareStatement(sqlInsert);
            
            for (int i = 0; i < args.length; i++)
            {
                prepareStatement.setObject(i + 1, args[i]);
            }
            prepareStatement.executeUpdate();
            
            BLOB blob = null;
            
            prepareStatement = conn.prepareStatement(sqlQuery);
            
            for (int i = 0; i < args.length; i++)
            {
                prepareStatement.setObject(i + 1, args[i]);
            }
            
            // prepareStatement.setString(1, "QQA");
            rs = prepareStatement.executeQuery();
            if (rs.next())
            {
                blob = (BLOB)rs.getBlob(1);
            }
            // 得到数据库的输出流
            os = blob.getBinaryOutputStream();
            // 得到要插入文件的输入流
            fis = new FileInputStream("Tulips.jpg");
            
            byte[] b = new byte[1024];
            int len;
            while (-1 != (len = fis.read(b)))
            {
                os.write(b, 0, len);
            }
            // 清空流的缓存
            os.flush();
            // 事物处理：如果事物处理成功则提交事物
            conn.commit();
        }
        catch (Exception e)
        {
            e.printStackTrace();
            try
            {
                // 事物处理：如果出现异常 则在catch块中回滚事物
                conn.rollback();
            }
            catch (SQLException e1)
            {
                e1.printStackTrace();
            }
        }
        finally
        {
            if (null != fis)
            {
                try
                {
                    fis.close();
                }
                catch (IOException e)
                {
                    e.printStackTrace();
                }
            }
            if (null != os)
            {
                try
                {
                    os.close();
                }
                catch (IOException e)
                {
                    e.printStackTrace();
                }
            }
            
            releaseSource(prepareStatement, conn, null);
        }
    }
    
    /**
     * <一句话功能简述>测试方法，查询有blob的表 <功能详细描述>
     * 
     * @see [类、类#方法、类#成员]
     */
    @Test
    public void test01()
    {
        String sql = "select picture from t_emp13 where first_name = ?";
        queryBlob(sql, "QQA122");
    }
    
    /**
     * <一句话功能简述>查询有Blob的表 方法 <功能详细描述>
     * 
     * @param sql
     * @param args
     * @see [类、类#方法、类#成员]
     */
    public void queryBlob(String sql, Object... args)
    {
        Connection conn = null;
        PreparedStatement prepareStatement = null;
        ResultSet rs = null;
        
        FileOutputStream fos = null;
        InputStream is = null;
        
        try
        {
            conn = getConn();
            // 事物处理前，取消Connection的默认提交行为
            conn.setAutoCommit(false);
            prepareStatement = conn.prepareStatement(sql);
            for (int i = 0; i < args.length; i++)
            {
                prepareStatement.setObject(i + 1, args[i]);
            }
            rs = prepareStatement.executeQuery();
            if (rs.next())
            {
                BLOB blob = (BLOB)rs.getBlob(1);
                is = blob.getBinaryStream();
                
                fos = new FileOutputStream(new File("test.png"));
                
                byte[] b = new byte[1024];
                int len;
                
                while (-1 != (len = is.read(b)))
                {
                    fos.write(b, 0, len);
                }
                fos.flush();
            }
            // 事物处理：如果事物处理成功则提交事物
            conn.commit();
            
        }
        catch (Exception e)
        {
            e.printStackTrace();
            try
            {
                // 事物处理：如果出现异常 则在catch块中回滚事物
                conn.rollback();
            }
            catch (SQLException e1)
            {
                e1.printStackTrace();
            }
        }
        finally
        {
            if (null != is)
            {
                try
                {
                    is.close();
                }
                catch (IOException e)
                {
                    e.printStackTrace();
                }
            }
            
            if (null != fos)
            {
                try
                {
                    fos.close();
                }
                catch (IOException e)
                {
                    e.printStackTrace();
                }
            }
            
            releaseSource(prepareStatement, conn, rs);
        }
    }
    
    /**
     * <一句话功能简述> 连接数据库 <功能详细描述>
     * 
     * @return
     * @throws Exception
     * @see [类、类#方法、类#成员]
     */
    public Connection getConn()
        throws Exception
    {
        String dirverName = null;
        String jdbcUrl = null;
        String user = null;
        String password = null;
        
        Properties propertoes = new Properties();
        InputStream is = getClass().getClassLoader().getResourceAsStream("jdbc.properties");
        propertoes.load(is);
        
        dirverName = propertoes.getProperty("driver");
        jdbcUrl = propertoes.getProperty("jdbcURL");
        user = propertoes.getProperty("user");
        password = propertoes.getProperty("password");
        
        Class.forName(dirverName);
        
        // 通过DriverManager的getConnection获取数据库连接
        Connection connection = DriverManager.getConnection(jdbcUrl, user, password);
        
        return connection;
    }
    
    /**
     * <一句话功能简述>释放数据库资源 <功能详细描述>
     * 
     * @param statement
     * @param conn
     * @param resultSet
     * @see [类、类#方法、类#成员]
     */
    public void releaseSource(Statement statement, Connection conn, ResultSet resultSet)
    {
        if (null != resultSet)
        {
            try
            {
                resultSet.close();
            }
            catch (SQLException e)
            {
                e.printStackTrace();
            }
        }
        
        if (null != statement)
        {
            try
            {
                statement.close();
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
        if (null != conn)
        {
            try
            {
                conn.close();
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
        }
    }
}


版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（七）——java 程序操作sql  server
1、crud(增删改查)介绍：create/retrieve/update/delete


2、JDBC介绍
1）JDBC（java database connectivity，java数据库连接）
2）是一种用于执行SQL语句的java  API,可为多种关系数据库提供统一访问。由一组用Java语言编写的类和接口组成。
3）JDBC为工具/数据库开发人员提供了一个标准的API，据此可构建更高级的工具和接口，是数据库开发人员能够用纯Java API编写数据库应用程序。
4）JDBC也是闪避表明。


3、JDBC驱动分类
1）JDBC-ODBC桥连接
2）本地协议纯Java驱动程序
3）网络协议纯Java驱动程序
4）本地API


4、JDBC的不足
1）JDBC在java语言层面实现了统一，但不同数据库仍有许多差异。
2）Hibernate（跨数据库操作）是针对JDBC的再封装，实现了对数据库操作更宽泛的统一和更好的可移植性。


5、JDBC-ODBC桥连接操作SQL Server
步骤：
1）开始——控制面板——管理工具——数据源（ODBC）——ODBC数据源管理器——用户DSN——添加——创建新数据源——SQL Server——名称——服务器“local”或“.”（均表示本地）——下一步——“使用Windows NT验证”——下一步——“更改默认的数据库为XXX”——完成——测试数据源


2）在程序中连接数据源（import  java.sql.*;）


加载驱动（把需要的驱动程序加入内存）

Class.forName("sun.jdbc.odbc.JdbcOdbcDriver")
得到连接（指定连接到哪个数据源，用户名，密码）

Connection  ct=DriverManager.getConnection("jdbc:odbc:数据库名","sa","密码")注：若为Windows验证，则不需要用户名和密码。
创建Statement或PreparedStatement





Statement主要用来发送sql语句到数据库Statement sm=ct.creatStatement();
执行（crud，创建database,备份，恢复）





int  i=sm.executeUpdate("insert into dept values('50','保安部','西永')");i表示成功添加的记录条数executeUpdate可cud操作
关闭资源

if(sm!=null) 

                         sm.close();
                    if(ct!=null)
                          ct.close();


6、补充：


删除一条记录

int i==sm.executeUpdate("delete  from  dept  where  dept=50");
查询

ResultSet  rs=sm.executeQuery("select  *  from dept");

                    while(rs.next(0){
                         int deptno=rs.getInt(1);
                         String dname=rs.getString(2);
                         String loc=rs.getString(3);
                    }





     rs指向结果集的第一行的前一行，循环取出。









版权声明：本文为博主原创文章，未经博主允许不得转载。

如何将含有byte数据项的结构存入MongoDb
我们知道MongoDb不支持byte(BsonType中根本没有定义byte), 但是在实际生产环境中数据结构（特别是远古时代的数据结构）往往包含byte数据项。 
这时候无法保存原有的数据结构，一般会另外创建一个Wrapper结构(Wrapper内部将byte转为int等MongoDb可以识别的类型),最后将Wrapper存入MongoDb。
无疑，这种方法并不优雅。 
（下面的方法实现于c#, MongoDb的驱动是2.0版本） 
我们希望能够让MongoDb自动将byte转化为int，这样所有问题就引刃而解了。 
用代码说话:
   class ByteSerializer :IBsonSerializer
    {

        public Type ValueType
        {
            get {
                return typeof(byte);
            }
        }

        public object Deserialize(BsonDeserializationContext context, BsonDeserializationArgs args)
        {
            var b = (byte)context.Reader.ReadInt32();
            return b;
        }

        public void Serialize(BsonSerializationContext context, BsonSerializationArgs args, object value)
        {
            context.Writer.WriteInt32((int)(byte)value);
        }
    }
上面的代码构建了一个针对byte的序列化类 
随后我们在全局部分对目标结构“MyClass” 操作。
     BsonClassMap.RegisterClassMap<MyClass>(cm =>
            {
                cm.AutoMap();
                cm.SetIdMember(cm.GetMemberMap(c => c.Id));               
                cm.GetMemberMap<byte>(o => o.ByteItem).SetSerializer(new ByteSerializer());
            });
完成这部份之后，以后就可以爽快的将数据结构上传到MongoDb了。 
其他类似情况，例如 underlying为byte的enum等等 
Have fun！
参考链接: 
http://stackoverflow.com/questions/19664394/mongodb-c-sharp-exception-cannot-deserialize-string-from-bsontype-int32

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

机房收费系统之上下机
总体思路导图：


上机：




上机思路：
输入卡号：判断是否注册，否的话提示信息。判断是否正在上机，是则提示 。断卡内余额是否充足，否，则提示信息，可联系操作员或者管理员到充值窗体对其进行充值。充足的话，则将数据导入到online表，更新。从online表中获得正在上机人数，窗体中显示。







下机：





下机思路：
判断卡号是否输入，否，提示！判断是否为数字，否，提示！从学生表中获得信息判断卡号是否存在，否，提示！判断卡号是否正在上机，否，提示没有上机！是，从online表中获得信息赋到窗体中，查询basicdata数据表信息，获得最少上机时间和准备时间，分别针对固定用户及临时用户进行金额的计算。其中若是上机时间在准备时间之内，上机时间和消费金额均为0，不计时不计费。当超过准备时间小于最少时间时按规定的最少时间计费（不管是多少，都按最少时间的收费）。当超过最少时间，则按照自己设定的消费方式进行计费，设置Timer事件，三分钟循环查看卡内余额是否小于设定的最低余额，只要消费到最低余额，便强制下机，更新Online,Student,Line表中相关信息。






版权声明：本文为博主原创文章，未经博主允许不得转载。

Postgresql主备同步流复制及主备切换、IP漂移
主备流复制环境搭建请参考http://blog.csdn.net/baiyinqiqi/article/details/47833811，参考文章介绍的是异步流复制环境的搭建，实际上同步模式只需要修改主库的几个参数即可实现。
主：192.168.3.201
从：192.168.3.202


postgresql 的几种流复制：
1+n异步流复制：1是指master，n是指slave。一个master可以有多个slave。
1+1+n同步加异步流复制：也就是说1个master与n个slave的流复制环境，在n个slave中有一个slave是同步模式，其它是异步模式。如果同步的slave挂掉，其它异步的slave会选举出一个slave切换到同步模式，整个流复制环境仍能够保证1+1+n的结构。
级联流复制：1个master A，一个slave B直连A，其它slave连B，从B获取数据。这种方式要9.2或更高版本支持。


这里流复制环境的搭建不再重复，仅介绍下如何将异步模式修改为同步模式：
修改master的postgresql.conf
synchronous_standby_names = '*'    # *=all，意思是所有slave都被允许以同步方式连接到master，但同一时间只能有一台slave是同步模式。另外可以指定slave，将值设置为slave的application_name即可。

synchronous_commit = on #这个参数控制是否等待wal日志buffer写入磁盘再返回用户事物状态信息。这个对性能影响还是比较大的，看业务实际情况可考虑关闭，在关键数据更新时在事物中将其暂时性打开，保证关键数据不会因意外停机而丢失。默认情况下是打开状态。同步流复制模式需要打开这个参数。
测试：
若使用同步流复制模式，那么master会等待同步slave返回事物状态后，才会完成当前事物。所以如果slave停掉，那么master的事物会一直等待下去。下面将slave 202数据库关掉，然后在master 201进行事物操作。
slave 202数据库停掉：


在master 201中进行数据更新，因为同步slave库已经停掉了，所以该事物一直等待：


查看进程，可以看到有一个insert waiting 进程：


如果将synchronous_commit = off 关掉，即使slave库关掉了，master的事物也不会出现等待的现象。


主备切换及IP漂移：
模拟环境：192.168.3.201 master 、192.168.3.202 slave  两台服务器设置对外ip 192.168.3.100。模拟201宕机，202从slave切换到master，同时100ip从201漂移到202。
设置漂移ip：一块网卡绑定多个ip网上有很多资料，可以去搜下。注意，master与slave的网卡都要绑定192.168.3.100。master是启用状态，slave是停用状态。
切换前IP配置情况：






切换前数据库状态：








在搭建流复制环境的时候，一般在master、与slave服务器上都要配置密码文件，以便将来master宕机后进行主备切换，这里就不赘述了。
开始模拟：
停掉master数据库：
[postgres@CentOS_201 ~]$ pg_ctl stop -m fast

停用master 的192.168.3.100 ip ：


启用slave的192.168.3.100 ip，完成ip漂移：


slave库进行切换：


查看slave状态，已经切换到production状态：




配置原master库recovery：
recovery_target_timeline = 'latest'

standby_mode = on

primary_conninfo = 'host=192.168.3.202 port=5432 user=rep'

启动原master，自动切换到standby模式：




ok！
主备切换完成，至于ip漂移，可以用其他服务器的客户端测试，比如，漂移前，master停用后，slave启用前，漂移完成后。
另外，postgresql在9.3增加了流复制协议自动拷贝时间线timeline的功能，本过程使用的是9.4。如果您在测试的时候，使用的是9.3以前的版本，可能需要手工拷贝时间线文件。
ip漂移可以使用第三方工具来做，比如keepalived等。





版权声明：本文为博主原创文章，未经博主允许不得转载。

SQL-Oracle游标
游标提供了一种从集合性质的结果集中提供出单条记录的手段，初始时指向首记录。

游标的种类 
静态游标、REF游标
静态游标：可以理解为一个数据快照，打开游标后的结果集是数据库表中数据的备份，数据不会对表的DML操作而改变。
①显式静态游标：是指在使用之前必须有明确的游标定义，这种游标的定义会关联数据查询语句，通常会返回一行或多行，打开游标后可以利用游标的位置对结果集进行检索，使之返回单一的行记录，用户可以操作该记录，关闭游标后就不能对结果集进行操作。
②隐式静态游标：和显式游标不同，它被PL/SQL自动管理，也被称为SQL游标。
显示游标的使用 
语法

cursor cursor_name[(parameter_name datatype,...)]
is 
    select_statement;
使用步骤：声明、打开、读取数据、关闭

①声明游标
declare cursor cursor_name is select_statement;
②打开游标（游标一旦被打开，结果就是静态的了）
open cursor_name;
③读取数据
读取数据要用到fetch，它可以吧游标指向位置的记录读取到pl/sql声明的变量中。
fetch cursor_name into record_name
④关闭游标
close cursor_name;

游标中简单的loop语句 
eg：
declare
    cursor test_cursor is select * from test1;
    test_id  test1.id%type;
    test_name test1.name%type;
    test_money test1.money%type;

begin
    open test_cursor;
    loop
        fetch test_cursor into test_id,test_name,test_money;
        exit when test_cursor%notfound;
        dbms_output.put_line('.....');
    end loop;
    close test_cursor;
end;
需要注意的是：使用fetch…into..提取数据的时候的单条提取，数据量较大时效率比较低。
使用fetch…bulk collect into 提取大数据量的游标数据 
eg:
declare
    cursor emp_cursor is 
       select * from emp;
    type emp_tab is table of emp%rowtype;
    emp_rd emp_tab;
begin
    open emp_cursor;
    loop
        fetch emp_cursor bulk collect into emp_rd limit 2;
        for i in 1...emp_rd.count
        loop
            dbms_output.put_line(......);
        end loop;
        exit when emp_cursor%notfound;
    end loop;
    close emp_cursor;
end;
利用cursor … for … loop 便利游标数据，使用简洁、方便 
eg：
declare
    cursor test_cursor is
       select * from test1;
begin
    for rec in test_cursor
    loop
        dbsm_output.put_line(.....);
    end loop;
end;
带参数的游标 
 eg：
declare
    test_id1 test.id%type := 1;
    test_id2 test.id%type := 2;
    cd_test test1%rowtype;
    cursor test_cursor(id1 number,id2 number)
       is select * from test1 where id in(id1,id2);
begin
    open test_cursor(test_id1,test_id2);
    loop
        fetch test_cursor into cd_test;
        exit when test_cursor%notfound;
           dbsm_output.put_line(...);
    end loop;
    close test_cursor;
end;

隐式游标 
隐式游标和显式游标有所差异，它显没有显式游标的课操作性，每当运行DQL或DML语句时，PL/SQL会打开一个隐式游标，隐式游标不受用户控制。 
①隐式游标由pl/sql自动管理 
②隐式游标的默认名称是SQL 
③DQL和DML语句产出隐式游标 
④隐式游标的属性值是指是最新执行的sql语句的。


            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Windows 下Oracle database 9i 64bit 只有 Windows Itanium 64bit
Windows 下Oracle database 9i 64bit 只有 Windows Itanium 64bit，没有Windows x86-64bit的
具体请见如下的certification：
 

版权声明：本文为博主原创文章，未经博主允许不得转载。

HBase之Memstore刷写
  HBase上Regionserver的内存分为两个部分，一部分作为Memstore，主要用来写；另外一部分作为BlockCache，主要用于读数据；这里主要介绍写数据的部分，即Memstore。
  当RegionServer(RS)收到写请求的时候(writerequest)，RS会将请求转至相应的Region。每一个Region都存储着一些列(a
 set of rows)。根据其列族的不同，将这些列数据存储在相应的列族中(Column Family，简写CF)。不同的CF中的数据存储在各自的HStore中，HStore由一个Memstore及一系列HFile组成。Memstore位于RS的主内存中，而HFiles被写入到HDFS中。当RS处理写请求的时候，数据首先写入到Memstore，然后当到达一定的阀值的时候，Memstore中的数据会被刷到HFile中。
  用到Memstore最主要的原因是：存储在HDFS上的数据需要按照row
 key 排序。而HDFS本身被设计为顺序读写(sequential reads/writes)，不允许修改。这样的话，HBase就不能够高效的写数据，因为要写入到HBase的数据不会被排序，这也就意味着没有为将来的检索优化。为了解决这个问题，HBase将最近接收到的数据缓存在内存中(in
 Memstore)，在持久化到HDFS之前完成排序，然后再快速的顺序写入HDFS。（这里参考：http://www.cnblogs.com/shitouer/archive/2013/02/05/configuring-hbase-memstore-what-you-should-know.html）
这里就留下个问题，MemStore何时刷写成HFile?
个人总结如下：
1.Region级别的触发刷写。
（1）hbase.hregion.memstore.flush.size
  单个region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。这里为什么是所有memsotre?因为一张表可能有多个CF，其对应的一个Region自然包含多个CF（即HStore），每个Store都有自己的memstore，这个配置值是所有的store的memstore的总和。当这个总和达到配置值时，即针对每个HSotre，都触发其Memstore，刷写成storefile(HFile的封装)文件。
（2）hbase.hstore.blockingStoreFiles 默认值：7
  说明：在flush时，当一个region中的Store（Coulmn
 Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。
  调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。这个值设置比较大，会增加客户端的负载处理能力（即影响读取性能），但是如果你的服务器一直处于一个高的水平，那说明你的机器已经达到性能瓶颈，需要其他方式解决。
（3）hbase.hregion.memstore.block.multiplier默认值：2
说明：当一个region里总的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size2倍时，block所有请求，遏制风险进一步扩大。
调优：这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase
 server OOM。
 
2.RegionServer全局性的触发刷写。
（1）hbase.regionserver.global.memstore.upperLimit
当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。
（2）hbase.regionserver.global.memstore.lowerLimit
同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为“**
 Flush thread woke up with memory above low water.”。
调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。
 
3. HLog (WAL)引起的regionserver全局性的触发刷写。
当数据被写入时会默认先写入Write-ahead Log(WAL)。WAL中包含了所有已经写入Memstore但还未Flush到HFile的更改(edits)。在Memstore中数据还没有持久化，当RegionSever宕掉的时候，可以使用WAL恢复数据。
当WAL(在HBase中成为HLog)变得很大的时候，在恢复的时候就需要很长的时间。因此，对WAL的大小也有一些限制，当达到这些限制的时候，就会触发Memstore的flush。Memstore
 flush会使WAL减少，因为数据持久化之后(写入到HFile)，就没有必要在WAL中再保存这些修改。有两个属性可以配置：
（1）hbase.regionserver.hlog.blocksize
（2）hbase.regionserver.maxlogs
WAL的最大值由hbase.regionserver.maxlogs*hbase.regionserver.hlog.blocksize
 (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。
提示：最好将hbase.regionserver.hlog.blocksize* hbase.regionserver.maxlogs设置为稍微大于hbase.regionserver.global.memstore.lowerLimit*
 HBASE_HEAPSIZE.
（这里参考：http://www.cnblogs.com/shitouer/archive/2013/02/05/configuring-hbase-memstore-what-you-should-know.html）

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-串
一、串的基本概念 
串（字符串）：是由零个或多个字符组成的有限序列，一般记为： 
s=‘a1a2…an’(n>=0) 
s是串名，单引号括起来的是串的值，ai(1<=i<=n)可以是字母、数字或其他字符。 
串中字符的数目n称为串的长度； 
长度为零的串称为空串； 
串中任意个连续的字符组成的子序列称为子串； 
包含子串的串相应的称为主串； 
字符在序列中的序号称为该字符在串中的位置； 
子串在主串中的位置以子串的第一个字符在主串中的位置来表示。

二、串的表示和实现 
1）定长顺序存储表示：用一组地址连续的存储单元存储串值的字符序列。 
2）堆分配存储表示：仍是用一组地址连续的存储单元存放串值字符序列，但它们的存储空间是在程序执行的过程中动态分配的。 
3）串的块链存储表示：用链表方式存储串值。 
用链表存储串值时，由于串中的数据元素是一个字符，每个结点可以存放一个字符，也可以存放多个字符。除头指针外还可附设一个尾指针指示链表中的最后一个结点，并给出当前串的长度。

三、串操作 
1）串联接Contact（&T，S1，S2）； 
2）求子串SubString（&Sub，S，pos，len）； 
3）模式匹配Index（S，T，pos）：子串的定位操作称作串的模式匹配。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

Oracle Instant Client（即时客户端） 安装与配置
一、下载
下载地址：http://www.oracle.com/technetwork/database/features/instant-client/index-097480.html
这是Oracle Instant Client的下载首页，有很多种版本可供下载。
但要注意第三方工具如：PL/SQL Developer和Toad的版本，32位的要对应32位的OracleInstant Client，不要因为系统是64位的就下载64位的，这个要注意。
 本文使用资源：Oracle instant client 32位(绿色版) 完美支持win7 64位系统

下载地址：http://download.csdn.net/detail/xuyongbeijing2008/9044301
二，配置
把下载的instantclient-basic-nt-11.2.0.2.0.zip压缩包解压，放到 C:\instantclient_11_2 目录下。
在“环境变量”的“系统变量”中增加：



ORACLE_HOME = C:\Oracle\instantclient_10_2TNS_ADMIN = C:\Oracle\instantclient_10_2NLS_LANG = SIMPLIFIED CHINESE_CHINA.ZHS16GBK

修改Path变量，在后面添加 C:\Oracle\instantclient_10_2
 
三，新建tnsnames.ora文件
在C:\instantclient_11_2 新建一个tnsnames.ora文件，增加自己的数据库别名配置。
示例如下：

ORCL=
  (DESCRIPTION=
    (ADDRESS=
      (PROTOCOL=TCP)
      (HOST=172.117.164.152)
      (PORT=1521)
    )
    (CONNECT_DATA=
      (SERVER=dedicated)
      (SERVICE_NAME=VPSADDB)
    )
  )



注意格式要排列好
主要改 = 前面的别名，Host为IP地址, SERVICE_NAME为数据库服务器的实例名。
 
四，卸载方法
“环境变量”中的“系统变量”中：



删除 ORACLE_HOME,TNS_ADMIN, NLS_LANG 三个变量,修改path变量，去掉C:\Oracle\instantclient_10_2目录删除C:\Oracle\instantclient_10_2目录

 
五，第三方工具使用
上面的任何一种客户端配置好后，都可以安装Toad 或者PL/SQL Developer 工具，不需要再额外进行任何设置，即可使用。



版权声明：本文为博主原创文章，未经博主允许不得转载。

postgresql  查看数据库连接数
查看所有连接的用户：
select * from pg_stat_activity;
查看连接总数：
select count(*) from pg_stat_activity;

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

JDBC ORACLE 数据库隔离级别

数据库的隔离级别：
于同时运行的多个事务, 当这些事务访问数据库中相同的数据时, 如果没有采取必要的隔离机制, 就会导致各种并发问题:
脏读: 对于两个事物 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段. 之后, 若 T2 回滚, 
         T1读取的内容就是临时且无效的.
不可重复读: 对于两个事物 T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了.
幻读: 对于两对个事物 T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行.
         之后, 如果 T1 再次读取同一个表, 就会多出几行.
数据库事务的隔离性: 数据库系统必须具有隔离并发运行各个事务的能力, 使它们不会相互影响, 避免各种并发问题. 
          一个事务与其他事务隔离的程度称为隔离级别. 数据库规定了多种事务隔离级别, 不同隔离级别对应不同的干扰程度,

隔离级别越高, 数据一致性就越好, 但并发性越弱.


JDBC Conntion类中的隔离级别
int TRANSACTION_NONE    = 0;
//读未提交数据
int TRANSACTION_READ_UNCOMMITTED = 1;
//读已提交数据
int TRANSACTION_READ_COMMITTED   = 2;
//可重复读数据：确保事物可以多次从一个字段中读取相同的值，在事物持续期间禁止其他事物对该事物进行更新，
//可以避免数据脏读和不可重复读，但幻读依然存在
int TRANSACTION_REPEATABLE_READ  = 4;
//串行话：确保事物可以从一个表中读取相同的行，在这个事物持续期间禁止其他事物对该表执行插入 更新和删除操作，
//所有的并发问题都可以避免 但效率非常低
int TRANSACTION_SERIALIZABLE     = 8;

Oracle 支持的 2 种事务隔离级别：READ COMMITED, SERIALIZABLE. Oracle 默认的事务隔离级别为: READ COMMITED 


有时候有必要在代码中设置事物隔离级别：
connection.setTransactionIsolation(Connection.TRANSACTION_NONE);
    
        
版权声明：本文为博主原创文章，未经博主允许不得转载。

Linux操作系统日志中常用的搜索关键字

Linux操作系统日志中常用的搜索关键字
我感觉，AIX操作系统日志要比Linux操作系统日志要简介扼要的多。AIX的errpt | more和 errpt -aj 错误码 | more 用起来那是相当爽。
这一点，Linux 就逊色不少。
下面我自己总结了在Linux操作系统日志中进行搜索的关键字：
charge
battery
fail
err
sata link down
link
down
    
        
版权声明：本文为博主原创文章，未经博主允许不得转载。

固定分组计算的sql简化

   在数据库应用开发中，我们经常需要面对复杂的SQL式计算，固定分组就是其中一种。固定分组的分组依据不在待分组的数据中，而是来自于外部，比如另一张表、外部参数、条件列表。对于特定类型的固定分组，用SQL实现还算简单，比如：分组依据来自另一张表，且对分组次序没有要求，但要实现其他情况就困难了。

   集算器可以轻松解决固定分组中的各类难题，下面用几个例子来说明。

   表sales存储着订单记录，其中CLIENT列是客户名，AMOUNT列是订单金额，请将sales按照“潜力客户列表”进行分组，并对各组的AMOUNT列汇总求和。表sales的部分数据如下：






   案例一：潜力客户列表来自于另外一张表potential的Std字段，只有四条记录，依次为：TAS、DSGC、GC、ZTOZ，并且客户ZTOZ不在sales表中。本案例要求按照上述记录顺序来分组汇总。

   假如我们对分组的顺序没有要求，那么SQL可以较简单地实现本案例，代码形如：

select potential.std as client, sum(sales.amount) as amount from potential left join client on potential.std=sales.client group by potential.std

   但本案例要求按照特定的顺序来分组，要实现这种算法，用SQL就必须制造一个用于排序的字段，最后还要用子查询去掉这个字段。而用集算器实现本案例会简单很多，代码如下：


   A1、B1：从数据库检索数据，分别命名为sales和potential，如下所示：







   A3=sales.align@a(potential:Std,Client)

   这句代码使用了函数align，它将sales的Client字段按照potentail的Std字段分为四个组，如下：




   可以看到，前三个组是sales中已有的数据，而第四个组不在sales中，因此是空值。另外，函数align的参数选项@a表示取出分组中的所有数据，如果不用这个函数选项，则只取每组的第一条。

   A4=pjoin(potential.(Std),A3.(~.sum(Amount)))

   这句代码用函数pjoin将两部分数据进行横向合并，一部分是potential.(Std)，这表示potential的Std字段；另一部分是A3.(~.sum(Amount))，这表示对A3中每组数据的Amount字段求和。本案例的最终结果如下：




   案例二：潜力客户列表是固定值，但客户的数量较多。

   如果客户的数量较少，用SQL时可以用union语句将所有的客户拼成一个假表，如果客户数量较多，这么做就可不取了，必须新建一张表持久保存数据才行。用集算器实现本案例可以省去建表的麻烦，代码如下：




   上述代码中，A2是个逗号分隔的字符串，可以轻松表达大量的固定值。

   案例三：潜力客户列表是外部参数，形如：TAS,BON,CHO,ZTOZ。

   外部参数经常变化，在SQL中用union来制造假表就很不方便了，只能创建一个临时表，将参数解析后一条条插入临时表，再进行后续的计算，但用集算器来做则不必建立临时表。

   首先定义一个参数arg1，如下：


   修改脚本文件，如下：





   运行脚本，并输入的参数值，假设参数值为”TAS,BON,CHO,ZTOZ”，如下：







   由于分组依据和案例一相同，因此最终的计算结果也一样。

   注意，A2中的代码可以将字符串”TAS,BON,CHO,ZTOZ“转变成序列["TAS","DSGC","GC","ZTOZ"]。如果输入参数直接就是["TAS","DSGC","GC","ZTOZ"]，则可以省略这个转换的步骤。

   案例四：固定分组的分组依据可以是数值，也可以是条件，比如：将订单金额按照1000、2000、4000划分为四个区间，每个区间一组订单，统计各组订单的总额。

   如果条件是已知，那就可以将这些条件写死在SQL里，如果条件是动态的外部参数，则需要用JAVA等高级语言拼凑SQL，过程非常复杂。而集算器支持动态表达式，可以轻松实现本案例，代码如下：


   上述代码中，变量byFac是本案例的分组依据，包含四个字符串条件。byFac也可以是外部参数，或者来自于数据库中的视图或表。A4中的最终结果如下：







   案例五：
   上述条件分组中，条件恰好没有发生重叠，但实际上发生重叠的情况很常见，比如将订单金额按照如下规则分组：

   1000至4000：常规订单r14

   2000以下：非重点订单r2

   3000以上：重点订单r3

   这时，r2和r3都会和r14发生条件重叠。条件发生重叠时，我们有时希望数据不重叠，即先取出符合r14的数据，再从剩下的数据中筛选出r2，以此类推。

   集算器的函数enum支持数据重叠的条件分组，如下：


   A3中的分组结果如下：





 
  计算结果如下：





 
  有时我们希望分组结果中包含重叠数据，即先从sales中取出符合r14的数据，再从完整的sales中取出符合r2的数据，以此类推。此时，只需要在函数enum中使用函数选项@r，即将A3中的代码改为：=sales.enum@r(byFac,Amount)，此时分组结果如下：





 
  图中红框里的数据是重复的。最后的计算结果如下：





 
  另外，集算器可被报表工具或java程序调用，调用的方法也和普通数据库相似，使用它提供的JDBC接口即可向java主程序返回ResultSet形式的计算结果，具体方法可参考相关文档。


版权声明：本文为博主原创文章，未经博主允许不得转载。

C#连接mysql三种方式

第一种方式：
使用MySQLDriverCS.dll连接
MySQLDriverCS软件下载：http://sourceforge.net/projects/mysqldrivercs/?source=typ_redirect
安装完之后再引用中添加引用，找到安装目录，找到MySQLDriverCS.dll文件，然后添加using MySQLDriverCS.dll文件
参考网址：http://www.cnblogs.com/genli/articles/1956537.html
C#连接mysql代码
MySQLConnection DBConn; 
DBConn = new MySQLConnection(new MySQLConnectionString("10.99.19.121","haha", "root", "root", 3306).AsString);
//DBConn = new MySQLConnection(new MySQLConnectionString("数据源","数据库名", "用户名", "密码", 端口号).AsString);
try
{
DBConn.Open(); // 执行查询语句
MessageBox.Show("数据库已经连接了！");
string sql = "select * from tb_user";
MySQLDataAdapter mda = new MySQLDataAdapter(sql, DBConn);
DataSet ds = new DataSet();
mda.Fill(ds, "table1");
this.dataGridView1.DataSource = ds.Tables["table1"];
}
catch(Exception ex)
{
MessageBox.Show(ex.Message);
}
DBConn.Close();


或者这么写：
MySQLConnectionString constr = new MySQLConnectionString("10.99.19.121", "haha", "root", "root", 3306);
MySQLConnection DBConn = new MySQLConnection(constr.AsString);
//MySQLConnection DBConn; 
//DBConn = new MySQLConnection(new MySQLConnectionString("10.99.19.121","haha", "root", "root", 3306).AsString);


try
{
DBConn.Open(); // 执行查询语句
MessageBox.Show("数据库已经连接了！");
string sql = "select * from tb_user";
MySQLDataAdapter mda = new MySQLDataAdapter(sql, DBConn);
DataSet ds = new DataSet();
mda.Fill(ds, "table1");
this.dataGridView1.DataSource = ds.Tables["table1"];
}
catch(Exception ex)
{
MessageBox.Show(ex.Message);
}
DBConn.Close();






第二种方法：
使用MySql.Data.dll连接
参考网址：http://www.cnblogs.com/sosoft/p/3906136.html
使用过程
dll文件修复方法：
1、解压下载的文件。
2、复制文件“mysql.data.dll”到系统目录下。
3、系统目录一般为：C:\WINNT\System32 64位系统为C:\Windows\SysWOW64
4、最后点击开始菜单-->运行-->输入regsvr32 mysql.data.dll 后，回车即可解决错误提示！ 
在再引用中添加引用，找到C:\Windows\SysWOW64目录，找到mysql.data.dll文件，然后添加using MySql.Data.MySqlClient;文件


string M_str_sqlcon = "server=10.99.19.121;user id=root;password=root;database=haha"; //根据自己的设置
MySqlConnection mycon = new MySqlConnection();
mycon.ConnectionString = M_str_sqlcon;
try
{
mycon.Open();


    MessageBox.Show("数据库已经连接了！");
string sql = "select * from tb_user";
MySqlDataAdapter mda = new MySqlDataAdapter(sql, mycon);
DataSet ds = new DataSet();
mda.Fill(ds, "table1");
this.dataGridView1.DataSource = ds.Tables["table1"];
}
catch(Exception ex)
{
MessageBox.Show(ex.Message);
}
mycon.Close();          






第三种方式：
通过ODBC访问mysql数据库
（没有时间研究那么多，之后会补充进来）
个人建议C#和sqlserver配合使用很好，但是和mysql不是说不好，只是不太合适，试想，你做一个项目，你还要给人家安装一个软件才能连接上数据库，感觉太麻烦，不专业，当然可以自己写一个库，但是很麻烦，而且又不是谁都会，所以个人建议用sqlserver,个人建议，不喜勿喷！！

版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-树和森林
一、树的存储结构
1）双亲表示法 
以一组地址连续的空间存储树的结点，同时在结点中附设一个指针域指示其双亲结点的位置。 
这种存储结构利用每个孩子只有一个双亲的性质，求结点的双亲可以在常量时间内实现，但求结点的孩子时需要遍历整个结构。

2）孩子表示法 
树中每个结点可能有多个孩子，所以结点除了一个数据域外，还需要有多个指针域指向孩子结点。 
有如下两种结点格式： 
同构结点：假设d为树的度，则每一个结点都有d个指针域。但是树中可能会有很多结点的度小于d，所以同构结点会有很多空的指针域，空间比较浪费。 
异构结点：结点结构中包含两个数据域，一个存放结点数据，一个结点存放结点的度（假设为n），另外还有n个指针域指向指向结点的n个孩子。

3）孩子兄弟法（二叉树法） 
以二叉链表作为树的存储结构，链表中结点的两个指针域分别指向该结点的第一个孩子结点和下一个兄弟结点。

二、森林和二叉树的转换
由于二叉树和树都可以用二叉链表作为存储结构，则以二叉链表作为媒介可导出树与二叉树之间的一个对应关系。也就是说，给定一棵树可以找到唯一的一棵二叉树与之对应。
将森林中每一棵树的根结点视为兄弟关系，根据孩子兄弟表示法，很容易将森林与唯一的一棵二叉树对应；由于这种对应关系是唯一的，所以给定一棵二叉树，必然能得到与之对应的森林。

三、树和森林的遍历
1）树的遍历 
A：先根遍历：先访问树的根节点，然后依次先根遍历根的每棵子树； 
B：后根遍历：先依次后根遍历每棵子树，然后访问根结点。

2）森林的遍历 
A：先序遍历森林：访问森林中第一棵树的根结点，先序遍历第一棵树中根结点的子树森林，先序遍历剩余树构成的森林。 
B：中序遍历森林：中序遍历森林中第一棵树的子树森林，访问第一棵树的根结点，中序遍历剩余树构成的森林。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

mysql-cluster数据自动修复（节点崩溃期间写入其他节点数据）
Mysql-cluster是具有故障节点恢复正常后的数据自动恢复功能的。 
实验环境中包含两个data节点：分别位于46，47两台服务器上 
ndb_mgm> show 
Connected to Management Server at: localhost:1186
[ndbd(NDB)] 2 node(s) 
id=2    @10.186.20.46  (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0) 
id=3    @10.186.20.47  (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0, *)
[ndb_mgmd(MGM)] 1 node(s) 
id=1    @10.186.20.45  (mysql-5.6.25 ndb-7.4.7)
[mysqld(API)]   2 node(s) 
id=4    @10.186.20.46  (mysql-5.6.25 ndb-7.4.7) 
id=5    @10.186.20.47  (mysql-5.6.25 ndb-7.4.7)
ndb_mgm> 
此处所说的数据自动修复是指的，在其中一个节点崩溃后，另外一个节点对外提供服务，在崩溃期间所有的数据变动（ndb表），可以在崩溃节点恢复并且假如集群后，自动同步数据。
实验过程如下： 
1，检验当前数据（test.a表为ndb表） 
46： 
mysql> select * from a; 
+——+ 
| id   | 
+——+ 
|    1 | 
|    2 | 
+——+ 
2 rows in set (0.00 sec)
47： 
mysql> select * from a; 
+——+ 
| id   | 
+——+ 
|    1 | 
|    2 | 
+——+ 
2 rows in set (0.01 sec)
2，停掉47上的sql，data节点
[root@10-186-20-47 mysql3307]# ps -ef|grep mysqld 
root      2216  2011  0 12:18 pts/0    00:00:00 /bin/sh ./bin/mysqld_safe –defaults-file=./my.cnf 
mysql     2385  2216  1 12:18 pts/0    00:01:54 /opt/mysql_s/bin/mysqld –defaults-file=./my.cnf –basedir=/opt/mysql_s/ –datadir=/opt/mysql_s/data –plugin-dir=/opt/mysql_s//lib/plugin –user=mysql –log-error=/opt/mysql_s/data/10-186-20-47.err –pid-file=/opt/mysql_s/data/10-186-20-47.pid –socket=/tmp/mysql3302.sock –port=3302 
root      4979  4641  0 12:37 pts/1    00:00:00 /bin/sh ./bin/mysqld_safe –defaults-file=./my.cnf 
mysql     5157  4979  0 12:37 pts/1    00:00:12 /usr/local/mysql3307/bin/mysqld –defaults-file=./my.cnf –basedir=/usr/local/mysql3307 –datadir=/usr/local/mysql3307/data –plugin-dir=/usr/local/mysql3307/lib/plugin –user=mysql –log-error=/usr/local/mysql3307/data/10-186-20-47.err –pid-file=/usr/local/mysql3307/data/10-186-20-47.pid –socket=/tmp/mysql3307.sock –port=3307 
root     14253  4641  0 14:44 pts/1    00:00:00 grep mysqld 
[root@10-186-20-47 mysql3307]# kill 2385 
[root@10-186-20-47 mysql3307]# 
[root@10-186-20-47 mysql3307]# ps -ef|grep ndb 
root      7810     1  0 13:13 ?        00:00:03 ndbd –ndb-connectstring=10.186.20.45:1186 
root      7811  7810  1 13:13 ?        00:01:28 ndbd –ndb-connectstring=10.186.20.45:1186 
root     14312  4641  0 14:44 pts/1    00:00:00 grep ndb 
[root@10-186-20-47 mysql3307]# kill 7811  7810 
[root@10-186-20-47 mysql3307]# 
通过管理节点show命令查看是否处于不可用的状态 
ndb_mgm> show
[ndbd(NDB)] 2 node(s) 
id=2    @10.186.20.46  (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0, *) 
id=3 (not connected, accepting connect from 10.186.20.47)
[ndb_mgmd(MGM)] 1 node(s) 
id=1    @10.186.20.45  (mysql-5.6.25 ndb-7.4.7)
[mysqld(API)]   2 node(s) 
id=4    @10.186.20.46  (mysql-5.6.25 ndb-7.4.7) 
id=5 (not connected, accepting connect from 10.186.20.47)
ndb_mgm> 
3，在46节点上做插入，删除操作。 
mysql> insert into a select 3; 
Query OK, 1 row affected (0.00 sec) 
Records: 1  Duplicates: 0  Warnings: 0
mysql> delete from a where id=1; 
Query OK, 1 row affected (0.02 sec)
mysql> 
4，开启47上的sql，data节点
[root@10-186-20-47 mysql_s]# ./bin/mysqld_safe –defaults-file=./my.cnf & 
[2] 14408 
[root@10-186-20-47 mysql_s]# 150825 14:46:26 mysqld_safe Logging to ‘/opt/mysql_s/data/10-186-20-47.err’. 
150825 14:46:26 mysqld_safe Starting mysqld daemon with databases from /opt/mysql_s/data
[root@10-186-20-47 mysql_s]#  
[root@10-186-20-47 mysql_s]#  
[root@10-186-20-47 mysql_s]# ndb 
ndbd    ndbmtd 
[root@10-186-20-47 mysql_s]# ndbd –ndb-connectstring=10.186.20.45:1186 
2015-08-25 14:46:54 [ndbd] INFO     – Angel connected to ‘10.186.20.45:1186’ 
2015-08-25 14:46:54 [ndbd] INFO     – Angel allocated nodeid: 3 
[root@10-186-20-47 mysql_s]# 
5，检验47上数据是否同步
mysql> select * from a; 
+——+ 
| id   | 
+——+ 
|    2 | 
|    3 | 
+——+ 
2 rows in set (0.00 sec)
数据已经自动修复。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

通过句柄恢复Linux下误删除的数据库数据文件
环境介绍：OS:Redhat EnterPrise 5.4DB:Oracle EnterPrise Database 11gR2(11.2.3.0)       在数据库正常运行时，运维人员在无意中将部分数据文件删除了，此时数据库管理员并不知道，且数据库运行正常，并没有立即抛出错误和告警；但是开发人员在对某张表进行更新的时候，正好这张表在被删除的数据文件中，报出ORA-01110和ORA-27041错误。随即数据库管理人员来看现象，发现有一个文件已经从系统层面删除了，并且数据库数据库没有进行重启操作，所以在没有进行更新的时候业务并没有造成影响。 非常幸运的是，在数据文件删除的情况下，数据库没有重启，使得这次的恢复变的较为简单，基于这次的恢复，我在我得实验设备上进行了重演，记录了完整的数据文件删除和恢复的完整步骤，如下：一、准备测试环境，创建新的表空间及数据文件：SQL> create tablespace woo datafile
  2  '/u01/app/oracle/oradata/PROD1/woo01.dbf'
  3  size 20m autoextend on;

Tablespace created.

SQL> select name from v$dbfile;

NAME                                                                            
--------------------------------------------------------------------------------
/u01/app/oracle/oradata/PROD1/users01.dbf                                       
/u01/app/oracle/oradata/PROD1/undotbs01.dbf                                     
/u01/app/oracle/oradata/PROD1/sysaux01.dbf                                      
/u01/app/oracle/oradata/PROD1/system01.dbf                                      
/u01/app/oracle/oradata/PROD1/example01.dbf                                     
/u01/app/oracle/oradata/PROD1/tools.dbf                                         
/u01/app/oracle/oradata/PROD1/test.dbf                                          
/u01/app/oracle/oradata/PROD1/woo01.dbf                                         

8 rows selected.二、模拟故障，在系统级别删除数据文件：SQL> !rm -rf /u01/app/oracle/oradata/PROD1/woo01.dbf三、检查数据库状态，并且创建测试数据SQL> !tail -100f /u01/app/oracle/diag/rdbms/prod1/PROD1/trace/alert*

Wed Aug 26 14:31:01 2015
create tablespace woo datafile
'/u01/app/oracle/oradata/PROD1/woo01.dbf'
size 20m autoextend on
Completed: create tablespace woo datafile
'/u01/app/oracle/oradata/PROD1/woo01.dbf'
size 20m autoextend on

#在这里我们可以看到数据文件在系统层面被删除之后数据库并没产生告警。


SQL> create table test tablespace woo as select * from dba_users;
create table test tablespace woo as select * from dba_users
                                                  *
ERROR at line 1:
ORA-01116: error in opening database file 8 
ORA-01110: data file 8: '/u01/app/oracle/oradata/PROD1/woo01.dbf' 
ORA-27041: unable to open file 
Linux Error: 2: No such file or directory 
Additional information: 3 


#而使在我们需要往这个删除的文件中写入数据的时候才发现数据文件被删除了。

SQL> !tail -50 /u01/app/oracle/diag/rdbms/prod1/PROD1/trace/alert*

Wed Aug 26 14:34:33 2015
Errors in file /u01/app/oracle/diag/rdbms/prod1/PROD1/trace/PROD1_smon_9564.trc:
ORA-01116: error in opening database file 8
ORA-01110: data file 8: '/u01/app/oracle/oradata/PROD1/woo01.dbf'
ORA-27041: unable to open file
Linux Error: 2: No such file or directory
Additional information: 3
Wed Aug 26 14:34:33 2015
Checker run found 1 new persistent data failures
Wed Aug 26 14:39:44 2015
Errors in file /u01/app/oracle/diag/rdbms/prod1/PROD1/trace/PROD1_m000_14015.trc:
ORA-01116: error in opening database file 8
ORA-01110: data file 8: '/u01/app/oracle/oradata/PROD1/woo01.dbf'
ORA-27041: unable to open file
Linux Error: 2: No such file or directory
Additional information: 3


SQL> select instance_name,status from v$instance;

INSTANCE_NAME    STATUS                                                         
---------------- ------------                                                   
PROD1            OPEN                                                           
                                  #即便数据库已经知道了数据库文件丢失，因为不是系统表空间的数据文件，所以数据库的运行并没有收到影响。四、查找数据文件        因为数据库并没有停止运行，这个时候我们可以通过dbwr写数据文件进程来找到进程句柄号，进入该句柄号就可以找到该进程锁定的相关数据文件了。SQL> !ps -ef|grep dbw|grep -v grep
oracle    9554     1  0 07:16 ?        00:00:01 ora_dbw0_PROD1

#我们可以看到进程的ID为9554，通过进程ID查找到进程下锁定的所有文件
SQL> !ls -rtl /proc/9554/fd

[oracle@edbjr2p1 trace]$ ls -rtl /proc//9554/fd
total 0
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 0 -> /dev/null
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 9 -> /dev/null
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 8 -> /u01/app/oracle/product/11.2.0/dbhome_1/bin/oracle
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 7 -> /dev/null
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 6 -> /u01/app/oracle/product/11.2.0/dbhome_1/hpatch/orapatchPROD1.cfg
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 5 -> /u01/app/oracle/product/11.2.0/dbhome_1/dbs/hc_PROD1.dat
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 4 -> /dev/null
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 3 -> /dev/null
l-wx------ 1 oracle oinstall 64 Aug 26 14:37 2 -> /dev/null
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 19 -> /u01/app/oracle/product/11.2.0/dbhome_1/hpatch/orapatchPROD1.cfg
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 18 -> /proc/9554/fd
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 17 -> /u01/app/oracle/product/11.2.0/dbhome_1/rdbms/mesg/oraus.msb
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 16 -> /u01/app/oracle/product/11.2.0/dbhome_1/dbs/hc_PROD1.dat
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 15 -> /u01/app/oracle/product/11.2.0/dbhome_1/bin/oracle
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 14 -> /dev/zero
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 13 -> /dev/zero
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 12 -> /u01/app/oracle/product/11.2.0/dbhome_1/hpatch/orapatchPROD1.cfg
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 11 -> /dev/null
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 10 -> /dev/null
l-wx------ 1 oracle oinstall 64 Aug 26 14:37 1 -> /dev/null
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 268 -> /u01/app/oracle/oradata/PROD1/woo01.dbf (deleted)//我们找到了这个文件，处于deleted
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 267 -> /u01/app/oracle/oradata/PROD1/temp2.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 266 -> /u01/app/oracle/oradata/PROD1/temp1.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 265 -> /u01/app/oracle/oradata/PROD1/temp01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 264 -> /u01/app/oracle/oradata/PROD1/test.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 263 -> /u01/app/oracle/oradata/PROD1/tools.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 262 -> /u01/app/oracle/oradata/PROD1/example01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 261 -> /u01/app/oracle/oradata/PROD1/users01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 260 -> /u01/app/oracle/oradata/PROD1/undotbs01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 259 -> /u01/app/oracle/oradata/PROD1/sysaux01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 258 -> /u01/app/oracle/oradata/PROD1/system01.dbf
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 257 -> /u01/app/oracle/oradata/PROD1/control02.ctl
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 256 -> /u01/app/oracle/oradata/PROD1/control01.ctl
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 24 -> /u01/app/oracle/product/11.2.0/dbhome_1/rdbms/mesg/oraus.msb
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 23 -> /u01/app/oracle/product/11.2.0/dbhome_1/dbs/lkPROD1
lrwx------ 1 oracle oinstall 64 Aug 26 14:37 22 -> /u01/app/oracle/product/11.2.0/dbhome_1/dbs/hc_PROD1.dat
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 21 -> /u01/app/oracle/product/11.2.0/dbhome_1/bin/oracle
lr-x------ 1 oracle oinstall 64 Aug 26 14:37 20 -> /dev/zero
lrwx------ 1 oracle oinstall 64 Aug 26 15:09 25 -> socket:[173283]
五、将该文件句柄拷贝到原文件位置：SQL> !cp /proc//9554/fd/268 /u01/app/oracle/oradata/PROD1/woo01.dbf六、对拷贝回来的文件执行常规的数据恢复操作，实际上也就是更新下数据文件头部的scn号：SQL> col name format a50
SQL> select file#,status,name from v$datafile;

     FILE# STATUS  NAME                                                         
---------- ------- --------------------------------------------------           
         1 SYSTEM  /u01/app/oracle/oradata/PROD1/system01.dbf                   
         2 ONLINE  /u01/app/oracle/oradata/PROD1/sysaux01.dbf                   
         3 ONLINE  /u01/app/oracle/oradata/PROD1/undotbs01.dbf                  
         4 ONLINE  /u01/app/oracle/oradata/PROD1/users01.dbf                    
         5 ONLINE  /u01/app/oracle/oradata/PROD1/example01.dbf                  
         6 ONLINE  /u01/app/oracle/oradata/PROD1/tools.dbf                      
         7 ONLINE  /u01/app/oracle/oradata/PROD1/test.dbf                       
         8 ONLINE  /u01/app/oracle/oradata/PROD1/woo01.dbf    	#我们可以看到这个时候该数据文件是ONLINE状态                  

8 rows selected.


#由于这是一套在线库，且有其它业务，不可随意停机，所以这个时候将需要恢复的数据文件offline，就可以直接在线执行恢复了。

SQL> alter database datafile 8 offline;

Database altered.

SQL> select file#,status,name from v$datafile;

     FILE# STATUS  NAME                                                         
---------- ------- --------------------------------------------------           
         1 SYSTEM  /u01/app/oracle/oradata/PROD1/system01.dbf                   
         2 ONLINE  /u01/app/oracle/oradata/PROD1/sysaux01.dbf                   
         3 ONLINE  /u01/app/oracle/oradata/PROD1/undotbs01.dbf                  
         4 ONLINE  /u01/app/oracle/oradata/PROD1/users01.dbf                    
         5 ONLINE  /u01/app/oracle/oradata/PROD1/example01.dbf                  
         6 ONLINE  /u01/app/oracle/oradata/PROD1/tools.dbf                      
         7 ONLINE  /u01/app/oracle/oradata/PROD1/test.dbf                       
         8 RECOVER /u01/app/oracle/oradata/PROD1/woo01.dbf       #在对数据文件进行操作，触发了该文件，发现文件头部的scn不一致，提示需要进行恢复。’                

8 rows selected.

#执行在线恢复并且online该数据文件。

SQL> recover datafile 8;
Media recovery complete.
SQL> alter database datafile 8 online;

Database altered.七、验证数据文件恢复后是否可以正常使用SQL> select file#,status,name from v$datafile;

     FILE# STATUS  NAME                                                         
---------- ------- --------------------------------------------------           
         1 SYSTEM  /u01/app/oracle/oradata/PROD1/system01.dbf                   
         2 ONLINE  /u01/app/oracle/oradata/PROD1/sysaux01.dbf                   
         3 ONLINE  /u01/app/oracle/oradata/PROD1/undotbs01.dbf                  
         4 ONLINE  /u01/app/oracle/oradata/PROD1/users01.dbf                    
         5 ONLINE  /u01/app/oracle/oradata/PROD1/example01.dbf                  
         6 ONLINE  /u01/app/oracle/oradata/PROD1/tools.dbf                      
         7 ONLINE  /u01/app/oracle/oradata/PROD1/test.dbf                       
         8 ONLINE  /u01/app/oracle/oradata/PROD1/woo01.dbf                      

8 rows selected.

SQL> create table test tablespace woo as select * from dba_users;

Table created.8、至此完成该数据文件的恢复

版权声明：本文为博主原创文章，未经博主允许不得转载。

数据库SQL Server2012笔记（五）——维护数据的完整性——约束
1、概念
1）约束用于确保数据库数据满足特定的商业规则。
2）在sql server中，约束包括：not null,unique,primary key,foreigh key,check五种。


2、not  null(非空)


如果在列上定义了not null，那么当插入数据时，必须为列提供数据。


3、unique(唯一)


当定义了唯一约束后，该列值是不能重复的，可以为null，但最多只能有一个是null。




4、primary  key


不能重复且不能为null。一张表最多只能有一个主键，但是可以有多个unique约束。表可以有复合主键

多个咧构成一个主键放在最后：primary  key(字段1，字段2)同时相同，才为重复。

5、行级定义和表级定义

     create  table  test
     (Id  int,
      name  nvarchar(30),
      age  int  not  null,——>行级定义
      primary  key(Id,name)——>表级定义)


6、foreign key:定义主表（被引用的表）和从表（引用外键的表）之间的关系。
1）外键约束要定义在从表。
2）主表必须有主键约束或unique约束。
3）外键列数据必须在主表的主键列存在或为null。


7、check：用于强制行数据必须满足的条件。


8、default：若不赋值，则给默认值
     create  table  mes  (Id  int  primary  key   identiry(1,1), mesDate  datatime  default  getdate())
插入数据时，若不给出时间，则赋本地时间。


版权声明：本文为博主原创文章，未经博主允许不得转载。

expdp和impdp会带着表的supplemental log

做ogg，hvr的工程师都知道table的supplemental log，这个对逻辑复制软件相当重要，若是表级不开启这个supplemental log，在目的端会遇到ora-01403 数据未找到。
经过试验证实，expdp和impdp会带着表的supplemental log
[oracle@gg321 ~]$ expdp sys/orace directory=dumpdir dumpfile=a1.dmp logfile=a1_exp.log tables=u1.a1

Export: Release 10.2.0.4.0 - Production on Monday, 31 August, 2015 13:39:30

Copyright (c) 2003, 2007, Oracle.  All rights reserved.
;;; 
Connected to: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
Starting "SYS"."SYS_EXPORT_TABLE_01":  /******** AS SYSDBA directory=dumpdir dumpfile=a1.dmp logfile=a1_exp.log tables=u1.a1
Estimate in progress using BLOCKS method...
Processing object type TABLE_EXPORT/TABLE/TABLE_DATA
Total estimation using BLOCKS method: 192 KB
Processing object type TABLE_EXPORT/TABLE/TABLE
Processing object type TABLE_EXPORT/TABLE/INDEX/INDEX
Processing object type TABLE_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type TABLE_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type TABLE_EXPORT/TABLE/COMMENT
Processing object type TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
. . exported "U1"."A1"                    42.87 KB      34 rows
Master table "SYS"."SYS_EXPORT_TABLE_01" successfully loaded/unloaded
******************************************************************************
Dump file set for SYS.SYS_EXPORT_TABLE_01 is:
  /u02/a1.dmp
Job "SYS"."SYS_EXPORT_TABLE_01" successfully completed at 13:39:59

[oracle@gg321 ~]$ impdp sys/oracle directory=dumpdir dumpfile=a1.dmp logfile=a1_imp.log REMAP_SCHEMA=U1:scott

Import: Release 10.2.0.4.0 - Production on Monday, 31 August, 2015 14:20:45

Copyright (c) 2003, 2007, Oracle.  All rights reserved.

UDI-00008: operation generated ORACLE error 1017
ORA-01017: invalid username/password; logon denied

Username: / as sysdba

Connected to: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
Master table "SYS"."SYS_IMPORT_FULL_02" successfully loaded/unloaded
Starting "SYS"."SYS_IMPORT_FULL_02":  /******** AS SYSDBA directory=dumpdir dumpfile=a1.dmp logfile=a1_imp.log REMAP_SCHEMA=u1:scott 
Processing object type TABLE_EXPORT/TABLE/TABLE
Processing object type TABLE_EXPORT/TABLE/TABLE_DATA
. . imported "SCOTT"."A1"                       42.87 KB      34 rows
Processing object type TABLE_EXPORT/TABLE/INDEX/INDEX
Processing object type TABLE_EXPORT/TABLE/CONSTRAINT/CONSTRAINT
Processing object type TABLE_EXPORT/TABLE/INDEX/STATISTICS/INDEX_STATISTICS
Processing object type TABLE_EXPORT/TABLE/COMMENT
Processing object type TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS
Job "SYS"."SYS_IMPORT_FULL_02" successfully completed at 14:21:07

 
[oracle@gg321 ~]$ 
上面的expdp和impdp的输出是没有体现log group的.需要查询dba_log_groups和DBA_LOG_GROUP_COLUMNS就可以看到 表的log group确实被expdp和impdp了。
 

版权声明：本文为博主原创文章，未经博主允许不得转载。

sql 在查询结果中增加标记字段
有时候需要在查询结果中增加该表在数据库中没有的字段作为标记字段，在sql中直接加：
select *, 'xiaojiayu' as mark from user_info
查询结果会在原表增加一个mark字段，值均为:xiaojiayu。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

mysql 按月/按周汇总统计函数 DATE_FORMAT() 函数
 
定义和用法
DATE_FORMAT() 函数用于以不同的格式显示日期/时间数据。
语法
DATE_FORMAT(date,format)
date 参数是合法的日期。format 规定日期/时间的输出格式。
可以使用的格式有：
格式 描述
%a 缩写星期名
%b 缩写月名
%c 月，数值
%D 带有英文前缀的月中的天
%d 月的天，数值(00-31)
%e 月的天，数值(0-31)
%f 微秒
%H 小时 (00-23)
%h 小时 (01-12)
%I 小时 (01-12)
%i 分钟，数值(00-59)
%j 年的天 (001-366)
%k 小时 (0-23)
%l 小时 (1-12)
%M 月名
%m 月，数值(00-12)
%p AM 或 PM
%r 时间，12-小时（hh:mm:ss AM 或 PM）
%S 秒(00-59)
%s 秒(00-59)
%T 时间, 24-小时 (hh:mm:ss)
%U 周 (00-53) 星期日是一周的第一天
%u 周 (00-53) 星期一是一周的第一天
%V 周 (01-53) 星期日是一周的第一天，与 %X 使用
%v 周 (01-53) 星期一是一周的第一天，与 %x 使用
%W 星期名
%w 周的天 （0=星期日, 6=星期六）
%X 年，其中的星期日是周的第一天，4 位，与 %V 使用
%x 年，其中的星期一是周的第一天，4 位，与 %v 使用
%Y 年，4 位
%y 年，2 位
实例
下面的脚本使用 DATE_FORMAT() 函数来显示不同的格式。我们使用 NOW() 来获得当前的日期/时间：
DATE_FORMAT(NOW(),'%b %d %Y %h:%i %p') DATE_FORMAT(NOW(),'%m-%d-%Y') DATE_FORMAT(NOW(),'%d %b %y') DATE_FORMAT(NOW(),'%d %b %Y %T:%f')
结果类似：
Dec 29 2008 11:45 PM 12-29-2008 29 Dec 08 29 Dec 2008 16:25:46.635
实例
SELECT DATE_FORMAT(`订单创建时间`,'%u') as week,
sum(`总金额`) as money from corder
where DATE_FORMAT(`订单创建时间`,'%Y')=2012 
GROUP BY  week
结果：
week money
13  854314.83
14  5177385.26
15  4870063.87
16  4529686.53
17  3678584.15
 
 
原文链接 ： 
http://blog.sina.com.cn/s/blog_9d0b00a401019arm.html


版权声明：本文为博主原创文章，未经博主允许不得转载。

【数据结构】-栈和队列
一、栈的基本概念
栈（stack）：限定在表尾进行插入和删除操作的线性表。 
栈顶（top）：允许进行插入和删除操作的一端，又称表尾。 
栈底（bottom）：固定端，又称表头。 
空栈：不含元素的空表称为空栈。 
 插入元素的操作称为入栈（压栈）。 
 删除元素的操作称为出栈（弹栈）。 
 顺序存储的栈称为顺序栈。 
 链式存储的栈称为链栈。
设栈S=（a1,a2,...an）,则a1称为栈底元素，an称为栈顶元素。栈中元素按后进先出（Last in First out）LIFO的顺序进出。

二、队列的基本概念 
队列：是一种先进先出（FIFO）的线性表，它只允许在表的一端进行插入，在表的另一端进行删除。 
队尾：允许插入的一端叫队尾。 
队头：允许删除的一端叫队头。 
 顺序存储的队列称为顺序队列。 
 链式存储的队列称为链队列。
设队列Q=（a1,a2,...an）,则a1称为队头元素，an称为队尾元素。队列中元素按先进先出（First in First out）FIFO的顺序进出。
双端队列：限定插入和删除操作在表的两端进行的线性表。

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

linux+tomcat+mysql相关命令
备份数据库：
mysqldump -u root -p  数据库名>sql文件所在的路径（例：/soft/bak/mysqlbak/xx201508242248.sql）
备份项目：
即：将tomcat的root文件夹下的所有文件备份到新文件夹中
cp 空格-rf空格 源文件夹 空格 目标文件夹
例：cp -rf soft/apache-tomcat-7.0.59/webapps/ROOT/ soft/bak/xxbak/bak_201508242248


将sql文件导入到mysql数据库：
1、mysql -u root -p
2、mysql>use 数据库名称；
3、mysql>source sql文件的路径全名
导入成功后用此命令查看表：mysql>describe 表名;


查看mysql字符集：show variables like 'character%';
退出mysql：mysql>quit;
新建文件夹：mkdir -p 文件夹名称
进入一级：cd /xx/xx
退出一级：cd ../
退出两级：cd ../..
查看目录中文件：ls
查看tomcat控制台打印日志：
1、cd /soft/apache-tomcat-7.0.59/logs回车
2、tail -f catalina.logs


﻿﻿

版权声明：本文为博主原创文章，未经博主允许不得转载。

[置顶]
        C++栈学习——赋值运算法的重载

对于编程，向来都是编的少，看的多，这种错误的学习方法一直延续至今，所以像运算符重载这么简单的东西，还是不太熟，今天借着学习栈的机会，自己写了一下链栈中赋值运算符的重载函数，写完之后对比了一下教材（《数据结构C++语言描述》任燕版），发现自己真是有种简单问题复杂化的天赋——写程序按部就班，不会优化。顺便发现了教材中一个小小的错误，在此加以纠正。


链栈赋值运算符的重载（自己写的重载函数） 
 说明一下自己的函数：想要实现不管左值链栈是不是为空，都能将右值链栈整个拷贝过来，所以整体思路就是：首先赋值两个链栈的公共部分的值，接着分为两种情况：（a）左值链栈元素个数小于右值链栈元素个数;(b)左值链栈元素个数大于右值链栈元素个数。

template<typename ElemType>
Linkstack <ElemType> Linkstack<ElemType>::operator = (Linkstack <ElemType> rightS)
{
    NodePointer rs,ls,s,r,p;
    rs=rightS.top;
    ls=top;
    r=NULL;
    if (this!=&rightS)
    {
        while (ls!=NULL&&rs!=NULL)//二者公共部分的赋值
        {
        ls->data=rs->data;
        rs=rs->next;
        r=ls;
        ls=ls->next;
        }

        //  左值链栈元素个数小于右值链栈元素个数
        while (ls==NULL&&rs!=NULL)
        {
        s=new(LinkNode);
        assert(s!=0);
        s->data=rs->data;
        s->next=NULL;
        rs=rs->next;
        r->next=s;
        r=r->next;
        ls=NULL;
        }

        //  左值链栈元素个数大于右值链栈元素个数
        while (ls!=NULL&&rs==NULL)
        {
            p=ls;
            r->next=ls->next;
            ls=ls->next;
            delete p;
        }

    }

    return *this;

}

链栈赋值运算符的重载（《数据结构C++语言描述》任燕版） 
看完作者的程序之后，觉得很汗颜，明明可以先将左值链栈清空，然后按照一种情况来拷贝呢，为啥就是想不到呢？哎，愚蠢之极。作者的代码如下：

template<typename ElemType>
Linkstack <ElemType> Linkstack<ElemType>::operator = (Linkstack <ElemType> rightS)
{
    NodePointer p=NULL;
    NodePointer rp=rightS.top;
    NodePointer s;

    if (this!=&rightS)
    {
        clear();
        while (rp)
        {
            s=new(LinkNode);
            assert(s!=0);
            s->data=rp->data;

            if (!top)
            {
                top=s;
            }
            else
            {
                p->next=s;
            }
            p=s;
            rp=rp->next;

        }
        if (p)
            p->next=NULL;
    }

    return *this;

}

纠正一个错误 
 但是，作者的代码中存在一个小小的错误，即没有对指针p进行初始化，修改后程序如下：

template<typename ElemType>
Linkstack <ElemType> Linkstack<ElemType>::operator = (Linkstack <ElemType> rightS)
{
    NodePointer p=NULL;
    NodePointer rp=rightS.top;
    NodePointer s;

    if (this!=&rightS)
    {
        clear();
        while (rp)
        {
            s=new(LinkNode);
            assert(s!=0);
            s->data=rp->data;

            if (!top)
            {
                top=s;
                p=top;/////////////////自己添加的指针初始化部分，否则，指针根本不知道移到哪里去了
            }
            else
            {
                p->next=s;
            }
            p=s;
            rp=rp->next;

        }
        if (p)
            p->next=NULL;


    }

    return *this;

}

            $(function () {
                $('pre.prettyprint code').each(function () {
                    var lines = $(this).text().split('\n').length;
                    var $numbering = $('<ul/>').addClass('pre-numbering').hide();
                    $(this).addClass('has-numbering').parent().append($numbering);
                    for (i = 1; i <= lines; i++) {
                        $numbering.append($('<li/>').text(i));
                    };
                    $numbering.fadeIn(1700);
                });
            });
        

版权声明：本文为博主原创文章，未经博主允许不得转载。

